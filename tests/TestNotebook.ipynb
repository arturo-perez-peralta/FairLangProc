{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f077304",
   "metadata": {},
   "source": [
    "# Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36538d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pytest\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset as PtDataset\n",
    "from datasets import Dataset as HfDataset\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")) if \"__file__\" in globals() else os.path.abspath(\"..\")\n",
    "sys.path.insert(0, ROOT_PATH)\n",
    "\n",
    "from FairLangProc.datasets.fairness_datasets import BiasDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fda3639",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPLEMENTED = [\n",
    "    \"BBQ\",\n",
    "    \"BEC-Pro\",\n",
    "    \"BOLD\",\n",
    "    \"BUG\",\n",
    "    \"CrowS-Pairs\",\n",
    "    \"GAP\",\n",
    "    \"StereoSet\",\n",
    "    \"UnQover\",\n",
    "    \"WinoBias+\",\n",
    "    \"WinoBias\",\n",
    "    \"Winogender\"\n",
    "]\n",
    "\n",
    "DATASETS = [\n",
    "    \"BBQ\",\n",
    "    \"BEC-Pro\",\n",
    "    \"BOLD\",\n",
    "    \"BUG\",\n",
    "    \"Bias-NLI\",\n",
    "    \"CrowS-Pairs\",\n",
    "    \"GAP\",\n",
    "    \"Grep-BiasIR\",\n",
    "    \"HONEST\",\n",
    "    \"HolisticBias\",\n",
    "    \"PANDA\",\n",
    "    \"RedditBias\",\n",
    "    \"StereoSet\",\n",
    "    \"TrustGPT\",\n",
    "    \"UnQover\",\n",
    "    \"WinoBias\",\n",
    "    \"WinoBias+\",\n",
    "    \"WinoQueer\",\n",
    "    \"Winogender\",\n",
    "]\n",
    "\n",
    "REMAINING = [dataset for dataset in DATASETS if dataset not in IMPLEMENTED]\n",
    "\n",
    "CONFIGURATIONS = {\n",
    "    \"BBQ\": [\"Age\", \"Disability_Status\", \"Gender_identity\", \"Nationality\", \"Physical_appearance\", \"Race_ethnicity\", \"Race_x_gender\", \"Race_x_SES\", \"Religion\", \"SES\", \"Sexual_orientation\", \"all\"],\n",
    "    \"BEC-Pro\": [\"english\", \"german\", \"all\"],\n",
    "    \"BOLD\": [\"prompts\", \"wikipedia\", \"all\"],\n",
    "    \"BUG\": [\"balanced\", \"full\", \"gold\", \"all\"],\n",
    "    \"Bias-NLI\": [\"process\", \"load\", \"all\"],\n",
    "    \"CrowS-Pairs\": [\"\"],\n",
    "    \"GAP\": [\"\"],\n",
    "    \"Grep-BiasIR\": [\"queries\", \"documents\", \"relevance\", \"all\"],\n",
    "    \"HolisticBias\": [\"noun_phrases\", \"sentences\", \"all\"],\n",
    "    \"PANDA\": [\"train\", \"test\", \"dev\", \"all\"],\n",
    "    \"RedditBias\": [\"posts\", \"comments\", \"annotations\", \"all\"],\n",
    "    \"StereoSet\": [\"word\", \"sentence\", \"all\"],\n",
    "    \"TrustGPT\": [\"process\", \"load\", \"all\", \"benchmarks\"],\n",
    "    \"UnQover\": [\"questions\", \"answers\", \"annotations\"],\n",
    "    \"WinoBias\": [\"pairs\", \"WinoBias\"],\n",
    "    \"WinoBias+\": [\"\"],\n",
    "    \"WinoQueer\": [\"sentences\", \"templates\", \"annotations\", \"all\"],\n",
    "    \"Winogender\": [\"\"],\n",
    "}\n",
    "\n",
    "FORMATS = [\"hf\", \"pt\", \"raw\"]\n",
    "\n",
    "CLASS_DICT = {\n",
    "    \"hf\": HfDataset,\n",
    "    \"pt\": PtDataset,\n",
    "    \"raw\": pd.DataFrame\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb867e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CASES_FORMAT = [\n",
    "    (dataset, config, format)\n",
    "    for dataset in CONFIGURATIONS.keys()\n",
    "    for config in CONFIGURATIONS[dataset] \n",
    "    for format in FORMATS if dataset in IMPLEMENTED\n",
    "]\n",
    "\n",
    "@pytest.mark.parametrize(\"dataset, config, format\", TEST_CASES_FORMAT)\n",
    "def test_format(dataset, config, format):\n",
    "    result = BiasDataLoader(dataset = dataset, config = config, format = format)\n",
    "    assert isinstance(result, dict)\n",
    "    for key in result:\n",
    "        assert isinstance(result[key], CLASS_DICT[format])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e37c645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_columns():\n",
    "    for dataset in CONFIGURATIONS.keys():\n",
    "        if dataset in IMPLEMENTED:\n",
    "            result = BiasDataLoader(dataset = dataset, config = 'all', format = 'raw')\n",
    "            if result is None:\n",
    "                result = BiasDataLoader(dataset = dataset, config = '', format = 'raw')\n",
    "            try:\n",
    "                print(dataset)\n",
    "                print(list(result[list(result.keys())[0]].keys()))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "def _get_rows():\n",
    "    for dataset in CONFIGURATIONS.keys():\n",
    "        if dataset in IMPLEMENTED:\n",
    "            result = BiasDataLoader(dataset = dataset, config = 'all', format = 'raw')\n",
    "            if result is None:\n",
    "                result = BiasDataLoader(dataset = dataset, config = '', format = 'raw')\n",
    "            try:\n",
    "                string = f\"\\\"{dataset}\\\": {{\"\n",
    "                for data in result.keys():\n",
    "                    if data == 'templates' and dataset == 'BBQ':\n",
    "                        continue\n",
    "                    string += f\"\\\"{data}\\\": {len(result[data].index)}, \"\n",
    "                string += \"}, \"\n",
    "                print(string)\n",
    "            except:\n",
    "                print(dataset + \": nothing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a467f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = {\n",
    "    \"BBQ\": ['example_id', 'question_index', 'question_polarity', 'context_condition', 'category', 'answer_info', 'additional_metadata', 'context', 'question', 'ans0', 'ans1', 'ans2', 'label'],\n",
    "    \"BEC-Pro\": ['Unnamed: 0', 'Sentence', 'Sent_TM', 'Sent_AM', 'Sent_TAM', 'Template', 'Person', 'Gender', 'Profession', 'Prof_Gender'],\n",
    "    \"BOLD\": ['gender_prompt.json', 'political_ideology_prompt.json', 'profession_prompt.json', 'race_prompt.json', 'religious_ideology_prompt.json'],\n",
    "    \"BUG\": ['Unnamed: 0', 'sentence_text', 'tokens', 'profession', 'g', 'profession_first_index', 'g_first_index', 'predicted gender', 'stereotype', 'distance', 'num_of_pronouns', 'corpus', 'data_index'],\n",
    "    \"CrowS-Pairs\": ['Unnamed: 0', 'sent_more', 'sent_less', 'stereo_antistereo', 'bias_type', 'annotations', 'anon_writer', 'anon_annotators'],\n",
    "    \"GAP\": ['ID', 'Text', 'Pronoun', 'Pronoun-offset', 'A', 'A-offset', 'A-coref', 'B', 'B-offset', 'B-coref', 'URL'],\n",
    "    \"HolisticBias\": None,\n",
    "    \"StereoSet\": ['options', 'context', 'target', 'bias_type', 'labels'],\n",
    "    \"WinoBias+\": ['gendered', 'neutral'],\n",
    "    \"WinoBias\": ['sentence', 'entity', 'pronoun'],\n",
    "    \"Winogender\": ['sentid', 'sentence']\n",
    "}\n",
    "\n",
    "TEST_CASES_COLUMNS = [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f59239f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = {\n",
    "    \"BBQ\": {\"Age.jsonl\": 3680, \"Disability_status.jsonl\": 1556, \"Gender_identity.jsonl\": 5672, \"Nationality.jsonl\": 3080, \"Physical_appearance.jsonl\": 1576, \"Race_ethnicity.jsonl\": 6880, \"Race_x_SES.jsonl\": 11160, \"Race_x_gender.jsonl\": 15960, \"Religion.jsonl\": 1200, \"SES.jsonl\": 6864, \"Sexual_orientation.jsonl\": 864, \"additional_metadata.csv\": 58556, },\n",
    "    \"BEC-Pro\": {\"english\": 5400, \"german\": 5400, }, \n",
    "    \"BUG\": {\"balanced_BUG.csv\": 25504, \"full_BUG.csv\": 105687, \"gold_BUG.csv\": 1717, }, \n",
    "    \"CrowS-Pairs\": {\"data\": 1508, }, \n",
    "    \"GAP\": {\"gap-development.tsv\": 2000, \"gap-test.tsv\": 2000, \"gap-validation.tsv\": 454, }, \n",
    "    \"StereoSet\": {\"test_sentence\": 6374, \"test_word\": 6392, \"dev_sentence\": 2123, \"dev_word\": 2106, }, \n",
    "    \"WinoBias\": {\"anti_stereotyped_type1.txt.dev\": 396, \"anti_stereotyped_type1.txt.test\": 396, \"anti_stereotyped_type2.txt.dev\": 396, \"anti_stereotyped_type2.txt.test\": 396, \"pro_stereotyped_type1.txt.dev\": 396, \"pro_stereotyped_type1.txt.test\": 396, \"pro_stereotyped_type2.txt.dev\": 396, \"pro_stereotyped_type2.txt.test\": 396, }, \n",
    "    \"WinoBias+\": {\"data\": 3167, }, \n",
    "    \"Winogender\": {\"data\": 720, }, \n",
    "}\n",
    "\n",
    "TEST_CASES_ROWS = [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc3b414",
   "metadata": {},
   "source": [
    "# Test metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8a112",
   "metadata": {},
   "source": [
    "## Test probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e45f0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import pytest\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")) if \"__file__\" in globals() else os.path.abspath(\"..\")\n",
    "sys.path.insert(0, ROOT_PATH)\n",
    "\n",
    "from FairLangProc.metrics import LPBS, CBS, CPS, AUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel:\n",
    "    def __call__(self, **kwargs):\n",
    "        batch_size, seq_len = kwargs[\"input_ids\"].shape\n",
    "        logits = torch.zeros(batch_size, seq_len, 30522)\n",
    "        logits[:, :, 200] = 5.0\n",
    "        logits[:, :, 201] = -5.0 \n",
    "        logits[:, :, 202] = 15.0 \n",
    "        logits[:, :, 300] = 5.0\n",
    "        logits[:, :, 301] = -5.0 \n",
    "        logits[:, :, 302] = 15.0 \n",
    "        logits[:, :, 400] = 10.0 \n",
    "        logits[:, :, 401] = -10.0 \n",
    "        logits[:, :, 402] = -15.0\n",
    "        return type(\"Output\", (), {\"logits\": logits})\n",
    "\n",
    "class DummyTokenizer:\n",
    "    pad_token_id = 101\n",
    "    cls_token_id = 102\n",
    "    mask_token_id = 103\n",
    "    hash_map_tokens = {\n",
    "        'doctor': 200,\n",
    "        'nurse': 201,\n",
    "        'engineer': 202,\n",
    "        'science': 300,\n",
    "        'art': 301,\n",
    "        'math': 302,\n",
    "        'he': 400,\n",
    "        'she': 401,\n",
    "        'it': 402,\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def __call__(self, sentences, padding=True, return_tensors=\"pt\"):\n",
    "        split = [sentence.split() for sentence in sentences]\n",
    "        maxLen = max([len(sentence) for sentence in split])\n",
    "        ids = [[self.convert_tokens_to_ids(word) for word in sentence] for sentence in split]\n",
    "        if padding:\n",
    "            for i in range(len(ids)):\n",
    "                lenId = len(ids[i])\n",
    "                if lenId < maxLen:\n",
    "                    ids[i] = ids[i] + [self.pad_token_id for _ in range(maxLen - lenId)] \n",
    "        return {\"input_ids\": torch.tensor(ids)}\n",
    "\n",
    "    def tokenize(self, word):\n",
    "        return [word]\n",
    "\n",
    "    def convert_tokens_to_ids(self, token):\n",
    "        return self.hash_map_tokens.get(token, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d3933ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     13\u001b[39m fill_words = [\n\u001b[32m     14\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mengineer\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     15\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdoctor\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmath\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     17\u001b[39m ]\n\u001b[32m     19\u001b[39m mask_indices = [\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m LPBSscore = \u001b[43mLPBS\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_words\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfill_words\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_indices\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_indices\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FairLangProc/FairLangProc/metrics/probability.py:223\u001b[39m, in \u001b[36mLPBS\u001b[39m\u001b[34m(model, tokenizer, sentences, target_words, fill_words, mask_indices)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    221\u001b[39m     mask_indices = [\u001b[32m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentences))]\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m probs = \u001b[43mMaskProbabilityQuotient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m scores = torch.log(probs[\u001b[32m0\u001b[39m]) - torch.log(probs[\u001b[32m1\u001b[39m])\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FairLangProc/FairLangProc/metrics/probability.py:160\u001b[39m, in \u001b[36mMaskProbabilityQuotient\u001b[39m\u001b[34m(model, tokenizer, sentences, target_words, fill_words, mask_indices)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_cat):\n\u001b[32m    158\u001b[39m     words = [word_tuple[cat] \u001b[38;5;28;01mfor\u001b[39;00m word_tuple \u001b[38;5;129;01min\u001b[39;00m target_words]\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     prior_probs = \u001b[43mMaskProbability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow_many\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m     post_probs = MaskProbability(model, tokenizer, filled_sentences, words, mask_indices, how_many = \u001b[32m1\u001b[39m)\n\u001b[32m    162\u001b[39m     prob_quotient = post_probs/prior_probs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FairLangProc/FairLangProc/metrics/probability.py:77\u001b[39m, in \u001b[36mMaskProbability\u001b[39m\u001b[34m(model, tokenizer, sentences, target_words, mask_indices, how_many)\u001b[39m\n\u001b[32m     75\u001b[39m input_ids = tokenizer(sentences, padding = \u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     76\u001b[39m target_ids = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word)[\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m target_words]\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m mask_index = torch.where(\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_ids\u001b[49m == tokenizer.mask_token_id)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     80\u001b[39m     outputs = model(**input_ids)\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'input_ids'"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"[MASK] is a [MASK]\",\n",
    "    \"[MASK] is a [MASK]\",\n",
    "    \"[MASK] teaches [MASK]\"\n",
    "]\n",
    "\n",
    "target_words = [\n",
    "    (\"he\", \"she\"),\n",
    "    (\"he\", \"she\"),\n",
    "    (\"he\", \"she\")\n",
    "]\n",
    "\n",
    "fill_words = [\n",
    "    'engineer',\n",
    "    'doctor',\n",
    "    'math',\n",
    "]\n",
    "\n",
    "mask_indices = [0, 0, 0]\n",
    "\n",
    "\n",
    "LPBSscore = LPBS(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    sentences = sentences,\n",
    "    target_words = target_words,\n",
    "    fill_words = fill_words,\n",
    "    mask_indices = mask_indices\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c535245",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DummyTokenizer()\n",
    "entrada = tokenizer(['hola que haces', 'aquí con el doctor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a704ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DummyModel()\n",
    "model(**entrada).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7df66b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(AutoTokenizer.from_pretrained(\"bert-base-uncased\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb033a",
   "metadata": {},
   "source": [
    "## Test embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a1539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from math import abs\n",
    "\n",
    "import torch\n",
    "import pytest\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")) if \"__file__\" in globals() else os.path.abspath(\"..\")\n",
    "sys.path.insert(0, ROOT_PATH)\n",
    "\n",
    "from FairLangProc.metrics import WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9aacc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyWEAT(WEAT):\n",
    "    def _get_embedding(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "def test_cosine_similarity():\n",
    "    tokenizer = DummyTokenizer()\n",
    "    model = DummyModel()\n",
    "    pass\n",
    "\n",
    "def test_effect_size():\n",
    "    pass\n",
    "\n",
    "def test_metric():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d6ce6",
   "metadata": {},
   "source": [
    "## Test generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bab6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pytest\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")) if \"__file__\" in globals() else os.path.abspath(\"..\")\n",
    "sys.path.insert(0, ROOT_PATH)\n",
    "\n",
    "from FairLangProc.metrics import DemRep, StereoAsoc, HONEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "008f7c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENDERED_WORDS = {\n",
    "    'male': ['he', 'him', 'his'],\n",
    "    'female': ['she', 'her', 'actress', 'hers']\n",
    "    }\n",
    "\n",
    "ATTRIBUTES = GENDERED_WORDS.keys()\n",
    "\n",
    "SENTENCES = [\n",
    "    'She is such a good match to him.',\n",
    "    'He is trying way too hard to be an actor.',\n",
    "    'Her mother is trying to make ends meet.'\n",
    "    'My aunt is baking, do you want to try?'\n",
    "]\n",
    "\n",
    "def test_demographic_representation_type():\n",
    "    DR = DemRep(sentences = SENTENCES, demWords = GENDERED_WORDS)\n",
    "    assert isinstance(DR, dict)\n",
    "\n",
    "def test_demographic_representation_keys():\n",
    "    DR = DemRep(sentences = SENTENCES, demWords = GENDERED_WORDS)\n",
    "    assert len(DR.keys()) == 2\n",
    "\n",
    "def test_demographic_representation_values():\n",
    "    DR = DemRep(sentences = SENTENCES, demWords = GENDERED_WORDS)\n",
    "    assert DR['male'] == 1\n",
    "    assert DR['female'] == 2\n",
    "\n",
    "def test_demographic_representation_empty_demwords():\n",
    "    DR = DemRep(sentences = SENTENCES, demWords = {})\n",
    "    assert DR == {}\n",
    "\n",
    "def test_demographic_representation_empty_sentences():\n",
    "    DR = DemRep(sentences = [], demWords = GENDERED_WORDS)\n",
    "    assert len(DR.keys()) == 2\n",
    "    assert DR['male'] == 0\n",
    "    assert DR['female'] == 0\n",
    "\n",
    "def test_demographic_representation_empty_demwords_sentences():\n",
    "    DR = DemRep(sentences = [], demWords = {})\n",
    "    assert DR == {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb23db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_WORDS = ['mother', 'baking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf23fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stereorep_type():\n",
    "    ST = StereoAsoc(sentences = SENTENCES, demWords = GENDERED_WORDS, targetWords = TARGET_WORDS)\n",
    "    assert isinstance(ST, dict)\n",
    "    for key in ST.keys():\n",
    "        assert isinstance(ST[key], dict)\n",
    "\n",
    "def test_stereorep_keys():\n",
    "    ST = StereoAsoc(sentences = SENTENCES, demWords = GENDERED_WORDS, targetWords = TARGET_WORDS)\n",
    "    assert len(ST.keys()) == 2\n",
    "    for key in ST.keys():\n",
    "        assert len(ST[key].keys()) == 2\n",
    "\n",
    "def test_stereorep_values():\n",
    "    ST = StereoAsoc(sentences = SENTENCES, demWords = GENDERED_WORDS, targetWords = TARGET_WORDS)\n",
    "    assert ST['mother']['male'] == 0\n",
    "    assert ST['mother']['female'] == 1\n",
    "    assert ST['baking']['male'] == 0\n",
    "    assert ST['baking']['female'] == 0\n",
    "\n",
    "def test_stereorep_empty_target():\n",
    "    ST = StereoAsoc(sentences = SENTENCES, demWords = GENDERED_WORDS, targetWords = [])\n",
    "    assert ST == {}\n",
    "\n",
    "def test_stereorep_empty_dem():\n",
    "    ST = StereoAsoc(sentences = SENTENCES, demWords = {}, targetWords = TARGET_WORDS)\n",
    "    assert ST['mother'] == {}\n",
    "    assert ST['baking'] == {}\n",
    "\n",
    "def test_stereorep_empty_sentences():\n",
    "    ST = StereoAsoc(sentences = [], demWords = GENDERED_WORDS, targetWords = TARGET_WORDS)\n",
    "    assert ST['mother']['male'] == 0\n",
    "    assert ST['mother']['female'] == 0\n",
    "    assert ST['baking']['male'] == 0\n",
    "    assert ST['baking']['female'] == 0\n",
    "\n",
    "def test_stereorep_empty_dem_sentences():\n",
    "    ST = StereoAsoc(sentences = [], demWords = {}, targetWords = TARGET_WORDS)\n",
    "    assert ST['mother'] == {}\n",
    "    assert ST['baking'] == {}\n",
    "\n",
    "def test_stereorep_empty_dem_target():\n",
    "    ST = StereoAsoc(sentences = SENTENCES, demWords = {}, targetWords = [])\n",
    "    assert ST == {}\n",
    "\n",
    "def test_stereorep_empty_sentences_target():\n",
    "    ST = StereoAsoc(sentences = [], demWords = GENDERED_WORDS, targetWords = [])\n",
    "    assert ST == {}\n",
    "\n",
    "def test_stereorep_empty_dem_sentences_target():\n",
    "    ST = StereoAsoc(sentences = [], demWords = {}, targetWords = [])\n",
    "    assert ST == {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0596a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETIONS = [\n",
    "    ['he is so stupid', 'he is such a fool', 'he is so misunderstood'],\n",
    "    ['she is so bossy', 'she is an incompetent manager', 'she does what is necessary'],\n",
    "    ['they were so agreeable', 'they were so nice to us', 'they showed hospitality']\n",
    "]\n",
    "\n",
    "COMPLETIONS_DIFFERENT_LENGTH = [\n",
    "    ['he is so stupid', 'he is such a fool', 'he is so misunderstood'],\n",
    "    ['she is so bossy', 'she is an incompetent manager'],\n",
    "    ['they were so agreeable', 'they were so nice to us', 'they showed hospitality']\n",
    "]\n",
    "\n",
    "COMPLETIONS_WITHOUT_LIST = [\n",
    "    ['he is so stupid', 'he is such a fool', 'he is so misunderstood'],\n",
    "    ('she is so bossy', 'she is an incompetent manager', 'she does what is necessary'),\n",
    "    ['they were so agreeable', 'they were so nice to us', 'they showed hospitality']\n",
    "]\n",
    "\n",
    "HURTLEX = ['fool', 'stupid', 'incompetent']\n",
    "\n",
    "def test_honest_type():\n",
    "    honest = HONEST(completions = COMPLETIONS, hurtLex = HURTLEX)\n",
    "    assert isinstance(honest, float)\n",
    "\n",
    "def test_honest_value():\n",
    "    honest = HONEST(completions = COMPLETIONS, hurtLex = HURTLEX)\n",
    "    assert abs(honest - 1/3) < 1e-15\n",
    "\n",
    "def test_honest_empty_hurt():\n",
    "    honest = HONEST(completions = COMPLETIONS, hurtLex = [])\n",
    "    assert abs(honest - 0.0) < 1e-15\n",
    "\n",
    "def test_honest_empty_completions():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        honest = HONEST(completions = [], hurtLex = HURTLEX)\n",
    "    assert \"completions is empty\" in excinfo\n",
    "\n",
    "def test_honest_not_list():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        honest = HONEST(completions = {}, hurtLex = HURTLEX)\n",
    "    assert \"completions is not a list\" in excinfo\n",
    "\n",
    "def test_element_not_list():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        honest = HONEST(completions = COMPLETIONS_WITHOUT_LIST, hurtLex = HURTLEX)\n",
    "    assert \"completions is not a list of lists\" in excinfo\n",
    "\n",
    "def test_honest_different_length():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        honest = HONEST(completions = COMPLETIONS_DIFFERENT_LENGTH, hurtLex = HURTLEX)\n",
    "    assert \"Number of completions is not uniform\" in excinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80077b7e",
   "metadata": {},
   "source": [
    "# Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c52f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
