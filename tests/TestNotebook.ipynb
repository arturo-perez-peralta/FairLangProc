{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f077304",
   "metadata": {},
   "source": [
    "# Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36538d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pytest\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset as PtDataset\n",
    "from datasets import Dataset as HfDataset\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")) if \"__file__\" in globals() else os.path.abspath(\"..\")\n",
    "sys.path.insert(0, ROOT_PATH)\n",
    "\n",
    "from FairLangProc.datasets.fairness_datasets import BiasDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fda3639",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPLEMENTED = [\n",
    "    \"BBQ\",\n",
    "    \"BEC-Pro\",\n",
    "    \"BOLD\",\n",
    "    \"BUG\",\n",
    "    \"CrowS-Pairs\",\n",
    "    \"GAP\",\n",
    "    \"StereoSet\",\n",
    "    \"UnQover\",\n",
    "    \"WinoBias+\",\n",
    "    \"WinoBias\",\n",
    "    \"Winogender\"\n",
    "]\n",
    "\n",
    "DATASETS = [\n",
    "    \"BBQ\",\n",
    "    \"BEC-Pro\",\n",
    "    \"BOLD\",\n",
    "    \"BUG\",\n",
    "    \"Bias-NLI\",\n",
    "    \"CrowS-Pairs\",\n",
    "    \"GAP\",\n",
    "    \"Grep-BiasIR\",\n",
    "    \"HONEST\",\n",
    "    \"HolisticBias\",\n",
    "    \"PANDA\",\n",
    "    \"RedditBias\",\n",
    "    \"StereoSet\",\n",
    "    \"TrustGPT\",\n",
    "    \"UnQover\",\n",
    "    \"WinoBias\",\n",
    "    \"WinoBias+\",\n",
    "    \"WinoQueer\",\n",
    "    \"Winogender\",\n",
    "]\n",
    "\n",
    "REMAINING = [dataset for dataset in DATASETS if dataset not in IMPLEMENTED]\n",
    "\n",
    "CONFIGURATIONS = {\n",
    "    \"BBQ\": [\"Age\", \"Disability_Status\", \"Gender_identity\", \"Nationality\", \"Physical_appearance\", \"Race_ethnicity\", \"Race_x_gender\", \"Race_x_SES\", \"Religion\", \"SES\", \"Sexual_orientation\", \"all\"],\n",
    "    \"BEC-Pro\": [\"english\", \"german\", \"all\"],\n",
    "    \"BOLD\": [\"prompts\", \"wikipedia\", \"all\"],\n",
    "    \"BUG\": [\"balanced\", \"full\", \"gold\", \"all\"],\n",
    "    \"Bias-NLI\": [\"process\", \"load\", \"all\"],\n",
    "    \"CrowS-Pairs\": [\"\"],\n",
    "    \"GAP\": [\"\"],\n",
    "    \"Grep-BiasIR\": [\"queries\", \"documents\", \"relevance\", \"all\"],\n",
    "    \"HolisticBias\": [\"noun_phrases\", \"sentences\", \"all\"],\n",
    "    \"PANDA\": [\"train\", \"test\", \"dev\", \"all\"],\n",
    "    \"RedditBias\": [\"posts\", \"comments\", \"annotations\", \"all\"],\n",
    "    \"StereoSet\": [\"word\", \"sentence\", \"all\"],\n",
    "    \"TrustGPT\": [\"process\", \"load\", \"all\", \"benchmarks\"],\n",
    "    \"UnQover\": [\"questions\", \"answers\", \"annotations\"],\n",
    "    \"WinoBias\": [\"pairs\", \"WinoBias\"],\n",
    "    \"WinoBias+\": [\"\"],\n",
    "    \"WinoQueer\": [\"sentences\", \"templates\", \"annotations\", \"all\"],\n",
    "    \"Winogender\": [\"\"],\n",
    "}\n",
    "\n",
    "FORMATS = [\"hf\", \"pt\", \"raw\"]\n",
    "\n",
    "CLASS_DICT = {\n",
    "    \"hf\": HfDataset,\n",
    "    \"pt\": PtDataset,\n",
    "    \"raw\": pd.DataFrame\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb867e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CASES_FORMAT = [\n",
    "    (dataset, config, format)\n",
    "    for dataset in CONFIGURATIONS.keys()\n",
    "    for config in CONFIGURATIONS[dataset] \n",
    "    for format in FORMATS if dataset in IMPLEMENTED\n",
    "]\n",
    "\n",
    "@pytest.mark.parametrize(\"dataset, config, format\", TEST_CASES_FORMAT)\n",
    "def test_format(dataset, config, format):\n",
    "    result = BiasDataLoader(dataset = dataset, config = config, format = format)\n",
    "    assert isinstance(result, dict)\n",
    "    for key in result:\n",
    "        assert isinstance(result[key], CLASS_DICT[format])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e37c645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_columns():\n",
    "    for dataset in CONFIGURATIONS.keys():\n",
    "        if dataset in IMPLEMENTED:\n",
    "            result = BiasDataLoader(dataset = dataset, config = 'all', format = 'raw')\n",
    "            if result is None:\n",
    "                result = BiasDataLoader(dataset = dataset, config = '', format = 'raw')\n",
    "            try:\n",
    "                print(dataset)\n",
    "                print(list(result[list(result.keys())[0]].keys()))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "def _get_rows():\n",
    "    for dataset in CONFIGURATIONS.keys():\n",
    "        if dataset in IMPLEMENTED:\n",
    "            result = BiasDataLoader(dataset = dataset, config = 'all', format = 'raw')\n",
    "            if result is None:\n",
    "                result = BiasDataLoader(dataset = dataset, config = '', format = 'raw')\n",
    "            try:\n",
    "                string = f\"\\\"{dataset}\\\": {{\"\n",
    "                for data in result.keys():\n",
    "                    if data == 'templates' and dataset == 'BBQ':\n",
    "                        continue\n",
    "                    string += f\"\\\"{data}\\\": {len(result[data].index)}, \"\n",
    "                string += \"}, \"\n",
    "                print(string)\n",
    "            except:\n",
    "                print(dataset + \": nothing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a467f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = {\n",
    "    \"BBQ\": ['example_id', 'question_index', 'question_polarity', 'context_condition', 'category', 'answer_info', 'additional_metadata', 'context', 'question', 'ans0', 'ans1', 'ans2', 'label'],\n",
    "    \"BEC-Pro\": ['Unnamed: 0', 'Sentence', 'Sent_TM', 'Sent_AM', 'Sent_TAM', 'Template', 'Person', 'Gender', 'Profession', 'Prof_Gender'],\n",
    "    \"BOLD\": ['gender_prompt.json', 'political_ideology_prompt.json', 'profession_prompt.json', 'race_prompt.json', 'religious_ideology_prompt.json'],\n",
    "    \"BUG\": ['Unnamed: 0', 'sentence_text', 'tokens', 'profession', 'g', 'profession_first_index', 'g_first_index', 'predicted gender', 'stereotype', 'distance', 'num_of_pronouns', 'corpus', 'data_index'],\n",
    "    \"CrowS-Pairs\": ['Unnamed: 0', 'sent_more', 'sent_less', 'stereo_antistereo', 'bias_type', 'annotations', 'anon_writer', 'anon_annotators'],\n",
    "    \"GAP\": ['ID', 'Text', 'Pronoun', 'Pronoun-offset', 'A', 'A-offset', 'A-coref', 'B', 'B-offset', 'B-coref', 'URL'],\n",
    "    \"HolisticBias\": None,\n",
    "    \"StereoSet\": ['options', 'context', 'target', 'bias_type', 'labels'],\n",
    "    \"WinoBias+\": ['gendered', 'neutral'],\n",
    "    \"WinoBias\": ['sentence', 'entity', 'pronoun'],\n",
    "    \"Winogender\": ['sentid', 'sentence']\n",
    "}\n",
    "\n",
    "TEST_CASES_COLUMNS = list(COLUMNS.keys())\n",
    "\n",
    "@pytest.mark.parametrize(\"dataset\", TEST_CASES_COLUMNS)\n",
    "def test_columns(dataset):\n",
    "    result = BiasDataLoader(dataset = dataset, config = 'all', format = 'raw')\n",
    "    data = result[list(dataset.keys())[0]]\n",
    "    assert len(column) == len(data.columns), \"Different number of columns\"\n",
    "    for column in COLUMNS[dataset]:\n",
    "        assert column in data.columns, \"Missing column\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59239f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = {\n",
    "    \"BBQ\": {\"Age.jsonl\": 3680, \"Disability_status.jsonl\": 1556, \"Gender_identity.jsonl\": 5672, \"Nationality.jsonl\": 3080, \"Physical_appearance.jsonl\": 1576, \"Race_ethnicity.jsonl\": 6880, \"Race_x_SES.jsonl\": 11160, \"Race_x_gender.jsonl\": 15960, \"Religion.jsonl\": 1200, \"SES.jsonl\": 6864, \"Sexual_orientation.jsonl\": 864, \"additional_metadata.csv\": 58556, },\n",
    "    \"BEC-Pro\": {\"english\": 5400, \"german\": 5400, }, \n",
    "    \"BUG\": {\"balanced_BUG.csv\": 25504, \"full_BUG.csv\": 105687, \"gold_BUG.csv\": 1717, }, \n",
    "    \"CrowS-Pairs\": {\"data\": 1508, }, \n",
    "    \"GAP\": {\"gap-development.tsv\": 2000, \"gap-test.tsv\": 2000, \"gap-validation.tsv\": 454, }, \n",
    "    \"StereoSet\": {\"test_sentence\": 6374, \"test_word\": 6392, \"dev_sentence\": 2123, \"dev_word\": 2106, }, \n",
    "    \"WinoBias\": {\"anti_stereotyped_type1.txt.dev\": 396, \"anti_stereotyped_type1.txt.test\": 396, \"anti_stereotyped_type2.txt.dev\": 396, \"anti_stereotyped_type2.txt.test\": 396, \"pro_stereotyped_type1.txt.dev\": 396, \"pro_stereotyped_type1.txt.test\": 396, \"pro_stereotyped_type2.txt.dev\": 396, \"pro_stereotyped_type2.txt.test\": 396, }, \n",
    "    \"WinoBias+\": {\"data\": 3167, }, \n",
    "    \"Winogender\": {\"data\": 720, }, \n",
    "}\n",
    "\n",
    "TEST_CASES_ROWS = list(ROWS.keys())\n",
    "\n",
    "@pytest.mark.parametrize(\"dataset\", TEST_CASES_ROWS)\n",
    "def test_row_number(dataset):\n",
    "    result = BiasDataLoader(dataset = dataset, config = 'all', format = 'raw')\n",
    "    for key in result:\n",
    "        if isinstance(result[key], pd.Dataframe):\n",
    "            assert len(result[key].index) == ROWS[dataset][key]\n",
    "        elif isinstance(result[key], list):\n",
    "            assert len(result[key]) == ROWS[dataset][key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc3b414",
   "metadata": {},
   "source": [
    "# Test metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8a112",
   "metadata": {},
   "source": [
    "## Test probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e45f0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import pytest\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")) if \"__file__\" in globals() else os.path.abspath(\"..\")\n",
    "sys.path.insert(0, ROOT_PATH)\n",
    "\n",
    "from FairLangProc.metrics import LPBS, CBS, CPS, AUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40e5469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({dict.__repr__(self)})\"\n",
    "    \n",
    "\n",
    "class DummyModel:\n",
    "    def assing_logits_mask(self, input_ids, logits, b, t):\n",
    "        input_sum = input_ids[b].sum().item()\n",
    "\n",
    "        # Logic depending on sentence and word\n",
    "        logits[b, t, 200] = 5.0 + input_sum % 3     # doctor\n",
    "        logits[b, t, 201] = -5.0 + input_sum % 4    # nurse\n",
    "        logits[b, t, 202] = 15.0 - input_sum % 5    # engineer\n",
    "\n",
    "        logits[b, t, 300] = 5.0 + input_sum % 2     # science\n",
    "        logits[b, t, 301] = -5.0 + input_sum % 6    # art\n",
    "        logits[b, t, 302] = 15.0 - input_sum % 7    # math\n",
    "\n",
    "        logits[b, t, 400] = 10.0 + input_sum % 3    # he\n",
    "        logits[b, t, 401] = -10.0 + input_sum % 4   # she\n",
    "        logits[b, t, 402] = -15.0 + input_sum % 10  # it\n",
    "\n",
    "\n",
    "    def assing_logits(self, input_ids, logits):\n",
    "        \n",
    "        ids = [200, 201, 202, 300, 301, 302, 400, 401, 402]\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for t in range(seq_len):\n",
    "                if input_ids[b, t] in ids:\n",
    "                    logits[b, t, input_ids[b, t]] = 5.0 + input_ids[b, t] % 3 \n",
    "\n",
    "\n",
    "    def __call__(self, input_ids = None, **kwargs):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        logits = torch.zeros(batch_size, seq_len, 30522)\n",
    "\n",
    "        mask_token_id = 103\n",
    "        mask_positions = (input_ids == mask_token_id)\n",
    "        noMask = mask_positions.sum().item() == 0  \n",
    "\n",
    "        # introduce dependency on input_ids\n",
    "        if noMask:\n",
    "            self.assing_logits(input_ids, logits)\n",
    "        else:\n",
    "            for b in range(batch_size):\n",
    "                for t in range(seq_len):\n",
    "                    if mask_positions[b, t]:\n",
    "                        self.assing_logits_mask(input_ids, logits, b, t)\n",
    "\n",
    "        return AttrDict(logits=logits)\n",
    "    \n",
    "\n",
    "class DummyTokenizer:\n",
    "    pad_token_type_id = 101\n",
    "    pad_token_id = 101\n",
    "    cls_token_id = 102\n",
    "    mask_token_id = 103\n",
    "    hash_map_tokens = {\n",
    "        '[PAD]': pad_token_type_id,\n",
    "        '[CLS]': cls_token_id,\n",
    "        '[MASK]': mask_token_id,\n",
    "        'doctor': 200,\n",
    "        'nurse': 201,\n",
    "        'engineer': 202,\n",
    "        'science': 300,\n",
    "        'art': 301,\n",
    "        'math': 302,\n",
    "        'he': 400,\n",
    "        'she': 401,\n",
    "        'it': 402,\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def __call__(self, sentences, padding=True, return_tensors=\"pt\"):\n",
    "        split = [[\"[CLS]\"] + sentence.split() for sentence in sentences]\n",
    "        maxLen = max([len(sentence) for sentence in split])\n",
    "        ids = [[self.convert_tokens_to_ids(word) for word in sentence] for sentence in split]\n",
    "        if padding:\n",
    "            for i in range(len(ids)):\n",
    "                ids[i] += [self.pad_token_id] * (maxLen - len(ids[i]))\n",
    "\n",
    "        return AttrDict(**{\"input_ids\": torch.tensor(ids)})\n",
    "\n",
    "    def tokenize(self, word):\n",
    "        return [word]\n",
    "\n",
    "    def convert_tokens_to_ids(self, token):\n",
    "        return self.hash_map_tokens.get(token, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d3933ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = DummyModel()\n",
    "TOKENIZER = DummyTokenizer()\n",
    "\n",
    "SENTENCES = [\n",
    "    \"[MASK] is a [MASK]\",\n",
    "    \"Is [MASK] a [MASK] ?\",\n",
    "    \"[MASK] teaches [MASK]\"\n",
    "]\n",
    "\n",
    "TARGET_WORDS_LPBS = [\n",
    "    (\"he\", \"she\"),\n",
    "    (\"he\", \"she\"),\n",
    "    (\"he\", \"she\")\n",
    "]\n",
    "\n",
    "TARGET_WORDS_CBS = [\n",
    "    (\"he\", \"she\", \"it\"),\n",
    "    (\"he\", \"she\", \"it\"),\n",
    "    (\"he\", \"she\", \"it\")\n",
    "]\n",
    "\n",
    "FILL_WORDS = [\n",
    "    'engineer',\n",
    "    'doctor',\n",
    "    'math',\n",
    "]\n",
    "\n",
    "MASK_INDICES = [0, 0, 0]\n",
    "\n",
    "def test_lpbs_type():\n",
    "    LPBSscore = LPBS(\n",
    "        model = MODEL,\n",
    "        tokenizer = TOKENIZER,\n",
    "        sentences = SENTENCES,\n",
    "        target_words = TARGET_WORDS_LPBS,\n",
    "        fill_words = FILL_WORDS,\n",
    "        mask_indices = MASK_INDICES\n",
    "    )\n",
    "    assert isinstance(LPBSscore, torch.Size)\n",
    "    assert LPBSscore.shape == torch.Size([3])\n",
    "\n",
    "def test_lpbs_value():\n",
    "    LPBSscore = LPBS(\n",
    "        model = MODEL,\n",
    "        tokenizer = TOKENIZER,\n",
    "        sentences = SENTENCES,\n",
    "        target_words = TARGET_WORDS_LPBS,\n",
    "        fill_words = FILL_WORDS,\n",
    "        mask_indices = MASK_INDICES\n",
    "    )\n",
    "      \n",
    "    assert abs(LPBSscore[0].item() - 1.0) < 1e-5\n",
    "    assert abs(LPBSscore[1].item() - (-3.0)) < 1e-5\n",
    "    assert abs(LPBSscore[2].item() - (-2.0)) < 1e-5\n",
    "\n",
    "def test_lpbs_less_target():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        LPBSscore = LPBS(\n",
    "            model = MODEL,\n",
    "            tokenizer = TOKENIZER,\n",
    "            sentences = SENTENCES[:1],\n",
    "            target_words = TARGET_WORDS_LPBS,\n",
    "            fill_words = FILL_WORDS,\n",
    "            mask_indices = MASK_INDICES\n",
    "        )\n",
    "    assert \"Different number of sentences\" in excinfo\n",
    "\n",
    "def test_lpbs_less_target():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        LPBSscore = LPBS(\n",
    "            model = MODEL,\n",
    "            tokenizer = TOKENIZER,\n",
    "            sentences = SENTENCES,\n",
    "            target_words = TARGET_WORDS_LPBS[:1],\n",
    "            fill_words = FILL_WORDS,\n",
    "            mask_indices = MASK_INDICES\n",
    "        )\n",
    "    assert \"Different number of sentences and target words\" in excinfo\n",
    "\n",
    "def test_lpbs_less_fill():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        LPBSscore = LPBS(\n",
    "            model = MODEL,\n",
    "            tokenizer = TOKENIZER,\n",
    "            sentences = SENTENCES,\n",
    "            target_words = TARGET_WORDS_LPBS,\n",
    "            fill_words = FILL_WORDS[:1],\n",
    "            mask_indices = MASK_INDICES\n",
    "        )\n",
    "    assert \"Different number of sentences and fill words\" in excinfo\n",
    "\n",
    "def test_lpbs_less_masks():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        LPBSscore = LPBS(\n",
    "            model = MODEL,\n",
    "            tokenizer = TOKENIZER,\n",
    "            sentences = SENTENCES,\n",
    "            target_words = TARGET_WORDS_LPBS,\n",
    "            fill_words = FILL_WORDS,\n",
    "            mask_indices = MASK_INDICES[:1]\n",
    "        )\n",
    "    assert \"Different number of sentences and mask indices\" in excinfo\n",
    "\n",
    "def test_lpbs_no_pairs():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        LPBSscore = LPBS(\n",
    "            model = MODEL,\n",
    "            tokenizer = TOKENIZER,\n",
    "            sentences = SENTENCES,\n",
    "            target_words = [('he', 'she', 'it')]*3,\n",
    "            fill_words = FILL_WORDS,\n",
    "            mask_indices = MASK_INDICES\n",
    "        )\n",
    "    assert \"Target words must consist of pairs of words\" in excinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c535245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cbs_type():\n",
    "    CBSscore = CBS(\n",
    "        model = MODEL,\n",
    "        tokenizer = TOKENIZER,\n",
    "        sentences = SENTENCES,\n",
    "        target_words = TARGET_WORDS_CBS,\n",
    "        fill_words = FILL_WORDS,\n",
    "        mask_indices = MASK_INDICES\n",
    "    )\n",
    "    assert isinstance(CBSscore, torch.Size)\n",
    "    assert CBSscore.shape == torch.Size([3])\n",
    "\n",
    "def test_cbs_value():\n",
    "    CBSscore = CBS(\n",
    "        model = MODEL,\n",
    "        tokenizer = TOKENIZER,\n",
    "        sentences = SENTENCES,\n",
    "        target_words = TARGET_WORDS_CBS,\n",
    "        fill_words = FILL_WORDS,\n",
    "        mask_indices = MASK_INDICES\n",
    "    )\n",
    "      \n",
    "    assert abs(CBSscore[0].item() - 1/3) < 1e-5\n",
    "    assert abs(CBSscore[1].item() - 13/3) < 1e-5\n",
    "    assert abs(CBSscore[2].item() - 4.0) < 1e-5\n",
    "\n",
    "def test_cbs_less_target():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        CBSscore = CBS(\n",
    "            model = MODEL,\n",
    "            tokenizer = TOKENIZER,\n",
    "            sentences = SENTENCES[:1],\n",
    "            target_words = TARGET_WORDS_CBS,\n",
    "            fill_words = FILL_WORDS,\n",
    "            mask_indices = MASK_INDICES\n",
    "        )\n",
    "    assert \"Different number of sentences\" in excinfo\n",
    "\n",
    "def test_cbs_less_target():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        CBSscore = CBS(\n",
    "            model = MODEL,\n",
    "            tokenizer = TOKENIZER,\n",
    "            sentences = SENTENCES,\n",
    "            target_words = TARGET_WORDS_CBS[:1],\n",
    "            fill_words = FILL_WORDS,\n",
    "            mask_indices = MASK_INDICES\n",
    "        )\n",
    "    assert \"Different number of sentences and target words\" in excinfo\n",
    "\n",
    "def test_cbs_less_fill():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        CBSscore = CBS(\n",
    "            model = MODEL,\n",
    "            tokenizer = TOKENIZER,\n",
    "            sentences = SENTENCES,\n",
    "            target_words = TARGET_WORDS_CBS,\n",
    "            fill_words = FILL_WORDS[:1],\n",
    "            mask_indices = MASK_INDICES\n",
    "        )\n",
    "    assert \"Different number of sentences and fill words\" in excinfo\n",
    "\n",
    "def test_cbs_less_masks():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        CBSscore = CBS(\n",
    "            model = MODEL,\n",
    "            tokenizer = TOKENIZER,\n",
    "            sentences = SENTENCES,\n",
    "            target_words = TARGET_WORDS_CBS,\n",
    "            fill_words = FILL_WORDS,\n",
    "            mask_indices = MASK_INDICES[:1]\n",
    "        )\n",
    "    assert \"Different number of sentences and mask indices\" in excinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a704ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLL_SENTENCES = [\n",
    "    'he is an exemplary doctor',\n",
    "    'she is an exemplary doctor',\n",
    "    'it is an exemplary doctor',\n",
    "]\n",
    "\n",
    "TARGET_WORDS_CPS = [\n",
    "    'doctor',\n",
    "    'doctor',\n",
    "    'doctor',\n",
    "]\n",
    "\n",
    "def test_cps_type():\n",
    "    CPSscore = CPS(\n",
    "        model = MODEL,\n",
    "        tokenizer = TOKENIZER,\n",
    "        sentences = PLL_SENTENCES,\n",
    "        target_words = TARGET_WORDS_CPS[:1]\n",
    "    )\n",
    "    assert isinstance(CPSscore, list)\n",
    "    assert len(CPSscore) == 3\n",
    "\n",
    "def test_cps_value():\n",
    "    CPSscore = CPS(\n",
    "        model = MODEL,\n",
    "        tokenizer = TOKENIZER,\n",
    "        sentences = PLL_SENTENCES,\n",
    "        target_words = TARGET_WORDS_CPS[:1]\n",
    "    )\n",
    "    assert abs(CPSscore[0] - ( -5.3755054473)) < 1e-6\n",
    "    assert abs(CPSscore[1] - (-15.9847412109)) < 1e-6\n",
    "    assert abs(CPSscore[2] - (-16.9847259521)) < 1e-6\n",
    "\n",
    "def test_cps_less_target():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        CPSscore = CPS(\n",
    "            model = MODEL,\n",
    "            tokenizer = TOKENIZER,\n",
    "            sentences = PLL_SENTENCES,\n",
    "            target_words = TARGET_WORDS_CPS[:1]\n",
    "        )\n",
    "    assert \"Number of sentences and target words must be the same\" in excinfo\n",
    "\n",
    "def test_cps_less_sentences():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        CPSscore = CPS(\n",
    "            model = MODEL,\n",
    "            tokenizer = TOKENIZER,\n",
    "            sentences = PLL_SENTENCES[:1],\n",
    "            target_words = TARGET_WORDS_CPS\n",
    "        )\n",
    "    assert \"Number of sentences and target words must be the same\" in excinfo\n",
    "\n",
    "def test_cps_empty_sentences():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        CPSscore = CPS(\n",
    "            model = MODEL,\n",
    "            tokenizer = TOKENIZER,\n",
    "            sentences = [],\n",
    "            target_words = []\n",
    "        )\n",
    "    assert \"Empty sentence list\" in excinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebfa99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_aul_type():\n",
    "    AULscore = AUL(\n",
    "        model = MODEL,\n",
    "        tokenizer = TOKENIZER,\n",
    "        sentences = PLL_SENTENCES\n",
    "    )\n",
    "    assert isinstance(AULscore, list)\n",
    "    assert len(AULscore) == 3\n",
    "\n",
    "def test_aul_value():\n",
    "    AULscore = AUL(\n",
    "        model = MODEL,\n",
    "        tokenizer = TOKENIZER,\n",
    "        sentences = PLL_SENTENCES\n",
    "    )\n",
    "    assert abs(AULscore[0] - (-1.3468990325)) < 1e-6\n",
    "    assert abs(AULscore[1] - (-1.3449568748)) < 1e-6\n",
    "    assert abs(AULscore[2] - (-1.3521032333)) < 1e-6\n",
    "\n",
    "def test_aul_empty_sentences():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        AULscore = AUL(\n",
    "            model = MODEL,\n",
    "            tokenizer = TOKENIZER,\n",
    "            sentences = []\n",
    "        )\n",
    "    assert \"Empty sentence list\" in excinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb033a",
   "metadata": {},
   "source": [
    "## Test embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "52a1539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import pytest\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")) if \"__file__\" in globals() else os.path.abspath(\"..\")\n",
    "sys.path.insert(0, ROOT_PATH)\n",
    "\n",
    "from FairLangProc.metrics import WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "79ec80e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({dict.__repr__(self)})\"\n",
    "    \n",
    "    def to(self, device):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class DummyModel:\n",
    "    privileged_tokens = [200, 201, 202, 300, 301, 302, 400, 401, 402, 500, 501, 502]\n",
    "\n",
    "    def primitive_embedding(self, id):\n",
    "        remainder = id % 100\n",
    "        output = torch.Tensor([0,0])\n",
    "        if remainder == 2:\n",
    "            output = torch.Tensor([1, 0])\n",
    "        elif remainder == 3:\n",
    "            output = torch.Tensor([-1,0])\n",
    "        elif remainder == 4:\n",
    "            output = torch.Tensor([0, 1])\n",
    "        elif remainder == 5:\n",
    "            output = torch.Tensor([0,-1])\n",
    "        return output\n",
    "\n",
    "    def __call__(self, input_ids = None, output_hidden_states=True, **kwargs):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        embedding = torch.zeros(batch_size, 2)\n",
    "        for b in range(batch_size):\n",
    "            for s in range(seq_len):\n",
    "                embedding[b] += self.primitive_embedding(input_ids[b, s])\n",
    "        return AttrDict(embedding=embedding)\n",
    "    \n",
    "    def to(self, device):\n",
    "        return self\n",
    "    \n",
    "    def eval(self):\n",
    "        return\n",
    "    \n",
    "\n",
    "class DummyTokenizer:\n",
    "    pad_token_type_id = 101\n",
    "    pad_token = 101\n",
    "    cls_token_id = 102\n",
    "    mask_token_id = 103\n",
    "    hash_map_tokens = {\n",
    "        '[PAD]': pad_token_type_id,\n",
    "        '[CLS]': cls_token_id,\n",
    "        '[MASK]': mask_token_id,\n",
    "        'secretary': 200,\n",
    "        'nurse': 201,\n",
    "        'teacher':202,\n",
    "        'engineer': 300,\n",
    "        'firefighter': 301,\n",
    "        'banker': 302,\n",
    "        'he': 400,\n",
    "        'actor': 401,\n",
    "        'son': 402,\n",
    "        'she': 500,\n",
    "        'actress': 501,\n",
    "        'daughter': 502\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def __call__(self, sentences, padding=True, return_tensors=\"pt\"):\n",
    "        split = [sentence.split() for sentence in sentences]\n",
    "        maxLen = max([len(sentence) for sentence in split])\n",
    "        ids = [[self.convert_tokens_to_ids(word) for word in sentence] for sentence in split]\n",
    "        if padding:\n",
    "            for i in range(len(ids)):\n",
    "                lenId = len(ids[i])\n",
    "                if lenId < maxLen:\n",
    "                    ids[i] = ids[i] + [self.pad_token_id for _ in range(maxLen - lenId)] \n",
    "        return AttrDict(input_ids = torch.tensor(ids))\n",
    "\n",
    "    def tokenize(self, word):\n",
    "        return [word]\n",
    "\n",
    "    def convert_tokens_to_ids(self, token):\n",
    "        return self.hash_map_tokens.get(token, 100)\n",
    "    \n",
    "    def to(self, device):\n",
    "        return self\n",
    "\n",
    "class DummyWEAT(WEAT):\n",
    "    def _get_embedding(self, outputs):\n",
    "        return outputs.embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9c9afbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = DummyModel()\n",
    "TOKENIZER = DummyTokenizer()\n",
    "TEST_WEAT = DummyWEAT(model = MODEL, tokenizer = TOKENIZER)\n",
    "\n",
    "X = torch.tensor([[1]*12+[0]*6, [0]*6+[1]*12], dtype = float).transpose(0, 1)\n",
    "Y = torch.tensor([[1,-1]*2+[0]*2, [0]*2+[1]*3 + [-1]], dtype = float).transpose(0, 1)\n",
    "\n",
    "COSXY = [[1.0, -1.0, 1/sqrt(2), -1/sqrt(2), 0.0, 0.0]]*6 \\\n",
    "    + [[1/sqrt(2), -1/sqrt(2), 1.0, 0.0, 1/sqrt(2), -1/sqrt(2)]]*6 \\\n",
    "    + [[0.0, 0.0, 1/sqrt(2), 1/sqrt(2), 1.0, -1.0]]*6\n",
    "COSXY = torch.tensor(COSXY)\n",
    "\n",
    "XEFFECT =  torch.tensor([[1]*4+[0]*2, [0]*2+[1]*4], dtype = float).transpose(0, 1)\n",
    "YEFFECT = -torch.tensor([[1]*4+[0]*2, [0]*2+[1]*4], dtype = float).transpose(0, 1)\n",
    "AEFFECT =  torch.tensor([[-1]*4+[0]*2, [0]*2+[1]*4], dtype = float).transpose(0, 1)\n",
    "BEFFECT = -torch.tensor([[1]*4+[0]*2, [0]*2+[1]*4], dtype = float).transpose(0, 1)\n",
    "\n",
    "WORDSX = ['he', 'actor', 'son']\n",
    "WORDSY = ['she', 'actress', 'daughter']\n",
    "WORDSA = ['banker', 'engineer', 'firefighter']\n",
    "WORDSB = ['secretary', 'nurse', 'teacher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9aacc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_type_cosine_similarity():\n",
    "    X = torch.tensor([[1]*12+[0]*6, [0]*6+[1]*12], dtype = float).transpose(0,1)\n",
    "    Y = torch.tensor([([1,-1]*2+[0]*2)*3, ([0]*2+[1]*4)*3], dtype = float).transpose(0,1)\n",
    "    output = TEST_WEAT.cosine_similarity(X, Y)\n",
    "    assert isinstance(output, torch.Tensor)\n",
    "    assert output.shape[0] == 18\n",
    "    assert output.shape[1] == 6\n",
    "\n",
    "def test_value_cosine_similarity():\n",
    "    X = torch.tensor([[1]*12+[0]*6, [0]*6+[1]*12], dtype = float).transpose(0,1)\n",
    "    Y = torch.tensor([([1,-1]*2+[0]*2)*3, ([0]*2+[1]*4)*3], dtype = float).transpose(0,1)\n",
    "    output = TEST_WEAT.cosine_similarity(X, Y)\n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            assert abs(output[i,j] - COSXY[i,j]) < 1e-7\n",
    "\n",
    "def test_type_effect_size():\n",
    "    result = TEST_WEAT.effect_size(XEFFECT, YEFFECT, AEFFECT, BEFFECT)\n",
    "    assert isinstance(result, float)\n",
    "\n",
    "def test_value_effect_size():\n",
    "    result = TEST_WEAT.effect_size(XEFFECT, YEFFECT, AEFFECT, BEFFECT)\n",
    "    assert abs(result - 1.699794717779) < 1e-7\n",
    "\n",
    "def test_type_metric():\n",
    "    result = TEST_WEAT.metric(WORDSX, WORDSY, WORDSA, WORDSB)\n",
    "    assert isinstance(result, dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d6ce6",
   "metadata": {},
   "source": [
    "## Test generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bab6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pytest\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")) if \"__file__\" in globals() else os.path.abspath(\"..\")\n",
    "sys.path.insert(0, ROOT_PATH)\n",
    "\n",
    "from FairLangProc.metrics import DemRep, StereoAsoc, HONEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "008f7c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENDERED_WORDS = {\n",
    "    'male': ['he', 'him', 'his'],\n",
    "    'female': ['she', 'her', 'actress', 'hers']\n",
    "    }\n",
    "\n",
    "ATTRIBUTES = GENDERED_WORDS.keys()\n",
    "\n",
    "SENTENCES = [\n",
    "    'She is such a good match to him.',\n",
    "    'He is trying way too hard to be an actor.',\n",
    "    'Her mother is trying to make ends meet.'\n",
    "    'My aunt is baking, do you want to try?'\n",
    "]\n",
    "\n",
    "def test_demographic_representation_type():\n",
    "    DR = DemRep(sentences = SENTENCES, demWords = GENDERED_WORDS)\n",
    "    assert isinstance(DR, dict)\n",
    "\n",
    "def test_demographic_representation_keys():\n",
    "    DR = DemRep(sentences = SENTENCES, demWords = GENDERED_WORDS)\n",
    "    assert len(DR.keys()) == 2\n",
    "\n",
    "def test_demographic_representation_values():\n",
    "    DR = DemRep(sentences = SENTENCES, demWords = GENDERED_WORDS)\n",
    "    assert DR['male'] == 1\n",
    "    assert DR['female'] == 2\n",
    "\n",
    "def test_demographic_representation_empty_demwords():\n",
    "    DR = DemRep(sentences = SENTENCES, demWords = {})\n",
    "    assert DR == {}\n",
    "\n",
    "def test_demographic_representation_empty_sentences():\n",
    "    DR = DemRep(sentences = [], demWords = GENDERED_WORDS)\n",
    "    assert len(DR.keys()) == 2\n",
    "    assert DR['male'] == 0\n",
    "    assert DR['female'] == 0\n",
    "\n",
    "def test_demographic_representation_empty_demwords_sentences():\n",
    "    DR = DemRep(sentences = [], demWords = {})\n",
    "    assert DR == {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb23db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_WORDS = ['mother', 'baking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf23fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stereorep_type():\n",
    "    ST = StereoAsoc(sentences = SENTENCES, demWords = GENDERED_WORDS, targetWords = TARGET_WORDS)\n",
    "    assert isinstance(ST, dict)\n",
    "    for key in ST.keys():\n",
    "        assert isinstance(ST[key], dict)\n",
    "\n",
    "def test_stereorep_keys():\n",
    "    ST = StereoAsoc(sentences = SENTENCES, demWords = GENDERED_WORDS, targetWords = TARGET_WORDS)\n",
    "    assert len(ST.keys()) == 2\n",
    "    for key in ST.keys():\n",
    "        assert len(ST[key].keys()) == 2\n",
    "\n",
    "def test_stereorep_values():\n",
    "    ST = StereoAsoc(sentences = SENTENCES, demWords = GENDERED_WORDS, targetWords = TARGET_WORDS)\n",
    "    assert ST['mother']['male'] == 0\n",
    "    assert ST['mother']['female'] == 1\n",
    "    assert ST['baking']['male'] == 0\n",
    "    assert ST['baking']['female'] == 0\n",
    "\n",
    "def test_stereorep_empty_target():\n",
    "    ST = StereoAsoc(sentences = SENTENCES, demWords = GENDERED_WORDS, targetWords = [])\n",
    "    assert ST == {}\n",
    "\n",
    "def test_stereorep_empty_dem():\n",
    "    ST = StereoAsoc(sentences = SENTENCES, demWords = {}, targetWords = TARGET_WORDS)\n",
    "    assert ST['mother'] == {}\n",
    "    assert ST['baking'] == {}\n",
    "\n",
    "def test_stereorep_empty_sentences():\n",
    "    ST = StereoAsoc(sentences = [], demWords = GENDERED_WORDS, targetWords = TARGET_WORDS)\n",
    "    assert ST['mother']['male'] == 0\n",
    "    assert ST['mother']['female'] == 0\n",
    "    assert ST['baking']['male'] == 0\n",
    "    assert ST['baking']['female'] == 0\n",
    "\n",
    "def test_stereorep_empty_dem_sentences():\n",
    "    ST = StereoAsoc(sentences = [], demWords = {}, targetWords = TARGET_WORDS)\n",
    "    assert ST['mother'] == {}\n",
    "    assert ST['baking'] == {}\n",
    "\n",
    "def test_stereorep_empty_dem_target():\n",
    "    ST = StereoAsoc(sentences = SENTENCES, demWords = {}, targetWords = [])\n",
    "    assert ST == {}\n",
    "\n",
    "def test_stereorep_empty_sentences_target():\n",
    "    ST = StereoAsoc(sentences = [], demWords = GENDERED_WORDS, targetWords = [])\n",
    "    assert ST == {}\n",
    "\n",
    "def test_stereorep_empty_dem_sentences_target():\n",
    "    ST = StereoAsoc(sentences = [], demWords = {}, targetWords = [])\n",
    "    assert ST == {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0596a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETIONS = [\n",
    "    ['he is so stupid', 'he is such a fool', 'he is so misunderstood'],\n",
    "    ['she is so bossy', 'she is an incompetent manager', 'she does what is necessary'],\n",
    "    ['they were so agreeable', 'they were so nice to us', 'they showed hospitality']\n",
    "]\n",
    "\n",
    "COMPLETIONS_DIFFERENT_LENGTH = [\n",
    "    ['he is so stupid', 'he is such a fool', 'he is so misunderstood'],\n",
    "    ['she is so bossy', 'she is an incompetent manager'],\n",
    "    ['they were so agreeable', 'they were so nice to us', 'they showed hospitality']\n",
    "]\n",
    "\n",
    "COMPLETIONS_WITHOUT_LIST = [\n",
    "    ['he is so stupid', 'he is such a fool', 'he is so misunderstood'],\n",
    "    ('she is so bossy', 'she is an incompetent manager', 'she does what is necessary'),\n",
    "    ['they were so agreeable', 'they were so nice to us', 'they showed hospitality']\n",
    "]\n",
    "\n",
    "HURTLEX = ['fool', 'stupid', 'incompetent']\n",
    "\n",
    "def test_honest_type():\n",
    "    honest = HONEST(completions = COMPLETIONS, hurtLex = HURTLEX)\n",
    "    assert isinstance(honest, float)\n",
    "\n",
    "def test_honest_value():\n",
    "    honest = HONEST(completions = COMPLETIONS, hurtLex = HURTLEX)\n",
    "    assert abs(honest - 1/3) < 1e-15\n",
    "\n",
    "def test_honest_empty_hurt():\n",
    "    honest = HONEST(completions = COMPLETIONS, hurtLex = [])\n",
    "    assert abs(honest - 0.0) < 1e-15\n",
    "\n",
    "def test_honest_empty_completions():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        honest = HONEST(completions = [], hurtLex = HURTLEX)\n",
    "    assert \"completions is empty\" in excinfo\n",
    "\n",
    "def test_honest_not_list():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        honest = HONEST(completions = {}, hurtLex = HURTLEX)\n",
    "    assert \"completions is not a list\" in excinfo\n",
    "\n",
    "def test_element_not_list():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        honest = HONEST(completions = COMPLETIONS_WITHOUT_LIST, hurtLex = HURTLEX)\n",
    "    assert \"completions is not a list of lists\" in excinfo\n",
    "\n",
    "def test_honest_different_length():\n",
    "    with pytest.raises(AssertionError) as excinfo:\n",
    "        honest = HONEST(completions = COMPLETIONS_DIFFERENT_LENGTH, hurtLex = HURTLEX)\n",
    "    assert \"Number of completions is not uniform\" in excinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80077b7e",
   "metadata": {},
   "source": [
    "# Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c52f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pytest\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")) if \"__file__\" in globals() else os.path.abspath(\"..\")\n",
    "sys.path.insert(0, ROOT_PATH)\n",
    "\n",
    "from FairLangProc.algorithms.preprocessors import CDA, BLINDTrainer, SentDebiasForSequenceClassification\n",
    "from FairLangProc.algorithms.inprocessors import EARModel, DebiasAdapter, selective_unfreezing \n",
    "from FairLangProc.algorithms.intraprocessors import add_EAT_hook, DiffPrunBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbffc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCES = [\n",
    "    'he is a good father',\n",
    "    'the actor gave a staggering performance',\n",
    "    'she tries very hard'\n",
    "]\n",
    "LABELS = [0, 1, 0]\n",
    "BATCH = {'sentence': SENTENCES, 'label': LABELS}\n",
    "PAIRS = {'he': 'she', 'actor': 'actress', 'father': 'mother'}\n",
    "\n",
    "def test_cda_bidirectional():\n",
    "    result = CDA(batch = BATCH, pairs = PAIRS, bidirectional = True)\n",
    "    assert isinstance(result, dict), f\"Wrong type: expected {dict}, got {type(result)}\"\n",
    "    assert len(result['sentence']) == 5, f\"Expected 5 sentences, got {len(result['sentence'])}\"\n",
    "    assert len(result['label']) == 5, f\"Expected 5 labels, got {len(result['label'])}\"\n",
    "\n",
    "def test_cda_no_bidirectional():\n",
    "    result = CDA(batch = BATCH, pairs = PAIRS, bidirectional = False)\n",
    "    assert isinstance(result, dict), f\"Wrong type: expected {dict}, got {type(result)}\"\n",
    "    assert len(result['sentence']) == 3, f\"Expected 3 sentences, got {len(result['sentence'])}\"\n",
    "    assert len(result['label']) == 3, f\"Expected 3 labels, got {len(result['label'])}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57cbf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLINDBERTTrainer(BLINDTrainer):\n",
    "    def _get_embedding(self, inputs):\n",
    "        return self.model.bert(\n",
    "            input_ids = inputs.get(\"input_ids\"), attention_mask = inputs.get(\"attention_mask\"), token_type_ids = inputs.get(\"token_type_ids\")\n",
    "            ).last_hidden_state[:,0,:]\n",
    "\n",
    "def test_blind_model_output_shape():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "    sample = tokenizer(\"A test sentence.\", return_tensors=\"pt\")\n",
    "    sample[\"labels\"] = torch.tensor([1])\n",
    "\n",
    "    # Dummy blind classifier\n",
    "    blind_classifier = torch.nn.Sequential(\n",
    "        torch.nn.Linear(768, 768),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(768, 2)\n",
    "    )\n",
    "\n",
    "    # Dummy training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/tmp/test_blind\",\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=1,\n",
    "        no_cuda=True\n",
    "    )\n",
    "\n",
    "    # Instantiate trainer\n",
    "    trainer = BLINDBERTTrainer(\n",
    "        model=model,\n",
    "        blind_model=blind_classifier,\n",
    "        args=training_args,\n",
    "        train_dataset=[sample],\n",
    "        eval_dataset=[sample],\n",
    "    )\n",
    "\n",
    "    # Forward pass\n",
    "    embedding = trainer._get_embedding(sample)\n",
    "    logits_blind = trainer.blind_model(embedding)\n",
    "\n",
    "    assert logits_blind.shape == (1, 2), f\"Expected blind logits of shape (1, 2), got {logits_blind.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09495b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = [\"attention.self\", \"attention.output\"]\n",
    "\n",
    "def test_unfreezing():\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "    selective_unfreezing(base_model, LAYERS)\n",
    "    for name, param in base_model.named_parameters():\n",
    "        if any(layer_key in name for layer_key in LAYERS):\n",
    "            assert param.requires_grad, f\"Expected param '{name}' to be trainable\"\n",
    "        else:\n",
    "            assert not param.requires_grad, f\"Expected param '{name}' to be frozen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hook():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "    pre_hook_counts = [\n",
    "        len(layer.attention.self._forward_hooks)\n",
    "        for layer in model.base_model.encoder.layer\n",
    "    ]\n",
    "\n",
    "    add_EAT_hook(model, beta=1.2)\n",
    "    post_hook_counts = [\n",
    "        len(layer.attention.self._forward_hooks)\n",
    "        for layer in model.base_model.encoder.layer\n",
    "    ]\n",
    "\n",
    "    for pre, post in zip(pre_hook_counts, post_hook_counts):\n",
    "        assert post == pre + 1, f\"Expected 1 new hook, but got {post - pre}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "NUM_LABELS = 2\n",
    "\n",
    "TEST_CASES_OUTPUT = [\n",
    "    \"emb\",\n",
    "    \"ear\",\n",
    "    \"selective\",\n",
    "    \"adele\",\n",
    "    \"diff\",\n",
    "    \"eat\"\n",
    "]\n",
    "\n",
    "PAIRS = [('actor', 'actress'), ('son', 'daughter'), ('father', 'mother'), ('he', 'she')]\n",
    "\n",
    "\n",
    "class SentDebiasBert(SentDebiasForSequenceClassification):        \n",
    "    def _get_embedding(self, input_ids, attention_mask = None, token_type_ids = None):\n",
    "        return self.model.bert(\n",
    "            input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids\n",
    "            ).last_hidden_state[:,0,:]\n",
    "\n",
    "def get_model(base_model, tokenizer, debias):\n",
    "    \n",
    "    if debias == \"emb\":\n",
    "        model = SentDebiasBert(\n",
    "            model = base_model,\n",
    "            config = None,\n",
    "            tokenizer = tokenizer,\n",
    "            word_pairs = PAIRS,\n",
    "            n_components = 1,\n",
    "            n_labels = NUM_LABELS\n",
    "        )\n",
    "    \n",
    "    elif debias == \"ear\":\n",
    "        model = EARModel(\n",
    "            model = base_model,\n",
    "            ear_reg_strength = 0.01\n",
    "        )\n",
    "\n",
    "    elif debias == \"selective\":\n",
    "        model = base_model\n",
    "        selective_unfreezing(model, LAYERS)\n",
    "\n",
    "    elif debias == \"adele\":\n",
    "        DebiasAdapter = DebiasAdapter(model = base_model)\n",
    "        model = DebiasAdapter.get_model()\n",
    "\n",
    "    elif debias == \"diff\":\n",
    "        tokens_male = [words[0] for words in PAIRS]\n",
    "        tokens_female = [words[1] for words in PAIRS]\n",
    "        inputs_male = tokenizer(tokens_male, padding = True, return_tensors = \"pt\")\n",
    "        inputs_female = tokenizer(tokens_female, padding = True, return_tensors = \"pt\")\n",
    "        model = DiffPrunBERT(\n",
    "            head = base_model.classifier,\n",
    "            encoder = base_model.bert,\n",
    "            loss_fn = torch.nn.CrossEntropyLoss(),\n",
    "            input_ids_A = inputs_male,\n",
    "            input_ids_B = inputs_female,\n",
    "            bias_kernel = None,\n",
    "            upper = 10,\n",
    "            lower = -0.001,\n",
    "            lambda_bias = 0.5,\n",
    "            lambda_sparse = 0.00001\n",
    "        )\n",
    "\n",
    "    elif debias == \"eat\":\n",
    "        model = base_model\n",
    "        add_EAT_hook(model, beta=0.7)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"debias\", TEST_CASES_OUTPUT)\n",
    "def test_sequence_classification_output_shape(debias):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "    model = get_model(base_model, tokenizer, debias)\n",
    "\n",
    "    inputs = tokenizer(\"This is a test sentence.\", return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    assert outputs.logits.shape == (1, NUM_LABELS), \\\n",
    "        f\"Expected logits shape (1, {NUM_LABELS}), but got {outputs.logits.shape}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FairLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
