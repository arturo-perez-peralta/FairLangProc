{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will showcase the different fairness processors we have implemented, showing a simple use case in which we debias the *BERT* model.\n",
    "\n",
    "Fairness processors can be classified according to the part of the machine learning pipeline they are introduced in:\n",
    "\n",
    "1. Pre processors: if they are introduced before the model has been trained.\n",
    "1. In processors: if they are introduced during the process of training the model.\n",
    "1. Post processors: if they are introduced after the training step.\n",
    "1. Intra processors: aditionally, we speak of *intra processors* when refering to fairness methods that do not modify a model's parameters. This notion overlaps with that of post processors and can be deemed equivalent.\n",
    "\n",
    "To showcase the implementation of these methods we will run then on the imdb data set without further considerations as it is only intended to serve as a proof of concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Hugging face\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    Dataset\n",
    ")\n",
    "\n",
    "\n",
    "# Custom imports\n",
    "LOCAL = True\n",
    "if LOCAL:\n",
    "    import os\n",
    "    import sys\n",
    "    ROOT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")) \\\n",
    "        if \"__file__\" in globals() else os.path.abspath(\"..\")\n",
    "    sys.path.insert(0, ROOT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Use GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load BERT\n",
    "def get_bert():\n",
    "    return BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", num_labels=2\n",
    "        )\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "BERT = get_bert()\n",
    "HIDDEN_DIM_BERT = BERT.config.hidden_size\n",
    "\n",
    "# Download data set, tokenize\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return TOKENIZER(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "        )\n",
    "\n",
    "dataset = imdb.map(tokenize_function, batched=True)\n",
    "dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    "    )\n",
    "\n",
    "# Train test split\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]\n",
    "\n",
    "# Trainer configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    save_safetensors=False, \n",
    "    weight_decay=0.1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run our proof of concept with a `Trainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 07:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.327300</td>\n",
       "      <td>0.293338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 01:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2933380901813507, 'eval_runtime': 82.3371, 'eval_samples_per_second': 303.63, 'eval_steps_per_second': 9.498, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=BERT,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=(\n",
    "        AdamW(BERT.parameters(), lr=1e-5, weight_decay=0.1),\n",
    "        None\n",
    "        )\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre processors are those methods that only affect the model's inputs and do not change their parameters. We have implemented:\n",
    "\n",
    "1. Counterfactual Data Augmentation (CDA).\n",
    "1. Projection based debiasing.\n",
    "1. BLIND debiasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CDA is based on the idea of augmenting the data by flipping words with information of the sensitive attribute (e.g. feminine vs. masculine words). This procedure is implemented with the `transform_batch` function which is applied to a hugging face data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e82f73113ad4c619d4301554f23133d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39684 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of original train data set: 25000\n",
      "Lenght of CDA augmented train data set: 39684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1241' max='1241' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1241/1241 04:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.195300</td>\n",
       "      <td>0.272441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 00:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27244114875793457, 'eval_runtime': 39.2718, 'eval_samples_per_second': 636.589, 'eval_steps_per_second': 19.912, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from FairLangProc.algorithms.preprocessors import CDA\n",
    "\n",
    "gendered_pairs = [\n",
    "    ('he', 'she'),\n",
    "    ('him', 'her'),\n",
    "    ('his', 'hers'),\n",
    "    ('actor', 'actress'),\n",
    "    ('priest', 'nun'),\n",
    "    ('father', 'mother'),\n",
    "    ('dad', 'mom'),\n",
    "    ('daddy', 'mommy'),\n",
    "    ('waiter', 'waitress'),\n",
    "    ('James', 'Jane')\n",
    "    ]\n",
    "\n",
    "cda_train = Dataset.from_dict(\n",
    "        CDA(imdb['train'][:], pairs = dict(gendered_pairs))\n",
    ")\n",
    "\n",
    "train_CDA = cda_train.map(tokenize_function, batched=True)\n",
    "train_CDA.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    ")\n",
    "\n",
    "# Check differences\n",
    "print(f'Lenght of original train data set: {len(train_dataset['text'])}')\n",
    "print(f'Lenght of CDA augmented train data set: {len(cda_train['text'])}')\n",
    "\n",
    "# Train model\n",
    "CDAModel = get_bert()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=CDAModel,\n",
    "    args=training_args,\n",
    "    train_dataset=train_CDA,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=(\n",
    "        AdamW(CDAModel.parameters(), lr=2e-5, weight_decay=0.01),\n",
    "        None\n",
    "        )\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLIND debiasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLIND debiasing incoroporates a classifier whom is tasked with identifying whether the base model will succeed in the task for a given training instance. The model then reweights each training instance depending on the probability that this auxiliary model, $g_{B}$, assigns to the base model that it will correctly perform the task. The loss is modified accordingly:\n",
    "\n",
    "$$     \\mathcal{L}_{BLIND} = \\left(1 - \\sigma \\left( g_{B}(h; \\theta_{B} ) \\right) \\right)^{\\gamma} \\mathcal{L}^{task}(\\hat{y}, y), $$\n",
    "\n",
    "where $\\gamma$ is a hyper-parameter.\n",
    "\n",
    "The implementation of BLIND is given by the `BLINDModel` abstract class, which requires the implementation of three abstract methods:\n",
    "\n",
    "1. `_get_loss`: sets the `self.loss_fct` attribute to the desired loss.\n",
    "1. `_loss`: computes the value of `self.loss_fct` for a training instance.\n",
    "1. `_get_embedding`: which retrieves the hidden representation of a given input.\n",
    "\n",
    "We have implemented the `BLINDModelForClassification` to handle classification tasks, which sets the loss function to the usual cross-entropy loss and only requires the definition of `_get_embedding`. Below we implement a custom class for the *BERT* model which showcases the ease of use of our class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 08:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.137576</td>\n",
       "      <td>0.079221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 03:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07922087609767914, 'eval_runtime': 197.1266, 'eval_samples_per_second': 126.822, 'eval_steps_per_second': 3.967, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from FairLangProc.algorithms.preprocessors import BLINDTrainer\n",
    "\n",
    "BLINDModel = get_bert()\n",
    "BLINDClassifier = nn.Sequential(\n",
    "      nn.Linear(HIDDEN_DIM_BERT, HIDDEN_DIM_BERT),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(HIDDEN_DIM_BERT, 2)\n",
    ")\n",
    "\n",
    "class BLINDBERTTrainer(BLINDTrainer):\n",
    "    def _get_embedding(self, inputs):\n",
    "        return self.model.bert(\n",
    "            input_ids = inputs.get(\"input_ids\"),\n",
    "            attention_mask = inputs.get(\"attention_mask\"),\n",
    "            token_type_ids = inputs.get(\"token_type_ids\")\n",
    "            ).last_hidden_state[:,0,:]\n",
    "    \n",
    "trainer = BLINDBERTTrainer(\n",
    "    blind_model = BLINDClassifier,\n",
    "    blind_optimizer = lambda x: AdamW(x, lr=1e-5, weight_decay=0.1),\n",
    "    temperature = 1.0,\n",
    "    gamma = 2.0,\n",
    "    alpha = 1.0,\n",
    "    model = BLINDModel,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    optimizers=(\n",
    "        AdamW(BLINDModel.parameters(), lr=1e-5, weight_decay=0.1),\n",
    "        None\n",
    "        )\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection based debiasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projection based debiasing identifies a bias subpsace by performing PCA on the difference of the hidden representation of counterfactual pairs of words or sentences. Then, the hidden representation of a given input is debiased by computing its projection on the bias-free subspace that's orthogonal to the bias subspace:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$    h_{proj} = h - \\sum_{i = 1}^{n_{bias} } \\langle h, v_i \\rangle \\, v_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of projection based debiasing is given by the `SentDebiasModel` abstract class, which requires the implementation of three abstract methods:\n",
    "\n",
    "1. `_get_loss`: sets the `self.loss_fct` attribute to the desired loss.\n",
    "1. `_loss`: computes the value of `self.loss_fct` for a training instance.\n",
    "1. `_get_embedding`: which retrieves the hidden representation of a given input.\n",
    "\n",
    "We have implemented the `SentDebiasForSequenceClassification` to handle classification tasks, which sets the loss function to the usual cross-entropy loss and only requires the definition of `_get_embedding`. Below we implement a custom class for the *BERT* model which showcases the ease of use of our class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 07:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.307300</td>\n",
       "      <td>0.288937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 01:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2889372706413269, 'eval_runtime': 82.5018, 'eval_samples_per_second': 303.024, 'eval_steps_per_second': 9.479, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from FairLangProc.algorithms.preprocessors\\\n",
    "import SentDebiasForSequenceClassification\n",
    "\n",
    "gendered_pairs = [('he', 'she'), ('his', 'hers'), ('monk', 'nun')]\n",
    "\n",
    "model = get_bert()\n",
    "\n",
    "class SentDebiasBert(SentDebiasForSequenceClassification):        \n",
    "    def _get_embedding(\n",
    "            self,\n",
    "            input_ids,\n",
    "            attention_mask = None,\n",
    "            token_type_ids = None\n",
    "            ):\n",
    "        return self.model.bert(\n",
    "            input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids\n",
    "            ).last_hidden_state[:,0,:]\n",
    "\n",
    "\n",
    "EmbedModel = SentDebiasBert(\n",
    "    model = model,\n",
    "    config = None,\n",
    "    tokenizer = TOKENIZER,\n",
    "    word_pairs = gendered_pairs,\n",
    "    n_components = 1,\n",
    "    n_labels = 2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=EmbedModel,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=(\n",
    "        AdamW(EmbedModel.parameters(), lr=1e-5, weight_decay=0.1),\n",
    "        None\n",
    "        )\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In processors are those methods that change the way the model is trained. In particular we have implemented:\n",
    "\n",
    "1. ADELE (adapter based debiasing).\n",
    "1. Selective updating.\n",
    "1. Regularizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADELE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ADELE method adopts an adapter-based approach where they include an adapter layer after each FNN layer of the transformer architecture, this layers take the form:\n",
    "\n",
    "$$\\text{Adapter}(h, r) = U \\cdot g(D \\cdot h) + r$$\n",
    "\n",
    "that is, it is a linear layer with an activation function and a bias, $r$. This layer has a smaller dimension than the corresponding FNN, compressing the data and providing a information bottleneck so the bias information gets discarded after carefully training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1241' max='1241' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1241/1241 07:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.661300</td>\n",
       "      <td>0.651091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 01:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6510912775993347, 'eval_runtime': 98.7479, 'eval_samples_per_second': 253.17, 'eval_steps_per_second': 7.919, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from adapters import AdapterTrainer\n",
    "from FairLangProc.algorithms.inprocessors import DebiasAdapter\n",
    "\n",
    "DebiasAdapter = DebiasAdapter(\n",
    "    model = get_bert(),\n",
    "    adapter_config = \"seq_bn\"\n",
    "    )\n",
    "AdeleModel = DebiasAdapter.get_model()\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=AdeleModel,\n",
    "    args=training_args,\n",
    "    train_dataset=train_CDA,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=(\n",
    "        AdamW(AdeleModel.parameters(),lr=1e-5, weight_decay=0.1),\n",
    "        None\n",
    "        )\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selective updating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selective updating aims to selectively update some of the model's parameters. The method `selective_unfreezing` allows to freeze all of the model's parameters with the exception of certain parameters specificied by their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1241' max='1241' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1241/1241 07:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.327800</td>\n",
       "      <td>0.318286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 01:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3182864487171173, 'eval_runtime': 74.9231, 'eval_samples_per_second': 333.675, 'eval_steps_per_second': 10.437, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from FairLangProc.algorithms.inprocessors import selective_unfreezing\n",
    "\n",
    "FrozenBert = get_bert()\n",
    "selective_unfreezing(FrozenBert, [\"attention.self\", \"attention.output\"])\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=FrozenBert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_CDA,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=(\n",
    "        AdamW(FrozenBert.parameters(), lr=1e-5, weight_decay=0.1),\n",
    "        None\n",
    "        )\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of regularizers is to modify the original task loss by adding a new term:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{reg} = \\mathcal{L}^{task} + \\lambda \\mathcal{R}\n",
    "$$,\n",
    "\n",
    "where $\\mathcal{R}$ represents a term that aims to debias the LLM. In particular, we have implemented Entropy Attention Regularizer (EAR) and a projection based regularizer. Here we showcase the EAR regularizer through the `EARModel` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 13:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.250200</td>\n",
       "      <td>-0.283381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 03:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': -0.28338149189949036, 'eval_runtime': 189.3683, 'eval_samples_per_second': 132.018, 'eval_steps_per_second': 4.13, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from FairLangProc.algorithms.inprocessors import EARModel\n",
    "\n",
    "model = get_bert()\n",
    "\n",
    "EARRegularizer = EARModel(\n",
    "     model = model,\n",
    "     ear_reg_strength = 0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=EARRegularizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=(\n",
    "        AdamW(EARRegularizer.parameters(), lr=1e-5, weight_decay=0.1),\n",
    "        None\n",
    "        )\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intra processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intra processors are those methods that happen after training has already been done but which do not change the model's parameters. There is certain overlap between intra processors and more traditional post processors.\n",
    "\n",
    "We have implemented:\n",
    "\n",
    "1. Diff pruning.\n",
    "1. Entropy Attention Temperature (EAT) scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diff pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diff pruning is a modular technique which freezes the model's parameters and trains a sparse set of parameters, $\\delta$, added over the original ones. These parameters are decomposed as $\\delta = m \\odot w$ where $m$ is a sparsity mask and $w$ is the magnitude of the parameter. The new parameters are trained on a new loss which:\n",
    "\n",
    "1. Learns the task at hand with $$\\mathcal{L}^{task}$$\n",
    "1. Learns to debias $$\\mathcal{L}^{debias} = \\left(\\frac{\\sum_{x_A \\in X^A} \\phi (E(x_A))}{|X^A |} - \\frac{\\sum_{x_B \\in X^B} \\phi (E(x_B))}{|X^B|} \\right)^2$$ where $\\phi$ is a kernel and $X^i, i \\in \\{A,B\\}$ are sets of words with demographic information.\n",
    "1. Promotes sparsity with $$\\mathcal{L}^{0} = \\sum_{i=1}^{|\\delta_{\\rho}|} \\sigma\\left( \\log \\alpha_{\\rho, i} - \\log\\left(- \\frac{\\gamma}{\\zeta}\\right) \\right)$$\n",
    "\n",
    "The total loss function is given by the sum of the previous three terms.\n",
    "\n",
    "We have implemented this method through the `DiffPrunedDebiasing` class which requires the implementation of the abstract method `_get_embedding`, which computes the embedding of a given input.\n",
    "We provide the implementation of `DiffPrunningBERT` to apply this method to the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 12:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.561900</td>\n",
       "      <td>1.519466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 02:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5194658041000366, 'eval_runtime': 163.0023, 'eval_samples_per_second': 153.372, 'eval_steps_per_second': 4.797, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from FairLangProc.algorithms.intraprocessors import DiffPrunBERT\n",
    "\n",
    "gendered_pairs = [\n",
    "    (\"manager\", \"manageress\"),\n",
    "    (\"nephew\", \"niece\"),\n",
    "    (\"prince\", \"princess\"),\n",
    "    (\"baron\", \"baroness\"),\n",
    "    (\"father\", \"mother\"),\n",
    "    (\"stepsons\", \"stepdaughters\"),\n",
    "    (\"boyfriend\", \"girlfriend\"),\n",
    "    (\"fiances\", \"fiancees\"),\n",
    "    (\"shepherd\", \"shepherdess\"),\n",
    "    (\"beau\", \"belle\"),\n",
    "    (\"males\", \"females\"),\n",
    "    (\"hunter\", \"huntress\"),\n",
    "    (\"grandfathers\", \"grandmothers\"),\n",
    "    (\"daddies\", \"mummies\"),\n",
    "    (\"step-son\", \"step-daughter\"),\n",
    "    (\"masters\", \"mistresses\"),\n",
    "    (\"nephews\", \"nieces\"),\n",
    "    (\"brother\", \"sister\"),\n",
    "    (\"grandfather\", \"grandmother\"),\n",
    "    (\"priest\", \"priestess\")\n",
    "]\n",
    "\n",
    "tokens_male = [words[0] for words in gendered_pairs]\n",
    "tokens_female = [words[1] for words in gendered_pairs]\n",
    "\n",
    "inputs_male = TOKENIZER(\n",
    "    tokens_male, padding = True, return_tensors = \"pt\"\n",
    "    )\n",
    "inputs_female = TOKENIZER(\n",
    "    tokens_female, padding = True, return_tensors = \"pt\"\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_by_column(x: torch.Tensor, eps: float = 1e-8):\n",
    "    mean = x.mean(dim=0, keepdim=True)\n",
    "    std = x.std(dim=0, keepdim=True)\n",
    "    return (x - mean) / (std + eps)\n",
    "\n",
    "original_model = get_bert()\n",
    "\n",
    "ModularDebiasingBERT = DiffPrunBERT(\n",
    "    head = original_model.classifier,\n",
    "    encoder = original_model.bert,\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(),\n",
    "    input_ids_A = inputs_male,\n",
    "    input_ids_B = inputs_female,\n",
    "    bias_kernel = normalize_by_column,\n",
    "    upper = 10,\n",
    "    lower = -0.001,\n",
    "    lambda_bias = 0.5,\n",
    "    lambda_sparse = 0.00001\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=ModularDebiasingBERT,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=(\n",
    "        AdamW(ModularDebiasingBERT.parameters(), lr=1e-5, weight_decay=0.1),\n",
    "        None\n",
    "        )\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy Attention Temperature (EAT) scaling proposes the use of Entropy-based Attention Temperature (EAT) scaling in order to modify the distribution of the attention scores with a temperature-related parameter, $\\beta \\in [0, \\infty)$:\n",
    "\n",
    "$$\\text{Attention}_{\\beta} (\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax} \\left(\\frac{\\beta \\mathbf{Q} \\mathbf{K}}{\\sqrt{d_k}} \\right) \\mathbf{V}.$$\n",
    "\n",
    "We have implemented EAT scaling through the `add_EAT_hook` which simply requires the specification of a LLM and the $\\beta$ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.798125147819519, 'eval_model_preparation_time': 0.0021, 'eval_runtime': 40.3474, 'eval_samples_per_second': 619.619, 'eval_steps_per_second': 19.382}\n"
     ]
    }
   ],
   "source": [
    "from FairLangProc.algorithms.intraprocessors import add_EAT_hook\n",
    "\n",
    "EATBert = BERT\n",
    "beta = 1.5\n",
    "\n",
    "add_EAT_hook(model=EATBert, beta=beta)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=EATBert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=(\n",
    "        AdamW(EATBert.parameters(), lr=1e-5, weight_decay=0.1),\n",
    "        None\n",
    "        )\n",
    ")\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FairLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
