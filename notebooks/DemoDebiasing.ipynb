{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab132f7",
   "metadata": {},
   "source": [
    "# Debiasing a Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abcdd76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Computing libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Check if CUDA is available and set device\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# huggging face\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "# Custom imports\n",
    "import os\n",
    "import sys\n",
    "ruta_raiz = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")) \\\n",
    "    if \"__file__\" in globals() else os.path.abspath(\"..\")\n",
    "sys.path.insert(0, ruta_raiz)\n",
    "\n",
    "from FairLangProc.datasets import BiasDataLoader\n",
    "from FairLangProc.metrics import WEAT\n",
    "\n",
    "from FairLangProc.algorithms.preprocessors import CDA, BLINDModelForClassification, SentDebiasForSequenceClassification\n",
    "from FairLangProc.algorithms.inprocessors import EARModel, DebiasAdapter, selective_unfreezing \n",
    "from FairLangProc.algorithms.intraprocessors import add_EAT_hook, DiffPrunningBERT, DiffPrunedDebiasing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc5db4",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c82514fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODELS = [\n",
    "    'bert-base-uncased',\n",
    "    'deepseek-ai/deepseek-llm-7b-base',\n",
    "    'huggyllama/llama-7b'\n",
    "]\n",
    "TASKS = [\n",
    "    \"cola\",\n",
    "    \"sst2\",\n",
    "    \"mrpc\",\n",
    "    \"stsb\",\n",
    "    \"qqp\",\n",
    "    \"mnli\",\n",
    "    \"qnli\",\n",
    "    \"rte\",\n",
    "    \"wnli\"\n",
    "]\n",
    "TASK_LABELS = {\n",
    "    \"cola\": 2,\n",
    "    \"sst2\": 2,\n",
    "    \"mrpc\": 2,\n",
    "    \"qqp\": 2,\n",
    "    \"stsb\": 1,\n",
    "    \"mnli\": 3,\n",
    "    \"qnli\": 2,\n",
    "    \"rte\": 2,\n",
    "    \"wnli\": 2,\n",
    "}\n",
    "DEBIAS_METHODS = [\n",
    "    \"none\",\n",
    "    \"cda\",\n",
    "    \"blind\",\n",
    "    \"embedding\",\n",
    "    \"ear\",\n",
    "    \"adele\",\n",
    "    \"selective\",\n",
    "    \"eat\",\n",
    "    \"diff\"\n",
    "]\n",
    "TASK_METRICS = {\n",
    "    \"cola\": \"eval_matthews_correlation\",\n",
    "    \"sst2\": \"eval_accuracy\",\n",
    "    \"mrpc\": \"eval_accuracy\",\n",
    "    \"stsb\": \"eval_pearson\",\n",
    "    \"mnli\": \"eval_accuracy\",\n",
    "    \"qnli\": \"eval_accuracy\",\n",
    "    \"rte\": \"eval_accuracy\",\n",
    "    \"wnli\": \"eval_accuracy\",\n",
    "}\n",
    "CDA_METHOD = {\n",
    "    \"none\": False,\n",
    "    \"cda\": True,\n",
    "    \"blind\": False,\n",
    "    \"embedding\": False,\n",
    "    \"ear\": False,\n",
    "    \"adele\": True,\n",
    "    \"selective\": True,\n",
    "    \"eat\": False,\n",
    "    \"diff\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4306857",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = MODELS[0]\n",
    "TASK = \"mnli\"\n",
    "DEBIAS = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ce127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_FOR_BEST = TASK_METRICS.get(TASK, \"eval_accuracy\")\n",
    "BATCH_SIZE = 16\n",
    "WEIGHT_DECAY = 0.1\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09fb250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_pairs = [\n",
    "    (\"gods\", \"goddesses\"), (\"manager\", \"manageress\"), (\"barons\", \"baronesses\"),\n",
    "    (\"nephew\", \"niece\"), (\"prince\", \"princess\"), (\"boars\", \"sows\"),\n",
    "    (\"baron\", \"baroness\"), (\"stepfathers\", \"stepmothers\"), (\"wizard\", \"witch\"),\n",
    "    (\"father\", \"mother\"), (\"stepsons\", \"stepdaughters\"), (\"sons-in-law\", \"daughters-in-law\"),\n",
    "    (\"dukes\", \"duchesses\"), (\"boyfriend\", \"girlfriend\"), (\"fiances\", \"fiancees\"),\n",
    "    (\"dad\", \"mom\"), (\"shepherd\", \"shepherdess\"), (\"uncles\", \"aunts\"),\n",
    "    (\"beau\", \"belle\"), (\"males\", \"females\"), (\"hunter\", \"huntress\"),\n",
    "    (\"beaus\", \"belles\"), (\"grandfathers\", \"grandmothers\"), (\"lads\", \"lasses\"),\n",
    "    (\"daddies\", \"mummies\"), (\"step-son\", \"step-daughter\"), (\"masters\", \"mistresses\"),\n",
    "    (\"policeman\", \"policewoman\"), (\"nephews\", \"nieces\"), (\"brother\", \"sister\"),\n",
    "    (\"grandfather\", \"grandmother\"), (\"priest\", \"priestess\"), (\"hosts\", \"hostesses\"),\n",
    "    (\"landlord\", \"landlady\"), (\"husband\", \"wife\"), (\"poet\", \"poetess\"),\n",
    "    (\"landlords\", \"landladies\"), (\"fathers\", \"mothers\"), (\"masseur\", \"masseuse\"),\n",
    "    (\"monks\", \"nuns\"), (\"usher\", \"usherette\"), (\"hero\", \"heroine\"),\n",
    "    (\"stepson\", \"stepdaughter\"), (\"postman\", \"postwoman\"), (\"god\", \"goddess\"),\n",
    "    (\"milkmen\", \"milkmaids\"), (\"stags\", \"hinds\"), (\"grandpa\", \"grandma\"),\n",
    "    (\"chairmen\", \"chairwomen\"), (\"husbands\", \"wives\"), (\"grandpas\", \"grandmas\"),\n",
    "    (\"stewards\", \"stewardesses\"), (\"murderer\", \"murderess\"), (\"manservant\", \"maidservant\"),\n",
    "    (\"men\", \"women\"), (\"host\", \"hostess\"), (\"heirs\", \"heiresses\"),\n",
    "    (\"masseurs\", \"masseuses\"), (\"boy\", \"girl\"), (\"male\", \"female\"),\n",
    "    (\"son-in-law\", \"daughter-in-law\"), (\"waiter\", \"waitress\"), (\"tutors\", \"governesses\"),\n",
    "    (\"priests\", \"priestesses\"), (\"bachelor\", \"spinster\"), (\"millionaire\", \"millionairess\"),\n",
    "    (\"steward\", \"stewardess\"), (\"businessmen\", \"businesswomen\"), (\"congressman\", \"congresswoman\"),\n",
    "    (\"emperor\", \"empress\"), (\"duke\", \"duchess\"), (\"sire\", \"dam\"),\n",
    "    (\"son\", \"daughter\"), (\"sirs\", \"madams\"), (\"widower\", \"widow\"),\n",
    "    (\"kings\", \"queens\"), (\"papas\", \"mamas\"), (\"grandsons\", \"granddaughters\"),\n",
    "    (\"proprietor\", \"proprietress\"), (\"monk\", \"nun\"), (\"headmasters\", \"headmistresses\"),\n",
    "    (\"grooms\", \"brides\"), (\"heir\", \"heiress\"), (\"boys\", \"girls\"),\n",
    "    (\"gentleman\", \"lady\"), (\"uncle\", \"aunt\"), (\"he\", \"she\"),\n",
    "    (\"king\", \"queen\"), (\"princes\", \"princesses\"), (\"policemen\", \"policewomen\"),\n",
    "    (\"governor\", \"matron\"), (\"fiance\", \"fiancee\"), (\"step-father\", \"step-mother\"),\n",
    "    (\"waiters\", \"waitresses\"), (\"mr\", \"mrs\"), (\"stepfather\", \"stepmother\"),\n",
    "    (\"daddy\", \"mummy\"), (\"lords\", \"ladies\"), (\"widowers\", \"widows\"),\n",
    "    (\"emperors\", \"empresses\"), (\"father-in-law\", \"mother-in-law\"), (\"abbot\", \"abbess\"),\n",
    "    (\"sir\", \"madam\"), (\"actor\", \"actress\"), (\"mr.\", \"mrs.\"),\n",
    "    (\"wizards\", \"witches\"), (\"actors\", \"actresses\"), (\"chairman\", \"chairwoman\"),\n",
    "    (\"sorcerer\", \"sorceress\"), (\"postmaster\", \"postmistress\"), (\"brothers\", \"sisters\"),\n",
    "    (\"lad\", \"lass\"), (\"headmaster\", \"headmistress\"), (\"papa\", \"mama\"),\n",
    "    (\"milkman\", \"milkmaid\"), (\"heroes\", \"heroines\"), (\"man\", \"woman\"),\n",
    "    (\"grandson\", \"granddaughter\"), (\"groom\", \"bride\"), (\"sons\", \"daughters\"),\n",
    "    (\"congressmen\", \"congresswomen\"), (\"businessman\", \"businesswoman\"), (\"boyfriends\", \"girlfriends\"),\n",
    "    (\"dads\", \"moms\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff62a3ca",
   "metadata": {},
   "source": [
    "## Load model and debias method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eed8b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = TASK_LABELS[TASK]\n",
    "if TASK == 'mnli':\n",
    "    problem_type='regression'\n",
    "else:\n",
    "    problem_type='single_label_classification'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "original_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels, problem_type=problem_type)\n",
    "\n",
    "hidden_dim = original_model.config.hidden_size\n",
    "if not hasattr(original_model, 'classifier'):\n",
    "    original_model.classifier = original_model.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c41df852",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS in (\"none\", \"cda\", \"eat\"):\n",
    "    model = original_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2811098",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"embedding\":\n",
    "\n",
    "    class SentDebiasBert(SentDebiasForSequenceClassification):        \n",
    "        def _get_embedding(self, input_ids, attention_mask = None, token_type_ids = None):\n",
    "            return self.model.bert(\n",
    "                input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids\n",
    "                ).last_hidden_state[:,0,:]\n",
    "\n",
    "    class SentDebiasAverageAutoreg(SentDebiasForSequenceClassification):\n",
    "        def _get_embedding(self, input_ids, attention_mask = None, token_type_ids = None):\n",
    "            return self.model.model(\n",
    "                input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids\n",
    "                ).last_hidden_state.mean(dim = 1)\n",
    "        \n",
    "    if MODEL_NAME == 'bert-base-uncased':\n",
    "        model = SentDebiasBert(\n",
    "            model = original_model,\n",
    "            config = None,\n",
    "            tokenizer = tokenizer,\n",
    "            word_pairs = counterfactual_pairs,\n",
    "            n_components = 1,\n",
    "            n_labels = num_labels\n",
    "        )\n",
    "    else:\n",
    "        model = SentDebiasAverageAutoreg(\n",
    "            model = original_model,\n",
    "            config = None,\n",
    "            tokenizer = tokenizer,\n",
    "            word_pairs = counterfactual_pairs,\n",
    "            n_components = 1,\n",
    "            n_labels = num_labels\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "308ef36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"blind\":\n",
    "    \n",
    "    class BLINDBERT(BLINDModelForClassification):\n",
    "        def _get_embedding(self, input_ids = None, attention_mask = None, token_type_ids = None):\n",
    "            return self.model.bert(\n",
    "                input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids\n",
    "                ).last_hidden_state[:,0,:]\n",
    "        \n",
    "    class BLINDAverageAutoreg(BLINDModelForClassification):\n",
    "        def _get_embedding(self, input_ids = None, attention_mask = None, token_type_ids = None):\n",
    "            return self.model.model(\n",
    "                input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids\n",
    "                ).last_hidden_state.mean(dim = 1)\n",
    "        \n",
    "    if MODEL_NAME == 'bert-base-uncased':\n",
    "        model = BLINDBERT(\n",
    "            model = original_model,\n",
    "            config = None,\n",
    "            gamma=2.0,\n",
    "            temperature=1.0,\n",
    "            hidden_dim = hidden_dim,\n",
    "            n_labels = num_labels\n",
    "        )\n",
    "    else:\n",
    "        model = BLINDAverageAutoreg(\n",
    "            model = original_model,\n",
    "            config = None,\n",
    "            gamma=2.0,\n",
    "            temperature=1.0,\n",
    "            hidden_dim = hidden_dim,\n",
    "            n_labels = num_labels\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67c6127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"adele\":\n",
    "    model = DebiasAdapter(\n",
    "        model = original_model,\n",
    "        config = 'lora'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd36df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"ear\":\n",
    "    model = EARModel(\n",
    "        model = original_model,\n",
    "        ear_reg_strength = 0.01\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7625cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"selective\":\n",
    "    model = original_model\n",
    "    selective_unfreezing(model, [\"attention.self\", \"attention.output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"diff\":\n",
    "\n",
    "    class DiffPrunningAvgAutoReg(DiffPrunedDebiasing):\n",
    "        def _get_embedding(self, outputs):\n",
    "            return outputs.mean(dim = 1)\n",
    "        def _get_encoder(self):\n",
    "            self.encoder = self.base_model.model\n",
    "\n",
    "    tokens_male = [words[0] for words in counterfactual_pairs]\n",
    "    tokens_female = [words[1] for words in counterfactual_pairs]\n",
    "\n",
    "    inputs_male = tokenizer(tokens_male, padding = True, return_tensors = \"pt\")\n",
    "    inputs_female = tokenizer(tokens_female, padding = True, return_tensors = \"pt\")\n",
    "\n",
    "    if MODEL_NAME == 'bert-base-uncased':\n",
    "        model = DiffPrunningBERT(\n",
    "            model = original_model,\n",
    "            input_ids_A = inputs_male,\n",
    "            input_ids_B = inputs_female\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model = DiffPrunningAvgAutoReg(\n",
    "            model = original_model,\n",
    "            input_ids_A = inputs_male,\n",
    "            input_ids_B = inputs_female\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f7502",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73088bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, task):\n",
    "    if task in [\"sst2\", \"cola\"]:\n",
    "        return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task == \"mnli\":\n",
    "        return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task == \"qnli\":\n",
    "        return tokenizer(examples[\"question\"], examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task in [\"rte\", \"wnli\"]:\n",
    "        return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task == \"mrpc\":\n",
    "        return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task == \"qqp\":\n",
    "        return tokenizer(examples[\"question1\"], examples[\"question2\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task == \"stsb\":\n",
    "        return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "def get_metrics(task_name):\n",
    "    metric = evaluate.load(\"glue\", task_name)\n",
    "    if task_name == \"stsb\":\n",
    "        return metric, lambda logits: np.squeeze(logits, axis=-1)\n",
    "    return metric, lambda logits: np.argmax(logits, axis=-1)\n",
    "\n",
    "def compute_metrics_fn(p, task_name):\n",
    "    logits = p.predictions\n",
    "    labels = p.label_ids\n",
    "\n",
    "    if isinstance(logits, tuple) or isinstance(logits, list):\n",
    "        logits = logits[0]\n",
    "\n",
    "    metric, postprocess_fn = get_metrics(task_name)\n",
    "    predictions = postprocess_fn(logits)\n",
    "\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5a06a",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45dca282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = load_dataset(\"glue\", TASK)\n",
    "\n",
    "if CDA_METHOD[DEBIAS] and TASK != 'mnli':\n",
    "    train_dataset = Dataset.from_dict(\n",
    "        CDA(dataset['train'][:], pairs = dict(counterfactual_pairs))\n",
    "        )\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": dataset[\"validation\"],\n",
    "        \"test\": dataset[\"test\"]\n",
    "    })\n",
    "elif CDA_METHOD[DEBIAS] and TASK == 'mnli':\n",
    "    train_dataset = Dataset.from_dict(\n",
    "        CDA(dataset['train'][:], pairs = dict(counterfactual_pairs))\n",
    "        )\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation_matched\": dataset[\"validation_matched\"],\n",
    "        \"validation_mismatched\": dataset[\"validation_mismatched\"],\n",
    "        \"test_matched\": dataset[\"test_matched\"],\n",
    "        \"test_mismatched\": dataset[\"test_mismatched\"]\n",
    "    })\n",
    "\n",
    "if TASK == 'mnli':\n",
    "    dataset[\"validation\"] = dataset[\"validation_matched\"]\n",
    "    \n",
    "tokenized_datasets = dataset.map(lambda x: preprocess_function(x, tokenizer, TASK), batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7318cc",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328879c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo_perez/miniconda3/envs/FairLLM/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_112991/3404903422.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12571' max='34113' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12571/34113 1:10:19 < 2:00:31, 2.98 it/s, Epoch 1.11/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.380800</td>\n",
       "      <td>0.368326</td>\n",
       "      <td>0.830547</td>\n",
       "      <td>0.798162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.337100</td>\n",
       "      <td>0.328900</td>\n",
       "      <td>0.852140</td>\n",
       "      <td>0.815903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.313100</td>\n",
       "      <td>0.315610</td>\n",
       "      <td>0.862083</td>\n",
       "      <td>0.828600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.311600</td>\n",
       "      <td>0.291948</td>\n",
       "      <td>0.872718</td>\n",
       "      <td>0.829760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.294600</td>\n",
       "      <td>0.289571</td>\n",
       "      <td>0.873460</td>\n",
       "      <td>0.840165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.290300</td>\n",
       "      <td>0.281131</td>\n",
       "      <td>0.879718</td>\n",
       "      <td>0.839563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.280100</td>\n",
       "      <td>0.276153</td>\n",
       "      <td>0.880979</td>\n",
       "      <td>0.839718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.263259</td>\n",
       "      <td>0.884145</td>\n",
       "      <td>0.843835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>0.260025</td>\n",
       "      <td>0.887831</td>\n",
       "      <td>0.848586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.270800</td>\n",
       "      <td>0.258583</td>\n",
       "      <td>0.888721</td>\n",
       "      <td>0.855035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.261000</td>\n",
       "      <td>0.259044</td>\n",
       "      <td>0.886421</td>\n",
       "      <td>0.840212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.222500</td>\n",
       "      <td>0.254043</td>\n",
       "      <td>0.891838</td>\n",
       "      <td>0.856147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     28\u001b[39m training_args = TrainingArguments(\n\u001b[32m     29\u001b[39m     output_dir=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33moutput/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTASK\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEBIAS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME.replace(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     30\u001b[39m     learning_rate=\u001b[32m2e-5\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m     fp16=FP16\n\u001b[32m     43\u001b[39m )\n\u001b[32m     45\u001b[39m trainer = Trainer(\n\u001b[32m     46\u001b[39m     model=model,\n\u001b[32m     47\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m     optimizers=(AdamW(model.parameters(), lr=\u001b[32m1e-5\u001b[39m, weight_decay=WEIGHT_DECAY), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     54\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DEBIAS == \u001b[33m'\u001b[39m\u001b[33meat\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     59\u001b[39m     add_EAT_hook(model, beta=\u001b[32m0.7\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/transformers/trainer.py:2164\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2162\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2165\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2169\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/transformers/trainer.py:2529\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2523\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2524\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2526\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2527\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2528\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2529\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2530\u001b[39m ):\n\u001b[32m   2531\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2532\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2533\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "EVAL_STRATEGY = \"epoch\"\n",
    "SAVE_STRATEGY = \"epoch\"\n",
    "LOAD_BEST_MODEL_AT_END = True\n",
    "EVAL_STEPS = None\n",
    "\n",
    "if DEBIAS in ('adele', 'diff', 'eat'):\n",
    "    SAVE_STRATEGY = \"no\"\n",
    "    LOAD_BEST_MODEL_AT_END = False\n",
    "\n",
    "if TASK in ('qqp', 'mnli'):\n",
    "    BATCH_SIZE = 32\n",
    "    FP16 = True\n",
    "    EVAL_STRATEGY = \"steps\"\n",
    "    EVAL_STEPS = 1000\n",
    "    SAVE_STEPS = 1000\n",
    "\n",
    "else:\n",
    "    BATCH_SIZE = 16\n",
    "    FP16 = False\n",
    "    EVAL_STRATEGY = \"epoch\"\n",
    "    EVAL_STEPS = None\n",
    "    SAVE_STEPS = None\n",
    "\n",
    "if LOAD_BEST_MODEL_AT_END:\n",
    "    SAVE_STRATEGY = EVAL_STRATEGY  \n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"output/{TASK}-{DEBIAS}-{MODEL_NAME.replace('/', '-')}\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=EVAL_STRATEGY,\n",
    "    evaluation_strategy=EVAL_STRATEGY,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=SAVE_STRATEGY,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    logging_dir=\"logs\",\n",
    "    load_best_model_at_end=LOAD_BEST_MODEL_AT_END,\n",
    "    metric_for_best_model=METRIC_FOR_BEST,\n",
    "    fp16=FP16,\n",
    "    greater_is_better = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda p: compute_metrics_fn(p, TASK),\n",
    "    optimizers=(AdamW(model.parameters(), lr=1e-5, weight_decay=WEIGHT_DECAY), None),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "if DEBIAS == 'eat':\n",
    "    add_EAT_hook(model, beta=0.7)\n",
    "\n",
    "if TASK == 'mnli':\n",
    "    eval_results_mismd = trainer.evaluate(tokenized_datasets[\"validation_mismatched\"])\n",
    "    eval_results_match = trainer.evaluate(tokenized_datasets[\"validation_matched\"])\n",
    "    test_results_mismd = trainer.evaluate(tokenized_datasets[\"test_mismatched\"])\n",
    "    test_results_match = trainer.evaluate(tokenized_datasets[\"test_matched\"])\n",
    "    print(\"Validation results (matched) in \", TASK, \":\", eval_results_match)\n",
    "    print(\"Validation results (mismatched) in \", TASK, \":\", eval_results_mismd)\n",
    "    print(\"Test results (matched) in \", TASK, \":\", test_results_match)\n",
    "    print(\"Test results (mismatched) in \", TASK, \":\", test_results_mismd)\n",
    "else:\n",
    "    eval_results = trainer.evaluate()\n",
    "    test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "    print(\"Validation results in \", TASK, \":\", eval_results)\n",
    "    print(\"Test results in \", TASK, \":\", test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622d772",
   "metadata": {},
   "source": [
    "## WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c1af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X-A_mean_sim': 0.5240193605422974, 'X-B_mean_sim': 0.6104025840759277, 'Y-A_mean_sim': 0.5816237926483154, 'Y-B_mean_sim': 0.6603400707244873, 'W1_size': 8, 'W2_size': 8, 'A1_size': 8, 'A2_size': 8, 'effect_size': -0.07666268199682236}\n"
     ]
    }
   ],
   "source": [
    "class BertWEAT(WEAT):\n",
    "    def _get_embedding(self, outputs):\n",
    "        return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "class AverageAutoregWEAT(WEAT):\n",
    "    def _get_embedding(self, outputs):\n",
    "        return outputs.last_hidden_state.mean(dim = 1)\n",
    "    \n",
    "if MODEL_NAME == 'bert-base-uncased':\n",
    "    try:\n",
    "        weat = BertWEAT(model = model.model.bert, tokenizer = tokenizer)\n",
    "    except:\n",
    "        try:\n",
    "            weat = BertWEAT(model = model.bert, tokenizer = tokenizer)\n",
    "        except:\n",
    "            weat = BertWEAT(model = model.base_model.bert, tokenizer = tokenizer)\n",
    "else:\n",
    "    try:\n",
    "        weat = AverageAutoregWEAT(model = model.model.base_model, tokenizer = tokenizer)\n",
    "    except:\n",
    "        try:\n",
    "            weat = AverageAutoregWEAT(model = model.base_model, tokenizer = tokenizer)\n",
    "        except:\n",
    "            weat = AverageAutoregWEAT(model = model.base_model.base_model, tokenizer = tokenizer)\n",
    "\n",
    "math = ['math', 'algebra', 'geometry', 'calculus', 'equations', 'computation', 'numbers', 'addition']\n",
    "arts = ['poetry', 'art', 'dance', 'literature', 'novel', 'symphony', 'drama', 'sculpture']\n",
    "male = ['male', 'man', 'boy', 'brother', 'he', 'him', 'his', 'son']\n",
    "female = ['female', 'woman', 'girl', 'sister', 'she', 'her', 'hers', 'daughter']\n",
    "\n",
    "bias_results = weat.run_test(\n",
    "    W1_words = math, W2_words = arts,\n",
    "    A1_words = male, A2_words = female,\n",
    "    pval = False\n",
    "    )\n",
    "print(bias_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e45ab",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b154252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == 'mnli':\n",
    "    with open(f\"output/{TASK}-{DEBIAS}-{MODEL_NAME.replace('/', '-')}/results.json\", \"w\") as f:\n",
    "        json.dump({\"eval_matched\": eval_results_match, \"eval_mismatched\": eval_results_mismd, \"test_mismatched\": test_results_mismd, \"test_matched\": test_results_match, \"bias\": bias_results}, f, indent=4)\n",
    "else:\n",
    "    with open(f\"output/{TASK}-{DEBIAS}-{MODEL_NAME.replace('/', '-')}/results.json\", \"w\") as f:\n",
    "        json.dump({\"eval\": eval_results, \"test\": test_results, \"bias\": bias_results}, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FairLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
