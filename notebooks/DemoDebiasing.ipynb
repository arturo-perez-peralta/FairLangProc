{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab132f7",
   "metadata": {},
   "source": [
    "# Debiasing a Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcdd76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# huggging face\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "# Custom imports\n",
    "from FairLangProc.datasets import BiasDataLoader\n",
    "from FairLangProc.metrics import WEAT\n",
    "\n",
    "from FairLangProc.algorithms.preprocessors import CDA, BLINDModelForClassification, SentDebiasForSequenceClassification\n",
    "from FairLangProc.algorithms.inprocessors import EARModel, DebiasAdapter, selective_unfreezing \n",
    "from FairLangProc.algorithms.intraprocessors import add_EAT_hook, DiffPrunningBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc5db4",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82514fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODELS = [\n",
    "    'bert-base-uncased',\n",
    "    'deepseek-ai/deepseek-llm-7b-base',\n",
    "    'huggyllama/llama-7b'\n",
    "]\n",
    "TASKS = [\n",
    "    \"cola\",\n",
    "    \"sst2\",\n",
    "    \"mrpc\",\n",
    "    \"stsb\",\n",
    "    \"qqp\",\n",
    "    \"mnli\",\n",
    "    \"qnli\",\n",
    "    \"rte\",\n",
    "    \"wnli\"\n",
    "]\n",
    "TASK_LABELS = {\n",
    "    \"cola\": 2,\n",
    "    \"sst2\": 2,\n",
    "    \"mrpc\": 2,\n",
    "    \"qqp\": 2,\n",
    "    \"stsb\": 1,\n",
    "    \"mnli\": 3,\n",
    "    \"qnli\": 2,\n",
    "    \"rte\": 2,\n",
    "    \"wnli\": 2,\n",
    "}\n",
    "DEBIAS_METHODS = [\n",
    "    \"none\",\n",
    "    \"cda\",\n",
    "    \"blind\",\n",
    "    \"embedding\",\n",
    "    \"ear\",\n",
    "    \"adele\",\n",
    "    \"selective\",\n",
    "    \"eat\",\n",
    "    \"diff\"\n",
    "]\n",
    "\n",
    "\n",
    "MODEL_NAME = MODELS[0]\n",
    "TASK = \"cola\"\n",
    "BATCH_SIZE = 16\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEBIAS = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_pairs = [\n",
    "    (\"gods\", \"goddesses\"), (\"manager\", \"manageress\"), (\"barons\", \"baronesses\"),\n",
    "    (\"nephew\", \"niece\"), (\"prince\", \"princess\"), (\"boars\", \"sows\"),\n",
    "    (\"baron\", \"baroness\"), (\"stepfathers\", \"stepmothers\"), (\"wizard\", \"witch\"),\n",
    "    (\"father\", \"mother\"), (\"stepsons\", \"stepdaughters\"), (\"sons-in-law\", \"daughters-in-law\"),\n",
    "    (\"dukes\", \"duchesses\"), (\"boyfriend\", \"girlfriend\"), (\"fiances\", \"fiancees\"),\n",
    "    (\"dad\", \"mom\"), (\"shepherd\", \"shepherdess\"), (\"uncles\", \"aunts\"),\n",
    "    (\"beau\", \"belle\"), (\"males\", \"females\"), (\"hunter\", \"huntress\"),\n",
    "    (\"beaus\", \"belles\"), (\"grandfathers\", \"grandmothers\"), (\"lads\", \"lasses\"),\n",
    "    (\"daddies\", \"mummies\"), (\"step-son\", \"step-daughter\"), (\"masters\", \"mistresses\"),\n",
    "    (\"policeman\", \"policewoman\"), (\"nephews\", \"nieces\"), (\"brother\", \"sister\"),\n",
    "    (\"grandfather\", \"grandmother\"), (\"priest\", \"priestess\"), (\"hosts\", \"hostesses\"),\n",
    "    (\"landlord\", \"landlady\"), (\"husband\", \"wife\"), (\"poet\", \"poetess\"),\n",
    "    (\"landlords\", \"landladies\"), (\"fathers\", \"mothers\"), (\"masseur\", \"masseuse\"),\n",
    "    (\"monks\", \"nuns\"), (\"usher\", \"usherette\"), (\"hero\", \"heroine\"),\n",
    "    (\"stepson\", \"stepdaughter\"), (\"postman\", \"postwoman\"), (\"god\", \"goddess\"),\n",
    "    (\"milkmen\", \"milkmaids\"), (\"stags\", \"hinds\"), (\"grandpa\", \"grandma\"),\n",
    "    (\"chairmen\", \"chairwomen\"), (\"husbands\", \"wives\"), (\"grandpas\", \"grandmas\"),\n",
    "    (\"stewards\", \"stewardesses\"), (\"murderer\", \"murderess\"), (\"manservant\", \"maidservant\"),\n",
    "    (\"men\", \"women\"), (\"host\", \"hostess\"), (\"heirs\", \"heiresses\"),\n",
    "    (\"masseurs\", \"masseuses\"), (\"boy\", \"girl\"), (\"male\", \"female\"),\n",
    "    (\"son-in-law\", \"daughter-in-law\"), (\"waiter\", \"waitress\"), (\"tutors\", \"governesses\"),\n",
    "    (\"priests\", \"priestesses\"), (\"bachelor\", \"spinster\"), (\"millionaire\", \"millionairess\"),\n",
    "    (\"steward\", \"stewardess\"), (\"businessmen\", \"businesswomen\"), (\"congressman\", \"congresswoman\"),\n",
    "    (\"emperor\", \"empress\"), (\"duke\", \"duchess\"), (\"sire\", \"dam\"),\n",
    "    (\"son\", \"daughter\"), (\"sirs\", \"madams\"), (\"widower\", \"widow\"),\n",
    "    (\"kings\", \"queens\"), (\"papas\", \"mamas\"), (\"grandsons\", \"granddaughters\"),\n",
    "    (\"proprietor\", \"proprietress\"), (\"monk\", \"nun\"), (\"headmasters\", \"headmistresses\"),\n",
    "    (\"grooms\", \"brides\"), (\"heir\", \"heiress\"), (\"boys\", \"girls\"),\n",
    "    (\"gentleman\", \"lady\"), (\"uncle\", \"aunt\"), (\"he\", \"she\"),\n",
    "    (\"king\", \"queen\"), (\"princes\", \"princesses\"), (\"policemen\", \"policewomen\"),\n",
    "    (\"governor\", \"matron\"), (\"fiance\", \"fiancee\"), (\"step-father\", \"step-mother\"),\n",
    "    (\"waiters\", \"waitresses\"), (\"mr\", \"mrs\"), (\"stepfather\", \"stepmother\"),\n",
    "    (\"daddy\", \"mummy\"), (\"lords\", \"ladies\"), (\"widowers\", \"widows\"),\n",
    "    (\"emperors\", \"empresses\"), (\"father-in-law\", \"mother-in-law\"), (\"abbot\", \"abbess\"),\n",
    "    (\"sir\", \"madam\"), (\"actor\", \"actress\"), (\"mr.\", \"mrs.\"),\n",
    "    (\"wizards\", \"witches\"), (\"actors\", \"actresses\"), (\"chairman\", \"chairwoman\"),\n",
    "    (\"sorcerer\", \"sorceress\"), (\"postmaster\", \"postmistress\"), (\"brothers\", \"sisters\"),\n",
    "    (\"lad\", \"lass\"), (\"headmaster\", \"headmistress\"), (\"papa\", \"mama\"),\n",
    "    (\"milkman\", \"milkmaid\"), (\"heroes\", \"heroines\"), (\"man\", \"woman\"),\n",
    "    (\"grandson\", \"granddaughter\"), (\"groom\", \"bride\"), (\"sons\", \"daughters\"),\n",
    "    (\"congressmen\", \"congresswomen\"), (\"businessman\", \"businesswoman\"), (\"boyfriends\", \"girlfriends\"),\n",
    "    (\"dads\", \"moms\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff62a3ca",
   "metadata": {},
   "source": [
    "## Load model and debias method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed8b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84639f9edff49fba6243046b0d7d926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at deepseek-ai/deepseek-llm-7b-base and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = TASK_LABELS[TASK]\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "original_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "hidden_dim = original_model.config.hidden_size\n",
    "\n",
    "if not hasattr(original_model, 'classifier'):\n",
    "    original_model.classifier = original_model.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41df852",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS in (\"none\", \"cda\", \"eat\"):\n",
    "    model = original_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2811098",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"embedding\":\n",
    "\n",
    "    class SentDebiasBert(SentDebiasForSequenceClassification):        \n",
    "        def _get_embedding(self, input_ids, attention_mask = None, token_type_ids = None):\n",
    "            return self.model.bert(\n",
    "                input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids\n",
    "                ).last_hidden_state[:,0,:]\n",
    "\n",
    "    class SentDebiasAverageAutoreg(SentDebiasForSequenceClassification):\n",
    "        def _get_embedding(self, input_ids, attention_mask = None, token_type_ids = None):\n",
    "            return self.model.model(\n",
    "                input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids\n",
    "                ).last_hidden_state.mean(dim = 1)\n",
    "        \n",
    "    if model == 'bert-base-uncased':\n",
    "        SentDebias = SentDebiasBert(\n",
    "            model = model,\n",
    "            config = None,\n",
    "            tokenizer = tokenizer,\n",
    "            word_pairs = counterfactual_pairs,\n",
    "            n_components = 1\n",
    "        )\n",
    "    else:\n",
    "        SentDebias = SentDebiasAverageAutoreg(\n",
    "            model = model,\n",
    "            config = None,\n",
    "            tokenizer = tokenizer,\n",
    "            word_pairs = counterfactual_pairs,\n",
    "            n_components = 1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ef36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"blind\":\n",
    "    \n",
    "    class BLINDBERT(BLINDModelForClassification):\n",
    "        def _get_embedding(self, input_ids = None, attention_mask = None, token_type_ids = None):\n",
    "            return self.model.bert(\n",
    "                input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids\n",
    "                ).last_hidden_state[:,0,:]\n",
    "        \n",
    "    class BLINDAverageAutoreg(BLINDModelForClassification):\n",
    "        def _get_embedding(self, input_ids = None, attention_mask = None, token_type_ids = None):\n",
    "            return self.model.model(\n",
    "                input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids\n",
    "                ).last_hidden_state.mean(dim = 1)\n",
    "        \n",
    "    if model == 'bert-base-uncased':\n",
    "        model = BLINDBERT(\n",
    "            model = model,\n",
    "            config = None,\n",
    "            base_loss = nn.CrossEntropyLoss(),\n",
    "            alpha=1.0,\n",
    "            gamma=2.0,\n",
    "            temperature=1.0,\n",
    "            size_average=True,\n",
    "            hidden_dim = hidden_dim\n",
    "        )\n",
    "    else:\n",
    "        model = BLINDAverageAutoreg(\n",
    "            model = model,\n",
    "            config = None,\n",
    "            base_loss = nn.CrossEntropyLoss(),\n",
    "            alpha=1.0,\n",
    "            gamma=2.0,\n",
    "            temperature=1.0,\n",
    "            size_average=True,\n",
    "            hidden_dim = hidden_dim\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c6127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"adele\":\n",
    "    model = DebiasAdapter(\n",
    "        model = original_model,\n",
    "        config = 'lora'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd36df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"ear\":\n",
    "    model = EARModel(\n",
    "        model = original_model,\n",
    "        ear_reg_strength = 0.01\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"selective\":\n",
    "    model = original_model\n",
    "    selective_unfreezing(model, [\"attention.self\", \"attention.output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"diff\":\n",
    "    tokens_male = [words[0] for words in counterfactual_pairs]\n",
    "    tokens_female = [words[1] for words in counterfactual_pairs]\n",
    "\n",
    "    inputs_male = tokenizer(tokens_male, padding = True, return_tensors = \"pt\")\n",
    "    inputs_female = tokenizer(tokens_female, padding = True, return_tensors = \"pt\")\n",
    "\n",
    "    ModularDebiasingBERT = DiffPrunningBERT(\n",
    "        base_model = original_model,\n",
    "        input_ids_A = inputs_male,\n",
    "        input_ids_B = inputs_female\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f7502",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73088bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, task):\n",
    "    if task in [\"sst2\", \"cola\"]:\n",
    "        return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task in [\"mnli\", \"qnli\", \"rte\", \"wnli\"]:\n",
    "        return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task in [\"mrpc\", \"qqp\"]:\n",
    "        return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task == \"stsb\":\n",
    "        return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "def get_metric(task_name):\n",
    "    if task_name == \"stsb\":\n",
    "        return evaluate.load(\"glue\", task_name), lambda x: x\n",
    "    return evaluate.load(\"glue\", task_name), lambda logits: np.argmax(logits, axis=1)\n",
    "\n",
    "def compute_metrics_fn(eval_pred, task_name):\n",
    "    logits, labels = eval_pred\n",
    "    metric, pred_fn = get_metric(task_name)\n",
    "    if task_name == \"stsb\":\n",
    "        predictions = np.squeeze(logits)\n",
    "    else:\n",
    "        predictions = pred_fn(logits)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5a06a",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dca282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = load_dataset(\"glue\", TASK)\n",
    "\n",
    "if DEBIAS == \"cda\":\n",
    "    train_dataset = Dataset.from_dict(\n",
    "        CDA(dataset['train'][:], pairs = dict(counterfactual_pairs))\n",
    "        )\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        'validation': dataset[\"validation\"],\n",
    "        \"test\": dataset[\"test\"]\n",
    "        \n",
    "    })\n",
    "    \n",
    "tokenized_datasets = dataset.map(lambda x: preprocess_function(x, tokenizer, TASK), batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7318cc",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328879c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4096, out_features=2, bias=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./output/{TASK}-{DEBIAS}-{MODEL_NAME.replace('/', '-')}\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\" if TASK != \"stsb\" else \"pearson\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda p: compute_metrics_fn(p, TASK),\n",
    ")\n",
    "\n",
    "if DEBIAS == 'eat':\n",
    "    model = add_EAT_hook(\n",
    "        model = model,\n",
    "        beta = 0.7\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda p: compute_metrics_fn(p, TASK),\n",
    "    )\n",
    "\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Results in \", TASK, \":\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622d772",
   "metadata": {},
   "source": [
    "## WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "353adb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79825b04b964161a443e0d213509f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Config name is missing.\nPlease pick one among the available configs: ['words', 'associations', 'associations_wefat']\nExample of usage:\n\t`load_dataset('fairnlp/weat', 'words')`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m words = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfairnlp/weat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/datasets/load.py:2061\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2056\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   2057\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   2058\u001b[39m )\n\u001b[32m   2060\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2061\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2062\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2078\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   2079\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/datasets/load.py:1818\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[39m\n\u001b[32m   1816\u001b[39m builder_cls = get_dataset_builder_class(dataset_module, dataset_name=dataset_name)\n\u001b[32m   1817\u001b[39m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1818\u001b[39m builder_instance: DatasetBuilder = \u001b[43mbuilder_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1820\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1821\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1822\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1824\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mdataset_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1825\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1829\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1830\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1831\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1832\u001b[39m builder_instance._use_legacy_cache_dir_if_possible(dataset_module)\n\u001b[32m   1834\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/datasets/builder.py:343\u001b[39m, in \u001b[36mDatasetBuilder.__init__\u001b[39m\u001b[34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, repo_id, data_files, data_dir, storage_options, writer_batch_size, **config_kwargs)\u001b[39m\n\u001b[32m    341\u001b[39m     config_kwargs[\u001b[33m\"\u001b[39m\u001b[33mdata_dir\u001b[39m\u001b[33m\"\u001b[39m] = data_dir\n\u001b[32m    342\u001b[39m \u001b[38;5;28mself\u001b[39m.config_kwargs = config_kwargs\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28mself\u001b[39m.config, \u001b[38;5;28mself\u001b[39m.config_id = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_builder_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# Prefill datasetinfo\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    352\u001b[39m     \u001b[38;5;66;03m# TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/datasets/builder.py:555\u001b[39m, in \u001b[36mDatasetBuilder._create_builder_config\u001b[39m\u001b[34m(self, config_name, custom_features, **config_kwargs)\u001b[39m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_kwargs:\n\u001b[32m    552\u001b[39m         example_of_usage = (\n\u001b[32m    553\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mload_dataset(\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.repo_id\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m.dataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.BUILDER_CONFIGS[\u001b[32m0\u001b[39m].name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    554\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mConfig name is missing.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    557\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease pick one among the available configs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.builder_configs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    558\u001b[39m             + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExample of usage:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_of_usage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    559\u001b[39m         )\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    561\u001b[39m     builder_config = \u001b[38;5;28mself\u001b[39m.BUILDER_CONFIGS[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Config name is missing.\nPlease pick one among the available configs: ['words', 'associations', 'associations_wefat']\nExample of usage:\n\t`load_dataset('fairnlp/weat', 'words')`"
     ]
    }
   ],
   "source": [
    "words = load_dataset(\"fairnlp/weat\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertWEAT(WEAT):\n",
    "    def _get_embedding(self, outputs):\n",
    "        return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "class AverageAutoregWEAT(WEAT):\n",
    "    def _get_embedding(self, outputs):\n",
    "        return outputs.last_hidden_state.mean(dim = 1)\n",
    "    \n",
    "if MODEL_NAME == 'bert-base-uncased':\n",
    "    weat = BertWEAT(model = model, tokenizer = tokenizer)\n",
    "else:\n",
    "    weat = AverageAutoregWEAT(model = model, tokenizer = tokenizer)\n",
    "\n",
    "math = ['math', 'algebra', 'geometry', 'calculus', 'equations', 'computation', 'numbers', 'addition']\n",
    "arts = ['poetry', 'art', 'dance', 'literature', 'novel', 'symphony', 'drama', 'sculpture']\n",
    "male = ['male', 'man', 'boy', 'brother', 'he', 'him', 'his', 'son']\n",
    "female = ['female', 'woman', 'girl', 'sister', 'she', 'her', 'hers', 'daughter']\n",
    "\n",
    "results = weat.run_test(\n",
    "    W1_words = math, W2_words = arts,\n",
    "    A1_words = male, A2_words = female,\n",
    "    pval = False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FairLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
