{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab132f7",
   "metadata": {},
   "source": [
    "# Debiasing a Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abcdd76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import json\n",
    "\n",
    "# Computing libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Check if CUDA is available and set device\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# huggging face\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from adapters import AdapterTrainer\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "# Custom imports\n",
    "LOCAL = True\n",
    "if LOCAL:\n",
    "    import os\n",
    "    import sys\n",
    "    ROOT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")) \\\n",
    "        if \"__file__\" in globals() else os.path.abspath(\"..\")\n",
    "    sys.path.insert(0, ROOT_PATH)\n",
    "\n",
    "from FairLangProc.datasets import BiasDataLoader\n",
    "from FairLangProc.metrics import WEAT\n",
    "\n",
    "from FairLangProc.algorithms.preprocessors import CDA, BLINDTrainer, SentDebiasForSequenceClassification\n",
    "from FairLangProc.algorithms.inprocessors import EARModel, DebiasAdapter, selective_unfreezing \n",
    "from FairLangProc.algorithms.intraprocessors import add_EAT_hook, DiffPrunBERT, DiffPrunDebiasing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc5db4",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c82514fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODELS = [\n",
    "    'bert-base-uncased',\n",
    "    'deepseek-ai/deepseek-llm-7b-base',\n",
    "    'huggyllama/llama-7b'\n",
    "]\n",
    "TASKS = [\n",
    "    \"cola\",\n",
    "    \"sst2\",\n",
    "    \"mrpc\",\n",
    "    \"stsb\",\n",
    "    \"qqp\",\n",
    "    \"mnli\",\n",
    "    \"qnli\",\n",
    "    \"rte\",\n",
    "    \"wnli\"\n",
    "]\n",
    "TASK_LABELS = {\n",
    "    \"cola\": 2,\n",
    "    \"sst2\": 2,\n",
    "    \"mrpc\": 2,\n",
    "    \"qqp\": 2,\n",
    "    \"stsb\": 1,\n",
    "    \"mnli\": 3,\n",
    "    \"qnli\": 2,\n",
    "    \"rte\": 2,\n",
    "    \"wnli\": 2,\n",
    "}\n",
    "DEBIAS_METHODS = [\n",
    "    \"none\",\n",
    "    \"cda\",\n",
    "    \"blind\",\n",
    "    \"embedding\",\n",
    "    \"ear\",\n",
    "    \"adele\",\n",
    "    \"selective\",\n",
    "    \"eat\",\n",
    "    \"diff\"\n",
    "]\n",
    "TASK_METRICS = {\n",
    "    \"cola\": \"eval_matthews_correlation\",\n",
    "    \"sst2\": \"eval_accuracy\",\n",
    "    \"mrpc\": \"eval_accuracy\",\n",
    "    \"stsb\": \"eval_pearson\",\n",
    "    \"mnli\": \"eval_accuracy\",\n",
    "    \"qnli\": \"eval_accuracy\",\n",
    "    \"rte\": \"eval_accuracy\",\n",
    "    \"wnli\": \"eval_accuracy\",\n",
    "}\n",
    "CDA_METHOD = {\n",
    "    \"none\": False,\n",
    "    \"cda\": True,\n",
    "    \"blind\": False,\n",
    "    \"embedding\": False,\n",
    "    \"ear\": False,\n",
    "    \"adele\": True,\n",
    "    \"selective\": True,\n",
    "    \"eat\": False,\n",
    "    \"diff\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4306857",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = MODELS[0]\n",
    "TASK = \"cola\"\n",
    "DEBIAS = \"diff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7ce127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_FOR_BEST = TASK_METRICS.get(TASK, \"eval_accuracy\")\n",
    "BATCH_SIZE = 16\n",
    "WEIGHT_DECAY = 0.1\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09fb250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_pairs = [\n",
    "    (\"gods\", \"goddesses\"), (\"manager\", \"manageress\"), (\"barons\", \"baronesses\"),\n",
    "    (\"nephew\", \"niece\"), (\"prince\", \"princess\"), (\"boars\", \"sows\"),\n",
    "    (\"baron\", \"baroness\"), (\"stepfathers\", \"stepmothers\"), (\"wizard\", \"witch\"),\n",
    "    (\"father\", \"mother\"), (\"stepsons\", \"stepdaughters\"), (\"sons-in-law\", \"daughters-in-law\"),\n",
    "    (\"dukes\", \"duchesses\"), (\"boyfriend\", \"girlfriend\"), (\"fiances\", \"fiancees\"),\n",
    "    (\"dad\", \"mom\"), (\"shepherd\", \"shepherdess\"), (\"uncles\", \"aunts\"),\n",
    "    (\"beau\", \"belle\"), (\"males\", \"females\"), (\"hunter\", \"huntress\"),\n",
    "    (\"beaus\", \"belles\"), (\"grandfathers\", \"grandmothers\"), (\"lads\", \"lasses\"),\n",
    "    (\"daddies\", \"mummies\"), (\"step-son\", \"step-daughter\"), (\"masters\", \"mistresses\"),\n",
    "    (\"policeman\", \"policewoman\"), (\"nephews\", \"nieces\"), (\"brother\", \"sister\"),\n",
    "    (\"grandfather\", \"grandmother\"), (\"priest\", \"priestess\"), (\"hosts\", \"hostesses\"),\n",
    "    (\"landlord\", \"landlady\"), (\"husband\", \"wife\"), (\"poet\", \"poetess\"),\n",
    "    (\"landlords\", \"landladies\"), (\"fathers\", \"mothers\"), (\"masseur\", \"masseuse\"),\n",
    "    (\"monks\", \"nuns\"), (\"usher\", \"usherette\"), (\"hero\", \"heroine\"),\n",
    "    (\"stepson\", \"stepdaughter\"), (\"postman\", \"postwoman\"), (\"god\", \"goddess\"),\n",
    "    (\"milkmen\", \"milkmaids\"), (\"stags\", \"hinds\"), (\"grandpa\", \"grandma\"),\n",
    "    (\"chairmen\", \"chairwomen\"), (\"husbands\", \"wives\"), (\"grandpas\", \"grandmas\"),\n",
    "    (\"stewards\", \"stewardesses\"), (\"murderer\", \"murderess\"), (\"manservant\", \"maidservant\"),\n",
    "    (\"men\", \"women\"), (\"host\", \"hostess\"), (\"heirs\", \"heiresses\"),\n",
    "    (\"masseurs\", \"masseuses\"), (\"boy\", \"girl\"), (\"male\", \"female\"),\n",
    "    (\"son-in-law\", \"daughter-in-law\"), (\"waiter\", \"waitress\"), (\"tutors\", \"governesses\"),\n",
    "    (\"priests\", \"priestesses\"), (\"bachelor\", \"spinster\"), (\"millionaire\", \"millionairess\"),\n",
    "    (\"steward\", \"stewardess\"), (\"businessmen\", \"businesswomen\"), (\"congressman\", \"congresswoman\"),\n",
    "    (\"emperor\", \"empress\"), (\"duke\", \"duchess\"), (\"sire\", \"dam\"),\n",
    "    (\"son\", \"daughter\"), (\"sirs\", \"madams\"), (\"widower\", \"widow\"),\n",
    "    (\"kings\", \"queens\"), (\"papas\", \"mamas\"), (\"grandsons\", \"granddaughters\"),\n",
    "    (\"proprietor\", \"proprietress\"), (\"monk\", \"nun\"), (\"headmasters\", \"headmistresses\"),\n",
    "    (\"grooms\", \"brides\"), (\"heir\", \"heiress\"), (\"boys\", \"girls\"),\n",
    "    (\"gentleman\", \"lady\"), (\"uncle\", \"aunt\"), (\"he\", \"she\"),\n",
    "    (\"king\", \"queen\"), (\"princes\", \"princesses\"), (\"policemen\", \"policewomen\"),\n",
    "    (\"governor\", \"matron\"), (\"fiance\", \"fiancee\"), (\"step-father\", \"step-mother\"),\n",
    "    (\"waiters\", \"waitresses\"), (\"mr\", \"mrs\"), (\"stepfather\", \"stepmother\"),\n",
    "    (\"daddy\", \"mummy\"), (\"lords\", \"ladies\"), (\"widowers\", \"widows\"),\n",
    "    (\"emperors\", \"empresses\"), (\"father-in-law\", \"mother-in-law\"), (\"abbot\", \"abbess\"),\n",
    "    (\"sir\", \"madam\"), (\"actor\", \"actress\"), (\"mr.\", \"mrs.\"),\n",
    "    (\"wizards\", \"witches\"), (\"actors\", \"actresses\"), (\"chairman\", \"chairwoman\"),\n",
    "    (\"sorcerer\", \"sorceress\"), (\"postmaster\", \"postmistress\"), (\"brothers\", \"sisters\"),\n",
    "    (\"lad\", \"lass\"), (\"headmaster\", \"headmistress\"), (\"papa\", \"mama\"),\n",
    "    (\"milkman\", \"milkmaid\"), (\"heroes\", \"heroines\"), (\"man\", \"woman\"),\n",
    "    (\"grandson\", \"granddaughter\"), (\"groom\", \"bride\"), (\"sons\", \"daughters\"),\n",
    "    (\"congressmen\", \"congresswomen\"), (\"businessman\", \"businesswoman\"), (\"boyfriends\", \"girlfriends\"),\n",
    "    (\"dads\", \"moms\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff62a3ca",
   "metadata": {},
   "source": [
    "## Load model and debias method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed8b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = TASK_LABELS[TASK]\n",
    "if TASK == 'stsb':\n",
    "    problem_type='regression'\n",
    "else:\n",
    "    problem_type='single_label_classification'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "if DEBIAS in (\"adele\", \"eat\", \"diff\"):\n",
    "    try:\n",
    "        RESULTS_PATH = f'../output/{TASK}-none-{MODEL_NAME}/'\n",
    "        CHECKPOINTS = [direction for direction in os.listdir(RESULTS_PATH) if direction.startswith('checkpoint')]\n",
    "        LAST_CHECKPOINT_PATH = RESULTS_PATH + CHECKPOINTS[-1]\n",
    "        original_model = AutoModelForSequenceClassification.from_pretrained(LAST_CHECKPOINT_PATH)\n",
    "    except:\n",
    "        RESULTS_PATH = f'output/{TASK}-none-{MODEL_NAME}/'\n",
    "        CHECKPOINTS = [direction for direction in os.listdir(RESULTS_PATH) if direction.startswith('checkpoint')]\n",
    "        LAST_CHECKPOINT_PATH = RESULTS_PATH + CHECKPOINTS[-1]\n",
    "        original_model = AutoModelForSequenceClassification.from_pretrained(LAST_CHECKPOINT_PATH)\n",
    "else:\n",
    "    original_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels, problem_type=problem_type)\n",
    "\n",
    "\n",
    "hidden_dim = original_model.config.hidden_size\n",
    "if not hasattr(original_model, 'classifier'):\n",
    "    original_model.classifier = original_model.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c41df852",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS in (\"none\", \"cda\", \"eat\"):\n",
    "    model = original_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2811098",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"embedding\":\n",
    "\n",
    "    class SentDebiasBert(SentDebiasForSequenceClassification):        \n",
    "        def _get_embedding(self, input_ids, attention_mask = None, token_type_ids = None):\n",
    "            return self.model.bert(\n",
    "                input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids\n",
    "                ).last_hidden_state[:,0,:]\n",
    "\n",
    "    class SentDebiasAverageAutoreg(SentDebiasForSequenceClassification):\n",
    "        def _get_embedding(self, input_ids, attention_mask = None, token_type_ids = None):\n",
    "            return self.model.model(\n",
    "                input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids\n",
    "                ).last_hidden_state.mean(dim = 1)\n",
    "        \n",
    "    if MODEL_NAME == 'bert-base-uncased':\n",
    "        model = SentDebiasBert(\n",
    "            model = original_model,\n",
    "            config = None,\n",
    "            tokenizer = tokenizer,\n",
    "            word_pairs = counterfactual_pairs,\n",
    "            n_components = 1,\n",
    "            n_labels = num_labels\n",
    "        )\n",
    "    else:\n",
    "        model = SentDebiasAverageAutoreg(\n",
    "            model = original_model,\n",
    "            config = None,\n",
    "            tokenizer = tokenizer,\n",
    "            word_pairs = counterfactual_pairs,\n",
    "            n_components = 1,\n",
    "            n_labels = num_labels\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4e8fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"blind\":\n",
    "\n",
    "    model = original_model\n",
    "\n",
    "    class BLINDBERTTrainer(BLINDTrainer):\n",
    "        def _get_embedding(self, inputs):\n",
    "            return self.model.bert(\n",
    "                input_ids = inputs.get(\"input_ids\"), attention_mask = inputs.get(\"attention_mask\"), token_type_ids = inputs.get(\"token_type_ids\")\n",
    "                ).last_hidden_state[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67c6127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"adele\":\n",
    "    DebiasAdapter = DebiasAdapter(model = original_model)\n",
    "    model = DebiasAdapter.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd36df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"ear\":\n",
    "    model = EARModel(\n",
    "        model = original_model,\n",
    "        ear_reg_strength = 0.01\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7625cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"selective\":\n",
    "    model = original_model\n",
    "    selective_unfreezing(model, [\"attention.self\", \"attention.output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"diff\":\n",
    "\n",
    "    def normalize_by_column(x: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "        mean = x.mean(dim=0, keepdim=True)\n",
    "        std = x.std(dim=0, keepdim=True)\n",
    "        return (x - mean) / (std + eps)\n",
    "\n",
    "    if TASK == \"stsb\":\n",
    "        loss_fn = nn.MSELoss()\n",
    "    else:\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    class DiffPrunAvgAutoReg(DiffPrunDebiasing):\n",
    "        def _forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "            outputs = self.encoder(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                token_type_ids = token_type_ids\n",
    "                )\n",
    "            return outputs.last_hidden_state.mean(dim = 1)\n",
    "\n",
    "    tokens_male = [words[0] for words in counterfactual_pairs]\n",
    "    tokens_female = [words[1] for words in counterfactual_pairs]\n",
    "\n",
    "    inputs_male = tokenizer(tokens_male, padding = True, return_tensors = \"pt\")\n",
    "    inputs_female = tokenizer(tokens_female, padding = True, return_tensors = \"pt\")\n",
    "\n",
    "    if MODEL_NAME == 'bert-base-uncased':\n",
    "        model = DiffPrunBERT(\n",
    "            head = original_model.classifier,\n",
    "            encoder = original_model.bert,\n",
    "            loss_fn = loss_fn,\n",
    "            input_ids_A = inputs_male,\n",
    "            input_ids_B = inputs_female,\n",
    "            bias_kernel = normalize_by_column,\n",
    "            upper = 10,\n",
    "            lower = -0.001,\n",
    "            lambda_bias = 0.5,\n",
    "            lambda_sparse = 0.00001\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model = DiffPrunAvgAutoReg(\n",
    "            head = original_model.classifier,\n",
    "            encoder = original_model.base_model,\n",
    "            loss_fn = loss_fn,\n",
    "            input_ids_A = inputs_male[:50],\n",
    "            input_ids_B = inputs_female[:50],\n",
    "            bias_kernel = normalize_by_column,\n",
    "            upper = 10,\n",
    "            lower = -0.001,\n",
    "            lambda_bias = 0.5,\n",
    "            lambda_sparse = 0.00001\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f7502",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73088bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, task):\n",
    "    if task in [\"sst2\", \"cola\"]:\n",
    "        return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task == \"mnli\":\n",
    "        return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task == \"qnli\":\n",
    "        return tokenizer(examples[\"question\"], examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task in [\"rte\", \"wnli\"]:\n",
    "        return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task == \"mrpc\":\n",
    "        return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task == \"qqp\":\n",
    "        return tokenizer(examples[\"question1\"], examples[\"question2\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task == \"stsb\":\n",
    "        return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "def get_metrics(task_name):\n",
    "    metric = evaluate.load(\"glue\", task_name)\n",
    "    if task_name == \"stsb\":\n",
    "        return metric, lambda logits: np.squeeze(logits, axis=-1)\n",
    "    return metric, lambda logits: np.argmax(logits, axis=-1)\n",
    "\n",
    "def compute_metrics_fn(p, task_name):\n",
    "    logits = p.predictions\n",
    "    labels = p.label_ids\n",
    "\n",
    "    if isinstance(logits, tuple) or isinstance(logits, list):\n",
    "        logits = logits[0]\n",
    "\n",
    "    metric, postprocess_fn = get_metrics(task_name)\n",
    "    predictions = postprocess_fn(logits)\n",
    "\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5a06a",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45dca282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = load_dataset(\"glue\", TASK)\n",
    "\n",
    "if CDA_METHOD[DEBIAS] and TASK != 'mnli':\n",
    "    train_dataset = Dataset.from_dict(\n",
    "        CDA(dataset['train'][:], pairs = dict(counterfactual_pairs))\n",
    "        )\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": dataset[\"validation\"],\n",
    "        \"test\": dataset[\"test\"]\n",
    "    })\n",
    "elif CDA_METHOD[DEBIAS] and TASK == 'mnli':\n",
    "    train_dataset = Dataset.from_dict(\n",
    "        CDA(dataset['train'][:], pairs = dict(counterfactual_pairs))\n",
    "        )\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation_matched\": dataset[\"validation_matched\"],\n",
    "        \"validation_mismatched\": dataset[\"validation_mismatched\"],\n",
    "        \"test_matched\": dataset[\"test_matched\"],\n",
    "        \"test_mismatched\": dataset[\"test_mismatched\"]\n",
    "    })\n",
    "\n",
    "if TASK == 'mnli':\n",
    "    dataset[\"validation\"] = dataset[\"validation_matched\"]\n",
    "    \n",
    "tokenized_datasets = dataset.map(lambda x: preprocess_function(x, tokenizer, TASK), batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7318cc",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a328879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_STRATEGY = \"epoch\"\n",
    "SAVE_STRATEGY = \"epoch\"\n",
    "LOAD_BEST_MODEL_AT_END = True\n",
    "SAVE_SAFETENSORS = True\n",
    "FP16 = True\n",
    "\n",
    "if DEBIAS == 'diff':\n",
    "    PATIENCE = 5\n",
    "    SAVE_SAFETENSORS = False\n",
    "else:\n",
    "    PATIENCE = 2\n",
    "    \n",
    "CALLBACKS = [EarlyStoppingCallback(early_stopping_patience=PATIENCE)]\n",
    "EVAL_STEPS = None\n",
    "\n",
    "# if DEBIAS in ('eat', 'diff):\n",
    "if DEBIAS == 'eat':\n",
    "    SAVE_STRATEGY = \"no\"\n",
    "    LOAD_BEST_MODEL_AT_END = False\n",
    "    CALLBACKS = None \n",
    "\n",
    "\n",
    "if DEBIAS == 'adele':\n",
    "    trainer = AdapterTrainer\n",
    "elif DEBIAS == 'blind':\n",
    "    trainer = BLINDBERTTrainer\n",
    "else:\n",
    "    trainer = Trainer\n",
    "\n",
    "\n",
    "if TASK in ('qqp', 'mnli'):\n",
    "    BATCH_SIZE = 32\n",
    "    EVAL_STRATEGY = \"steps\"\n",
    "    EVAL_STEPS = 1000\n",
    "    SAVE_STEPS = 1000\n",
    "else:\n",
    "    BATCH_SIZE = 16\n",
    "    EVAL_STRATEGY = \"epoch\"\n",
    "    EVAL_STEPS = None\n",
    "    SAVE_STEPS = None\n",
    "\n",
    "\n",
    "if LOAD_BEST_MODEL_AT_END:\n",
    "    SAVE_STRATEGY = EVAL_STRATEGY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cd0d024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_261927/3312708484.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='1605' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  32/1605 00:19 < 17:21, 1.51 it/s, Epoch 0.06/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m     add_EAT_hook(model, beta=\u001b[32m0.7\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/transformers/trainer.py:2164\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2162\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2165\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2169\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/transformers/trainer.py:2524\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2517\u001b[39m context = (\n\u001b[32m   2518\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2519\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2520\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2521\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2522\u001b[39m )\n\u001b[32m   2523\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2524\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2526\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2527\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2528\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2529\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2530\u001b[39m ):\n\u001b[32m   2531\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2532\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/transformers/trainer.py:3687\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3685\u001b[39m         scaled_loss.backward()\n\u001b[32m   3686\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3687\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3688\u001b[39m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[32m   3689\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/accelerate/accelerator.py:2355\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2354\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2355\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2356\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_lomo_optimizer:\n\u001b[32m   2357\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"output/{TASK}-{DEBIAS}-{MODEL_NAME.replace('/', '-')}\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=EVAL_STRATEGY,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=SAVE_STRATEGY,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_safetensors=SAVE_SAFETENSORS, \n",
    "    logging_dir=\"logs\",\n",
    "    load_best_model_at_end=LOAD_BEST_MODEL_AT_END,\n",
    "    metric_for_best_model=METRIC_FOR_BEST,\n",
    "    fp16=FP16,\n",
    "    greater_is_better = True\n",
    ")\n",
    "\n",
    "if DEBIAS == 'blind':\n",
    "    trainer = trainer(\n",
    "        blind_optimizer= lambda x: AdamW(x, lr=1e-5, weight_decay=WEIGHT_DECAY),\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda p: compute_metrics_fn(p, TASK),\n",
    "        callbacks=CALLBACKS,\n",
    "        optimizers=(AdamW(model.parameters(), lr=1e-5, weight_decay=WEIGHT_DECAY), None)\n",
    "    )\n",
    "else:\n",
    "    trainer = trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda p: compute_metrics_fn(p, TASK),\n",
    "        callbacks=CALLBACKS,\n",
    "        optimizers=(AdamW(model.parameters(), lr=1e-5, weight_decay=WEIGHT_DECAY), None)\n",
    "    )\n",
    "\n",
    "if DEBIAS == 'eat':\n",
    "    add_EAT_hook(model, beta=0.7)\n",
    "else:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe58ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results in  cola : {'eval_loss': 0.1315392404794693, 'eval_matthews_correlation': 0.0, 'eval_runtime': 9.9141, 'eval_samples_per_second': 105.204, 'eval_steps_per_second': 3.329, 'epoch': 0.14925373134328357}\n"
     ]
    }
   ],
   "source": [
    "if TASK == 'mnli':\n",
    "    eval_results_mismd = trainer.evaluate(tokenized_datasets[\"validation_mismatched\"])\n",
    "    eval_results_match = trainer.evaluate(tokenized_datasets[\"validation_matched\"])\n",
    "    print(\"Validation results (matched) in \", TASK, \":\", eval_results_match)\n",
    "    print(\"Validation results (mismatched) in \", TASK, \":\", eval_results_mismd)\n",
    "else:\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(\"Validation results in \", TASK, \":\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622d772",
   "metadata": {},
   "source": [
    "## WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c1af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X-A_mean_sim': 0.9595464468002319, 'X-B_mean_sim': 0.9583925604820251, 'Y-A_mean_sim': 0.9550148248672485, 'Y-B_mean_sim': 0.95367032289505, 'W1_size': 8, 'W2_size': 8, 'A1_size': 8, 'A2_size': 8, 'effect_size': -0.02097395434975624}\n"
     ]
    }
   ],
   "source": [
    "class BertWEAT(WEAT):\n",
    "    def _get_embedding(self, outputs):\n",
    "        return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "class AverageAutoregWEAT(WEAT):\n",
    "    def _get_embedding(self, outputs):\n",
    "        return outputs.last_hidden_state.mean(dim = 1)\n",
    "    \n",
    "if MODEL_NAME == 'bert-base-uncased':\n",
    "    if DEBIAS == 'diff':\n",
    "        weat = BertWEAT(model = model.encoder, tokenizer = tokenizer)\n",
    "    else:\n",
    "        try:\n",
    "            weat = BertWEAT(model = model.model.bert, tokenizer = tokenizer)\n",
    "        except:\n",
    "            try:\n",
    "                weat = BertWEAT(model = model.bert, tokenizer = tokenizer)\n",
    "            except:\n",
    "                weat = BertWEAT(model = model.base_model.bert, tokenizer = tokenizer)\n",
    "else:\n",
    "    if DEBIAS == 'diff':\n",
    "        weat = AverageAutoregWEAT(model = model.encoder, tokenizer = tokenizer)\n",
    "    else:\n",
    "        try:\n",
    "            weat = AverageAutoregWEAT(model = model.model.base_model, tokenizer = tokenizer)\n",
    "        except:\n",
    "            try:\n",
    "                weat = AverageAutoregWEAT(model = model.base_model, tokenizer = tokenizer)\n",
    "            except:\n",
    "                weat = AverageAutoregWEAT(model = model.base_model.base_model, tokenizer = tokenizer)\n",
    "\n",
    "math = ['math', 'algebra', 'geometry', 'calculus', 'equations', 'computation', 'numbers', 'addition']\n",
    "arts = ['poetry', 'art', 'dance', 'literature', 'novel', 'symphony', 'drama', 'sculpture']\n",
    "male = ['male', 'man', 'boy', 'brother', 'he', 'him', 'his', 'son']\n",
    "female = ['female', 'woman', 'girl', 'sister', 'she', 'her', 'hers', 'daughter']\n",
    "\n",
    "bias_results = weat.metric(\n",
    "    W1_words = math, W2_words = arts,\n",
    "    A1_words = male, A2_words = female,\n",
    "    pval = False\n",
    "    )\n",
    "print(bias_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e45ab",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b154252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == 'mnli':\n",
    "    with open(f\"output/{TASK}-{DEBIAS}-{MODEL_NAME.replace('/', '-')}/results.json\", \"w\") as f:\n",
    "        json.dump({\"eval_matched\": eval_results_match, \"eval_mismatched\": eval_results_mismd, \"bias\": bias_results}, f, indent=4)\n",
    "else:\n",
    "    with open(f\"output/{TASK}-{DEBIAS}-{MODEL_NAME.replace('/', '-')}/results.json\", \"w\") as f:\n",
    "        json.dump({\"eval\": eval_results, \"bias\": bias_results}, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FairLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
