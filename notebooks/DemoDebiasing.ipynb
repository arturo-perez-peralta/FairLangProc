{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab132f7",
   "metadata": {},
   "source": [
    "# Debiasing a Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abcdd76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Computing libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Check if CUDA is available and set device\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# huggging face\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "# Custom imports\n",
    "import os\n",
    "import sys\n",
    "ruta_raiz = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")) \\\n",
    "    if \"__file__\" in globals() else os.path.abspath(\"..\")\n",
    "sys.path.insert(0, ruta_raiz)\n",
    "\n",
    "from FairLangProc.datasets import BiasDataLoader\n",
    "from FairLangProc.metrics import WEAT\n",
    "\n",
    "from FairLangProc.algorithms.preprocessors import CDA, BLINDModelForClassification, SentDebiasForSequenceClassification\n",
    "from FairLangProc.algorithms.inprocessors import EARModel, DebiasAdapter, selective_unfreezing \n",
    "from FairLangProc.algorithms.intraprocessors import add_EAT_hook, DiffPrunningBERT, DiffPrunedDebiasing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc5db4",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c82514fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODELS = [\n",
    "    'bert-base-uncased',\n",
    "    'deepseek-ai/deepseek-llm-7b-base',\n",
    "    'huggyllama/llama-7b'\n",
    "]\n",
    "TASKS = [\n",
    "    \"cola\",\n",
    "    \"sst2\",\n",
    "    \"mrpc\",\n",
    "    \"stsb\",\n",
    "    \"qqp\",\n",
    "    \"mnli\",\n",
    "    \"qnli\",\n",
    "    \"rte\",\n",
    "    \"wnli\"\n",
    "]\n",
    "TASK_LABELS = {\n",
    "    \"cola\": 2,\n",
    "    \"sst2\": 2,\n",
    "    \"mrpc\": 2,\n",
    "    \"qqp\": 2,\n",
    "    \"stsb\": 1,\n",
    "    \"mnli\": 3,\n",
    "    \"qnli\": 2,\n",
    "    \"rte\": 2,\n",
    "    \"wnli\": 2,\n",
    "}\n",
    "DEBIAS_METHODS = [\n",
    "    \"none\",\n",
    "    \"cda\",\n",
    "    \"blind\",\n",
    "    \"embedding\",\n",
    "    \"ear\",\n",
    "    \"adele\",\n",
    "    \"selective\",\n",
    "    \"eat\",\n",
    "    \"diff\"\n",
    "]\n",
    "TASK_METRICS = {\n",
    "    \"cola\": \"eval_matthews_correlation\",\n",
    "    \"sst2\": \"eval_accuracy\",\n",
    "    \"mrpc\": \"eval_accuracy\",\n",
    "    \"stsb\": \"eval_pearson\",\n",
    "    \"mnli\": \"eval_accuracy\",\n",
    "    \"qnli\": \"eval_accuracy\",\n",
    "    \"rte\": \"eval_accuracy\",\n",
    "    \"wnli\": \"eval_accuracy\",\n",
    "}\n",
    "CDA_METHOD = {\n",
    "    \"none\": False,\n",
    "    \"cda\": True,\n",
    "    \"blind\": False,\n",
    "    \"embedding\": False,\n",
    "    \"ear\": False,\n",
    "    \"adele\": True,\n",
    "    \"selective\": True,\n",
    "    \"eat\": False,\n",
    "    \"diff\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4306857",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = MODELS[0]\n",
    "TASK = \"mnli\"\n",
    "DEBIAS = \"cda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7ce127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_FOR_BEST = TASK_METRICS.get(TASK, \"eval_accuracy\")\n",
    "BATCH_SIZE = 16\n",
    "WEIGHT_DECAY = 0.1\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09fb250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_pairs = [\n",
    "    (\"gods\", \"goddesses\"), (\"manager\", \"manageress\"), (\"barons\", \"baronesses\"),\n",
    "    (\"nephew\", \"niece\"), (\"prince\", \"princess\"), (\"boars\", \"sows\"),\n",
    "    (\"baron\", \"baroness\"), (\"stepfathers\", \"stepmothers\"), (\"wizard\", \"witch\"),\n",
    "    (\"father\", \"mother\"), (\"stepsons\", \"stepdaughters\"), (\"sons-in-law\", \"daughters-in-law\"),\n",
    "    (\"dukes\", \"duchesses\"), (\"boyfriend\", \"girlfriend\"), (\"fiances\", \"fiancees\"),\n",
    "    (\"dad\", \"mom\"), (\"shepherd\", \"shepherdess\"), (\"uncles\", \"aunts\"),\n",
    "    (\"beau\", \"belle\"), (\"males\", \"females\"), (\"hunter\", \"huntress\"),\n",
    "    (\"beaus\", \"belles\"), (\"grandfathers\", \"grandmothers\"), (\"lads\", \"lasses\"),\n",
    "    (\"daddies\", \"mummies\"), (\"step-son\", \"step-daughter\"), (\"masters\", \"mistresses\"),\n",
    "    (\"policeman\", \"policewoman\"), (\"nephews\", \"nieces\"), (\"brother\", \"sister\"),\n",
    "    (\"grandfather\", \"grandmother\"), (\"priest\", \"priestess\"), (\"hosts\", \"hostesses\"),\n",
    "    (\"landlord\", \"landlady\"), (\"husband\", \"wife\"), (\"poet\", \"poetess\"),\n",
    "    (\"landlords\", \"landladies\"), (\"fathers\", \"mothers\"), (\"masseur\", \"masseuse\"),\n",
    "    (\"monks\", \"nuns\"), (\"usher\", \"usherette\"), (\"hero\", \"heroine\"),\n",
    "    (\"stepson\", \"stepdaughter\"), (\"postman\", \"postwoman\"), (\"god\", \"goddess\"),\n",
    "    (\"milkmen\", \"milkmaids\"), (\"stags\", \"hinds\"), (\"grandpa\", \"grandma\"),\n",
    "    (\"chairmen\", \"chairwomen\"), (\"husbands\", \"wives\"), (\"grandpas\", \"grandmas\"),\n",
    "    (\"stewards\", \"stewardesses\"), (\"murderer\", \"murderess\"), (\"manservant\", \"maidservant\"),\n",
    "    (\"men\", \"women\"), (\"host\", \"hostess\"), (\"heirs\", \"heiresses\"),\n",
    "    (\"masseurs\", \"masseuses\"), (\"boy\", \"girl\"), (\"male\", \"female\"),\n",
    "    (\"son-in-law\", \"daughter-in-law\"), (\"waiter\", \"waitress\"), (\"tutors\", \"governesses\"),\n",
    "    (\"priests\", \"priestesses\"), (\"bachelor\", \"spinster\"), (\"millionaire\", \"millionairess\"),\n",
    "    (\"steward\", \"stewardess\"), (\"businessmen\", \"businesswomen\"), (\"congressman\", \"congresswoman\"),\n",
    "    (\"emperor\", \"empress\"), (\"duke\", \"duchess\"), (\"sire\", \"dam\"),\n",
    "    (\"son\", \"daughter\"), (\"sirs\", \"madams\"), (\"widower\", \"widow\"),\n",
    "    (\"kings\", \"queens\"), (\"papas\", \"mamas\"), (\"grandsons\", \"granddaughters\"),\n",
    "    (\"proprietor\", \"proprietress\"), (\"monk\", \"nun\"), (\"headmasters\", \"headmistresses\"),\n",
    "    (\"grooms\", \"brides\"), (\"heir\", \"heiress\"), (\"boys\", \"girls\"),\n",
    "    (\"gentleman\", \"lady\"), (\"uncle\", \"aunt\"), (\"he\", \"she\"),\n",
    "    (\"king\", \"queen\"), (\"princes\", \"princesses\"), (\"policemen\", \"policewomen\"),\n",
    "    (\"governor\", \"matron\"), (\"fiance\", \"fiancee\"), (\"step-father\", \"step-mother\"),\n",
    "    (\"waiters\", \"waitresses\"), (\"mr\", \"mrs\"), (\"stepfather\", \"stepmother\"),\n",
    "    (\"daddy\", \"mummy\"), (\"lords\", \"ladies\"), (\"widowers\", \"widows\"),\n",
    "    (\"emperors\", \"empresses\"), (\"father-in-law\", \"mother-in-law\"), (\"abbot\", \"abbess\"),\n",
    "    (\"sir\", \"madam\"), (\"actor\", \"actress\"), (\"mr.\", \"mrs.\"),\n",
    "    (\"wizards\", \"witches\"), (\"actors\", \"actresses\"), (\"chairman\", \"chairwoman\"),\n",
    "    (\"sorcerer\", \"sorceress\"), (\"postmaster\", \"postmistress\"), (\"brothers\", \"sisters\"),\n",
    "    (\"lad\", \"lass\"), (\"headmaster\", \"headmistress\"), (\"papa\", \"mama\"),\n",
    "    (\"milkman\", \"milkmaid\"), (\"heroes\", \"heroines\"), (\"man\", \"woman\"),\n",
    "    (\"grandson\", \"granddaughter\"), (\"groom\", \"bride\"), (\"sons\", \"daughters\"),\n",
    "    (\"congressmen\", \"congresswomen\"), (\"businessman\", \"businesswoman\"), (\"boyfriends\", \"girlfriends\"),\n",
    "    (\"dads\", \"moms\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff62a3ca",
   "metadata": {},
   "source": [
    "## Load model and debias method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eed8b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = TASK_LABELS[TASK]\n",
    "if TASK == 'stsb':\n",
    "    problem_type='regression'\n",
    "else:\n",
    "    problem_type='single_label_classification'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "original_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels, problem_type=problem_type)\n",
    "\n",
    "hidden_dim = original_model.config.hidden_size\n",
    "if not hasattr(original_model, 'classifier'):\n",
    "    original_model.classifier = original_model.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c41df852",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS in (\"none\", \"cda\", \"eat\"):\n",
    "    model = original_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2811098",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"embedding\":\n",
    "\n",
    "    class SentDebiasBert(SentDebiasForSequenceClassification):        \n",
    "        def _get_embedding(self, input_ids, attention_mask = None, token_type_ids = None):\n",
    "            return self.model.bert(\n",
    "                input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids\n",
    "                ).last_hidden_state[:,0,:]\n",
    "\n",
    "    class SentDebiasAverageAutoreg(SentDebiasForSequenceClassification):\n",
    "        def _get_embedding(self, input_ids, attention_mask = None, token_type_ids = None):\n",
    "            return self.model.model(\n",
    "                input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids\n",
    "                ).last_hidden_state.mean(dim = 1)\n",
    "        \n",
    "    if MODEL_NAME == 'bert-base-uncased':\n",
    "        model = SentDebiasBert(\n",
    "            model = original_model,\n",
    "            config = None,\n",
    "            tokenizer = tokenizer,\n",
    "            word_pairs = counterfactual_pairs,\n",
    "            n_components = 1,\n",
    "            n_labels = num_labels\n",
    "        )\n",
    "    else:\n",
    "        model = SentDebiasAverageAutoreg(\n",
    "            model = original_model,\n",
    "            config = None,\n",
    "            tokenizer = tokenizer,\n",
    "            word_pairs = counterfactual_pairs,\n",
    "            n_components = 1,\n",
    "            n_labels = num_labels\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "308ef36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"blind\":\n",
    "    \n",
    "    class BLINDBERT(BLINDModelForClassification):\n",
    "        def _get_embedding(self, input_ids = None, attention_mask = None, token_type_ids = None):\n",
    "            return self.model.bert(\n",
    "                input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids\n",
    "                ).last_hidden_state[:,0,:]\n",
    "        \n",
    "    class BLINDAverageAutoreg(BLINDModelForClassification):\n",
    "        def _get_embedding(self, input_ids = None, attention_mask = None, token_type_ids = None):\n",
    "            return self.model.model(\n",
    "                input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids\n",
    "                ).last_hidden_state.mean(dim = 1)\n",
    "        \n",
    "    if MODEL_NAME == 'bert-base-uncased':\n",
    "        model = BLINDBERT(\n",
    "            model = original_model,\n",
    "            config = None,\n",
    "            gamma=2.0,\n",
    "            temperature=1.0,\n",
    "            hidden_dim = hidden_dim,\n",
    "            n_labels = num_labels\n",
    "        )\n",
    "    else:\n",
    "        model = BLINDAverageAutoreg(\n",
    "            model = original_model,\n",
    "            config = None,\n",
    "            gamma=2.0,\n",
    "            temperature=1.0,\n",
    "            hidden_dim = hidden_dim,\n",
    "            n_labels = num_labels\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67c6127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"adele\":\n",
    "    model = DebiasAdapter(\n",
    "        model = original_model,\n",
    "        config = 'lora'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd36df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"ear\":\n",
    "    model = EARModel(\n",
    "        model = original_model,\n",
    "        ear_reg_strength = 0.01\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7625cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"selective\":\n",
    "    model = original_model\n",
    "    selective_unfreezing(model, [\"attention.self\", \"attention.output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0294e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS == \"diff\":\n",
    "\n",
    "    class DiffPrunningAvgAutoReg(DiffPrunedDebiasing):\n",
    "        def _get_embedding(self, outputs):\n",
    "            return outputs.mean(dim = 1)\n",
    "        def _get_encoder(self):\n",
    "            self.encoder = self.base_model.model\n",
    "\n",
    "    tokens_male = [words[0] for words in counterfactual_pairs]\n",
    "    tokens_female = [words[1] for words in counterfactual_pairs]\n",
    "\n",
    "    inputs_male = tokenizer(tokens_male, padding = True, return_tensors = \"pt\")\n",
    "    inputs_female = tokenizer(tokens_female, padding = True, return_tensors = \"pt\")\n",
    "\n",
    "    if MODEL_NAME == 'bert-base-uncased':\n",
    "        model = DiffPrunningBERT(\n",
    "            model = original_model,\n",
    "            input_ids_A = inputs_male,\n",
    "            input_ids_B = inputs_female\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model = DiffPrunningAvgAutoReg(\n",
    "            model = original_model,\n",
    "            input_ids_A = inputs_male,\n",
    "            input_ids_B = inputs_female\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f7502",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73088bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, task):\n",
    "    if task in [\"sst2\", \"cola\"]:\n",
    "        return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task == \"mnli\":\n",
    "        return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task == \"qnli\":\n",
    "        return tokenizer(examples[\"question\"], examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task in [\"rte\", \"wnli\"]:\n",
    "        return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task == \"mrpc\":\n",
    "        return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task == \"qqp\":\n",
    "        return tokenizer(examples[\"question1\"], examples[\"question2\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    elif task == \"stsb\":\n",
    "        return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "def get_metrics(task_name):\n",
    "    metric = evaluate.load(\"glue\", task_name)\n",
    "    if task_name == \"stsb\":\n",
    "        return metric, lambda logits: np.squeeze(logits, axis=-1)\n",
    "    return metric, lambda logits: np.argmax(logits, axis=-1)\n",
    "\n",
    "def compute_metrics_fn(p, task_name):\n",
    "    logits = p.predictions\n",
    "    labels = p.label_ids\n",
    "\n",
    "    if isinstance(logits, tuple) or isinstance(logits, list):\n",
    "        logits = logits[0]\n",
    "\n",
    "    metric, postprocess_fn = get_metrics(task_name)\n",
    "    predictions = postprocess_fn(logits)\n",
    "\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5a06a",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45dca282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a670055d4c442e9beda92f2c02d058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/449657 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset\n",
    "dataset = load_dataset(\"glue\", TASK)\n",
    "\n",
    "if CDA_METHOD[DEBIAS] and TASK != 'mnli':\n",
    "    train_dataset = Dataset.from_dict(\n",
    "        CDA(dataset['train'][:], pairs = dict(counterfactual_pairs))\n",
    "        )\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": dataset[\"validation\"],\n",
    "        \"test\": dataset[\"test\"]\n",
    "    })\n",
    "elif CDA_METHOD[DEBIAS] and TASK == 'mnli':\n",
    "    train_dataset = Dataset.from_dict(\n",
    "        CDA(dataset['train'][:], pairs = dict(counterfactual_pairs))\n",
    "        )\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation_matched\": dataset[\"validation_matched\"],\n",
    "        \"validation_mismatched\": dataset[\"validation_mismatched\"],\n",
    "        \"test_matched\": dataset[\"test_matched\"],\n",
    "        \"test_mismatched\": dataset[\"test_mismatched\"]\n",
    "    })\n",
    "\n",
    "if TASK == 'mnli':\n",
    "    dataset[\"validation\"] = dataset[\"validation_matched\"]\n",
    "    \n",
    "tokenized_datasets = dataset.map(lambda x: preprocess_function(x, tokenizer, TASK), batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7318cc",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328879c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo_perez/miniconda3/envs/FairLLM/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_267264/859754262.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     25\u001b[39m     SAVE_STRATEGY = EVAL_STRATEGY  \n\u001b[32m     28\u001b[39m training_args = TrainingArguments(\n\u001b[32m     29\u001b[39m     output_dir=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33moutput/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTASK\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEBIAS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME.replace(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     30\u001b[39m     learning_rate=\u001b[32m2e-5\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     greater_is_better = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     44\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m trainer = \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalidation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_metrics_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTASK\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAdamW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWEIGHT_DECAY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m trainer.train()\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DEBIAS == \u001b[33m'\u001b[39m\u001b[33meat\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/transformers/utils/deprecation.py:165\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS):\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    163\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/transformers/trainer.py:454\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    452\u001b[39m \u001b[38;5;28mself\u001b[39m.compute_loss_func = compute_loss_func\n\u001b[32m    453\u001b[39m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m enable_full_determinism(\u001b[38;5;28mself\u001b[39m.args.seed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.full_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[38;5;28mself\u001b[39m.hp_name = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[38;5;28mself\u001b[39m.deepspeed = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/transformers/trainer_utils.py:105\u001b[39m, in \u001b[36mset_seed\u001b[39m\u001b[34m(seed, deterministic)\u001b[39m\n\u001b[32m    103\u001b[39m np.random.seed(seed)\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     torch.cuda.manual_seed_all(seed)\n\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/torch/_compile.py:32\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     30\u001b[39m     fn.__dynamo_disable = disable_fn\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:745\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n\u001b[32m    742\u001b[39m     _is_skip_guard_eval_unsafe_stance()\n\u001b[32m    743\u001b[39m )\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    747\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/torch/random.py:46\u001b[39m, in \u001b[36mmanual_seed\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcuda\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.cuda._is_in_bad_fork():\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmps\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.mps._is_in_bad_fork():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/torch/cuda/random.py:127\u001b[39m, in \u001b[36mmanual_seed_all\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m    124\u001b[39m         default_generator = torch.cuda.default_generators[i]\n\u001b[32m    125\u001b[39m         default_generator.manual_seed(seed)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/torch/cuda/__init__.py:249\u001b[39m, in \u001b[36m_lazy_call\u001b[39m\u001b[34m(callable, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, **kwargs):\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    251\u001b[39m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[32m    252\u001b[39m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[32m    253\u001b[39m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[32m    254\u001b[39m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/FairLLM/lib/python3.13/site-packages/torch/cuda/random.py:125\u001b[39m, in \u001b[36mmanual_seed_all.<locals>.cb\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[32m    124\u001b[39m     default_generator = torch.cuda.default_generators[i]\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[43mdefault_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "EVAL_STRATEGY = \"epoch\"\n",
    "SAVE_STRATEGY = \"epoch\"\n",
    "LOAD_BEST_MODEL_AT_END = True\n",
    "EVAL_STEPS = None\n",
    "\n",
    "if DEBIAS in ('adele', 'diff', 'eat'):\n",
    "    SAVE_STRATEGY = \"no\"\n",
    "    LOAD_BEST_MODEL_AT_END = False\n",
    "\n",
    "if TASK in ('qqp', 'mnli'):\n",
    "    BATCH_SIZE = 32\n",
    "    FP16 = True\n",
    "    EVAL_STRATEGY = \"steps\"\n",
    "    EVAL_STEPS = 1000\n",
    "    SAVE_STEPS = 1000\n",
    "\n",
    "else:\n",
    "    BATCH_SIZE = 16\n",
    "    FP16 = False\n",
    "    EVAL_STRATEGY = \"epoch\"\n",
    "    EVAL_STEPS = None\n",
    "    SAVE_STEPS = None\n",
    "\n",
    "if LOAD_BEST_MODEL_AT_END:\n",
    "    SAVE_STRATEGY = EVAL_STRATEGY  \n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"output/{TASK}-{DEBIAS}-{MODEL_NAME.replace('/', '-')}\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=EVAL_STRATEGY,\n",
    "    evaluation_strategy=EVAL_STRATEGY,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=SAVE_STRATEGY,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    logging_dir=\"logs\",\n",
    "    load_best_model_at_end=LOAD_BEST_MODEL_AT_END,\n",
    "    metric_for_best_model=METRIC_FOR_BEST,\n",
    "    fp16=FP16,\n",
    "    greater_is_better = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda p: compute_metrics_fn(p, TASK),\n",
    "    optimizers=(AdamW(model.parameters(), lr=1e-5, weight_decay=WEIGHT_DECAY), None)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "if DEBIAS == 'eat':\n",
    "    add_EAT_hook(model, beta=0.7)\n",
    "\n",
    "if TASK == 'mnli':\n",
    "    eval_results_mismd = trainer.evaluate(tokenized_datasets[\"validation_mismatched\"])\n",
    "    eval_results_match = trainer.evaluate(tokenized_datasets[\"validation_matched\"])\n",
    "    print(\"Validation results (matched) in \", TASK, \":\", eval_results_match)\n",
    "    print(\"Validation results (mismatched) in \", TASK, \":\", eval_results_mismd)\n",
    "else:\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(\"Validation results in \", TASK, \":\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622d772",
   "metadata": {},
   "source": [
    "## WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c1af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X-A_mean_sim': 0.5240193605422974, 'X-B_mean_sim': 0.6104025840759277, 'Y-A_mean_sim': 0.5816237926483154, 'Y-B_mean_sim': 0.6603400707244873, 'W1_size': 8, 'W2_size': 8, 'A1_size': 8, 'A2_size': 8, 'effect_size': -0.07666268199682236}\n"
     ]
    }
   ],
   "source": [
    "class BertWEAT(WEAT):\n",
    "    def _get_embedding(self, outputs):\n",
    "        return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "class AverageAutoregWEAT(WEAT):\n",
    "    def _get_embedding(self, outputs):\n",
    "        return outputs.last_hidden_state.mean(dim = 1)\n",
    "    \n",
    "if MODEL_NAME == 'bert-base-uncased':\n",
    "    try:\n",
    "        weat = BertWEAT(model = model.model.bert, tokenizer = tokenizer)\n",
    "    except:\n",
    "        try:\n",
    "            weat = BertWEAT(model = model.bert, tokenizer = tokenizer)\n",
    "        except:\n",
    "            weat = BertWEAT(model = model.base_model.bert, tokenizer = tokenizer)\n",
    "else:\n",
    "    try:\n",
    "        weat = AverageAutoregWEAT(model = model.model.base_model, tokenizer = tokenizer)\n",
    "    except:\n",
    "        try:\n",
    "            weat = AverageAutoregWEAT(model = model.base_model, tokenizer = tokenizer)\n",
    "        except:\n",
    "            weat = AverageAutoregWEAT(model = model.base_model.base_model, tokenizer = tokenizer)\n",
    "\n",
    "math = ['math', 'algebra', 'geometry', 'calculus', 'equations', 'computation', 'numbers', 'addition']\n",
    "arts = ['poetry', 'art', 'dance', 'literature', 'novel', 'symphony', 'drama', 'sculpture']\n",
    "male = ['male', 'man', 'boy', 'brother', 'he', 'him', 'his', 'son']\n",
    "female = ['female', 'woman', 'girl', 'sister', 'she', 'her', 'hers', 'daughter']\n",
    "\n",
    "bias_results = weat.run_test(\n",
    "    W1_words = math, W2_words = arts,\n",
    "    A1_words = male, A2_words = female,\n",
    "    pval = False\n",
    "    )\n",
    "print(bias_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e45ab",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b154252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == 'mnli':\n",
    "    with open(f\"output/{TASK}-{DEBIAS}-{MODEL_NAME.replace('/', '-')}/results.json\", \"w\") as f:\n",
    "        json.dump({\"eval_matched\": eval_results_match, \"eval_mismatched\": eval_results_mismd, \"bias\": bias_results}, f, indent=4)\n",
    "else:\n",
    "    with open(f\"output/{TASK}-{DEBIAS}-{MODEL_NAME.replace('/', '-')}/results.json\", \"w\") as f:\n",
    "        json.dump({\"eval\": eval_results, \"bias\": bias_results}, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FairLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
