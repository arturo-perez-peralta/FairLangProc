<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>FairLangProc.algorithms.inprocessors package &#8212; FairLangProc 0.1.3 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=686e5160" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=360bc84d"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="FairLangProc.algorithms.intraprocessors package" href="FairLangProc.algorithms.intraprocessors.html" />
    <link rel="prev" title="FairLangProc.algorithms package" href="FairLangProc.algorithms.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="fairlangproc-algorithms-inprocessors-package">
<h1>FairLangProc.algorithms.inprocessors package<a class="headerlink" href="#fairlangproc-algorithms-inprocessors-package" title="Link to this heading">¶</a></h1>
<p>In-processors are fairness processors that modify the training process.</p>
<p>The supported methods are:</p>
<ul class="simple">
<li><p>ADELE <a class="reference external" href="https://arxiv.org/abs/2109.03646">(Lauscher et al., 2021)</a>.</p></li>
<li><p>Entropy Attention Regularizer (EAR) <a class="reference external" href="https://arxiv.org/abs/2203.09192">(Attanasio et al., 2022)</a>.</p></li>
<li><p>Selective unfreezing <a class="reference external" href="https://aclanthology.org/2022.ltedi-1.8/">(Gira et al., 2024)</a>.</p></li>
</ul>
<section id="adele">
<h2>ADELE<a class="headerlink" href="#adele" title="Link to this heading">¶</a></h2>
<p>The ADELE procedure <a class="reference external" href="https://arxiv.org/abs/2109.03646">(Lauscher et al., 2021)</a> is based on the adapter framework.
A single adapter module is included to each transformer layer after the feed-forward sub-layer, where the outputs are compressed to a bottleneck dimension
<span class="math notranslate nohighlight">\(m\)</span> and then decompressed back to the hidden size of the transformer, <span class="math notranslate nohighlight">\(d_L\)</span>. The adapter module itself consists of a two-layer feed-forward network:</p>
<div class="math notranslate nohighlight">
\[\text{Adapter}(\mathbf{h}, \mathbf{r}) = U \cdot g(D\cdot \mathbf{h) + \mathbf{r}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{r}\)</span> are the hidden state and residual of the corresponding transformer layer, <span class="math notranslate nohighlight">\(g\)</span> is an activation function and
<span class="math notranslate nohighlight">\(D \in \mathbb{R}^{m \times d_L}, U \in \mathbb{R}^{d_L\times m}\)</span> represent the projection matrices.
The idea behind the adapter layer is to introduce an information bottleneck which compresses the latent representation of the inputs,
forcing the model to discard all irrelevant information.</p>
</section>
<section id="ear">
<h2>EAR<a class="headerlink" href="#ear" title="Link to this heading">¶</a></h2>
<p>EAR <a href="#id2"><span class="problematic" id="id3">`(Attanasio et al., 2022) https://arxiv.org/abs/2203.09192`_</span></a> tries to maximize the entropy of the attention weights to encourage attention to the broader context
of the input,</p>
<div class="math notranslate nohighlight">
\[\mathcal{R} = - \sum_{l=1}^L \text{entropy}_l(\mathbf{A})\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{entropy}_l(\cdot)\)</span> denotes the entropy of the l-th layer.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">FairLangProc.algorithms.inprocessors.regularizers.</span></span><span class="sig-name descname"><span class="pre">EARModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ear_reg_strength</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.01</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/FairLangProc/algorithms/inprocessors/regularizers.html#EARModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Class for adding a regularizer based on entropy attention.</p>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">FairLangProc.algorithms.inprocessors</span><span class="w"> </span><span class="kn">import</span> <span class="n">EARModel</span>
<span class="gp">&gt;&gt;&gt; </span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">EARRegularizer</span> <span class="o">=</span> <span class="n">EARModel</span><span class="p">(</span>
<span class="go">         model = model,</span>
<span class="go">         ear_reg_strength = 0.01</span>
<span class="go">    )</span>
<span class="gp">&gt;&gt;&gt; </span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
<span class="go">        model=EARRegularizer,</span>
<span class="go">        args=training_args,</span>
<span class="go">        train_dataset=train_dataset,</span>
<span class="go">        eval_dataset=val_dataset,</span>
<span class="go">        optimizers=(</span>
<span class="go">            AdamW(EARRegularizer.parameters(), lr=1e-5, weight_decay=0.1),</span>
<span class="go">            None</span>
<span class="go">            )</span>
<span class="go">    )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ear_reg_strength</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.01</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/FairLangProc/algorithms/inprocessors/regularizers.html#EARModel.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Constructor for the EARModel class</p>
<section id="parameters">
<h4>Parameters<a class="headerlink" href="#parameters" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>model<span class="classifier">nn.Module </span></dt><dd><p>A language model.</p>
</dd>
<dt>ear_reg_strength<span class="classifier">float</span></dt><dd><p>Hyper-parameter containing the strength of the regularization term.</p>
</dd>
</dl>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="selective-unfreezing">
<h2>Selective unfreezing<a class="headerlink" href="#selective-unfreezing" title="Link to this heading">¶</a></h2>
<p>Selective unfreezing <a href="#id4"><span class="problematic" id="id5">`(Gira et al., 2024) https://aclanthology.org/2022.ltedi-1.8/`_</span></a> aims to circumvent catastrophic forgetting during fine-tuning
by freezing a big amount of the model parameters, which also helps lessening computational expenses.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">FairLangProc</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="FairLangProc.datasets.html">FairLangProc.datasets package</a></li>
<li class="toctree-l1"><a class="reference internal" href="FairLangProc.metrics.html">FairLangProc.metrics package</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="FairLangProc.algorithms.html">FairLangProc.algorithms package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="FairLangProc.algorithms.html#subpackages">Subpackages</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  <li><a href="FairLangProc.algorithms.html">FairLangProc.algorithms package</a><ul>
      <li>Previous: <a href="FairLangProc.algorithms.html" title="previous chapter">FairLangProc.algorithms package</a></li>
      <li>Next: <a href="FairLangProc.algorithms.intraprocessors.html" title="next chapter">FairLangProc.algorithms.intraprocessors package</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Arturo Perez-Peralta.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.1.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/FairLangProc.algorithms.inprocessors.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>