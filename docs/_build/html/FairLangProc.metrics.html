<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>FairLangProc.metrics package &#8212; FairLangProc 0.1.3 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=686e5160" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=360bc84d"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="FairLangProc.algorithms package" href="FairLangProc.algorithms.html" />
    <link rel="prev" title="FairLangProc.datasets package" href="FairLangProc.datasets.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="fairlangproc-metrics-package">
<h1>FairLangProc.metrics package<a class="headerlink" href="#fairlangproc-metrics-package" title="Link to this heading">¶</a></h1>
<p>FairLangProc supports different fairness metrics to measure discrimination in NLP. Broadly, they can be classified into three categories:</p>
<ul class="simple">
<li><p><strong>Embedding metrics</strong>: if they measure bias by examining the model’s hidden representations of input text.</p></li>
<li><p><strong>Probability metrics</strong>: if they measure bias by computing the probabilities of certain tokens or sentences.</p></li>
<li><p><strong>Generated text metrics</strong>: if they measure bias by examining text generated by the model, looking for harmful or stereotypical words.</p></li>
</ul>
<p>The supported metrics are:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#weat"><span class="std std-ref">Generalized association tests (WEAT)</span></a> <a class="reference external" href="https://arxiv.org/abs/1608.07187">(Caliskan et al., 2016)</a>.</p></li>
<li><p><a class="reference internal" href="#lpbs"><span class="std std-ref">Log Probability Bias Score (LPBS)</span></a> <a class="reference external" href="https://arxiv.org/abs/1906.07337">(Kurita et al., 2019)</a>.</p></li>
<li><p><a class="reference internal" href="#cbs"><span class="std std-ref">Categorical Bias Score (CBS)</span></a> <a class="reference external" href="https://aclanthology.org/2021.emnlp-main.42/">(Ahn et al., 2021)</a>.</p></li>
<li><p><a class="reference internal" href="#cps"><span class="std std-ref">CrowS-Pairs Score (CPS)</span></a> <a class="reference external" href="https://aclanthology.org/2020.emnlp-main.154/">(Nangia et al., 2020)</a>.</p></li>
<li><p><a class="reference internal" href="#aul"><span class="std std-ref">All Unmasked Score (AUL)</span></a> <a class="reference external" href="https://arxiv.org/abs/2104.07496">(Kaneko et al., 2021)</a>.</p></li>
<li><p><a class="reference internal" href="#dr"><span class="std std-ref">Demographic Representation (DR)</span></a> <a class="reference external" href="https://arxiv.org/abs/2211.09110">(Liang et al., 2022)</a>.</p></li>
<li><p><a class="reference internal" href="#sa"><span class="std std-ref">Stereotypical Association (SA)</span></a> <a class="reference external" href="https://arxiv.org/abs/2211.09110">(Liang et al., 2022)</a>.</p></li>
<li><p><a class="reference internal" href="#honest"><span class="std std-ref">HONEST</span></a> <a class="reference external" href="https://aclanthology.org/2021.naacl-main.191/">(Nozza et al., 2021)</a>.</p></li>
</ul>
<section id="weat">
<span id="id2"></span><h2>WEAT<a class="headerlink" href="#weat" title="Link to this heading">¶</a></h2>
<p>The most famous embedding metric is given the Word Embedding Association Test (WEAT) <a class="reference external" href="https://arxiv.org/abs/1608.07187">(Caliskan et al., 2016)</a>,
which aims to measure associations between demographic and neutral attributes. Demographic attributes are usually binary and denoted by <span class="math notranslate nohighlight">\(A_1, A_2\)</span>,
denoting two different societal groups (male and female, christians and atheist,…). Neutral attributes, on the other hand, are denoted by <span class="math notranslate nohighlight">\(W_1, W_2\)</span>
and represent two different stereotypes whose demographic association we are interested in. These stereotypes can be occupational
(technical and care work, proffesions and home roles,…), academic (mathematics and arts, engineering and social sciences, medicine and nursing,…)
or related to prejudice (competence and incompetence, insults and praises,..), and their association with a societal group indicates an existing bias towards said group.
Embedding metrics measure this association through the cosine similarity of the embeddings of words belonging to text corpora associated with each neutral attribute,
<span class="math notranslate nohighlight">\(\mathbb{W}_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[s(a, W_1, W_2) = \sum_{w_1\in \mathbb{W}_1} \frac{\cos(a, w_1)}{|\mathbb{W}_1|} - \sum_{w_2\in \mathbb{W}_2} \frac{\cos(a, w_2)}{|\mathbb{W}_2|},\]</div>
<p>where <span class="math notranslate nohighlight">\(a\)</span> represents the embedding of an arbitrary word and $s$ represents its similarity to the neutral attributes,
with a positive score signifying an association with <span class="math notranslate nohighlight">\(W_1\)</span> while a negative score implies a correlation with <span class="math notranslate nohighlight">\(W_2\)</span>.
WEAT then measures bias through the effect size, which computes the average similarity between a corpora of text related to each sensitive attribute,
<span class="math notranslate nohighlight">\(\mathbb{A}_i\)</span>, and the neutral parameters:</p>
<div class="math notranslate nohighlight">
\[WEAT(A_1, A_2, W_1, W_2) = \frac{\sum_{a_1 \in A_1} s(a_1, W_1, W_2)/ |A_1| - \sum_{a_2 \in A_2} s(a_2, W_1, w_2)/ |A_2| }{\text{std}_{a\in A_1 \cup A_2} s(a, W_1, W_2)}.\]</div>
<p>A large effect size in either direction indicates a strong bias at the semantic level. This test can be run at the word or sentence level,
and it can be further generalized to contextualized embeddings. These metrics are implemented through the <cite>WEAT</cite> abstract class</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.embedding.</span></span><span class="sig-name descname"><span class="pre">WEAT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TokenizerType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cuda'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/FairLangProc/metrics/embedding.html#WEAT"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Class for handling WEAT metric with a PyTorch model and tokenizer.</p>
<section id="attributes">
<h3>Attributes<a class="headerlink" href="#attributes" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model<span class="classifier">nn.Module     </span></dt><dd><p>PyTorch model (e.g., BERT, GPT from HuggingFace).</p>
</dd>
<dt>tokenizer<span class="classifier">TokenizerType</span></dt><dd><p>Tokenizer for the model.</p>
</dd>
<dt>device<span class="classifier">str</span></dt><dd><p>Device to run the WEAT test on.</p>
</dd>
</dl>
</section>
<section id="methods">
<h3>Methods<a class="headerlink" href="#methods" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>metric(W1_words, W2_words, A1_words, A2_words, n_perm, pval)</dt><dd><p>Computation of the WEAT effect size between W1, W2 and A1, A2.</p>
</dd>
<dt>_get_embedding(outputs)</dt><dd><p>Abstract method whose implementation is required and which aims to compute the embedding of an output given
by the model.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TokenizerType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cuda'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/embedding.html#WEAT.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Constructor for the WEAT class</p>
<section id="parameters">
<h4>Parameters<a class="headerlink" href="#parameters" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>model<span class="classifier">nn.Module     </span></dt><dd><p>PyTorch model (e.g., BERT, GPT from HuggingFace).</p>
</dd>
<dt>tokenizer<span class="classifier">TokenizerType</span></dt><dd><p>Tokenizer for the model.</p>
</dd>
<dt>device<span class="classifier">str</span></dt><dd><p>Device to run the WEAT test on.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_get_embedding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/FairLangProc/metrics/embedding.html#WEAT._get_embedding"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Abstract method that instructs the class on how to obtain the embedding of a given input.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">metric</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">W1_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">W2_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">A1_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">A2_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_perm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pval</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/embedding.html#WEAT.metric"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Run WEAT test.</p>
<section id="id4">
<h4>Parameters<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>W1_words<span class="classifier">list[str]</span></dt><dd><p>Target concept 1 words/sentences</p>
</dd>
<dt>W2_words<span class="classifier">list[str]</span></dt><dd><p>Target concept 2 words</p>
</dd>
<dt>A1_words<span class="classifier">list[str]</span></dt><dd><p>Attribute 1 words/sentences</p>
</dd>
<dt>A2_words<span class="classifier">list[str]</span></dt><dd><p>Attribute 2 words/sentences</p>
</dd>
<dt>n_perm<span class="classifier">int</span></dt><dd><p>Number of permutations for p-value</p>
</dd>
<dt>pval<span class="classifier">bool</span></dt><dd><p>Whether to compute or not the p-value</p>
</dd>
</dl>
</section>
<section id="returns">
<h4>Returns<a class="headerlink" href="#returns" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>results<span class="classifier">dict[str, float]</span></dt><dd><p>Dictionary with test results, namely mean similarity between W1, W2 and A1, A2; their sizes,
the WEAT effect size and the p-value if needed.</p>
</dd>
</dl>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="demographic-representation">
<span id="dr"></span><h2>Demographic Representation<a class="headerlink" href="#demographic-representation" title="Link to this heading">¶</a></h2>
<p>DR <a class="reference external" href="https://arxiv.org/abs/2211.09110">(Liang et al., 2022)</a> is computed as follows: Given a corpus of sequences of generated text
<span class="math notranslate nohighlight">\(\hat{\mathbb{Y}}\)</span>, for each societal group <span class="math notranslate nohighlight">\(a\)</span> with associated words <span class="math notranslate nohighlight">\(\mathbb{A}\)</span> its demographic representation is given by</p>
<div class="math notranslate nohighlight">
\[\text{DR}(a) = \sum_{w_i \in \mathbb{A}}\sum_{\hat{Y} \in \hat{\mathbb{Y}}} C(w_i, \hat{Y}),\]</div>
<p>where <span class="math notranslate nohighlight">\(C(w, Y)\)</span> denotes the count of how many times word <span class="math notranslate nohighlight">\(w\)</span> appears in text <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.generated_text.</span></span><span class="sig-name descname"><span class="pre">DemRep</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">demWords</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sentences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/generated_text.html#DemRep"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Computes Demographic representation.</p>
<section id="id6">
<h3>Parameters<a class="headerlink" href="#id6" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>demWords<span class="classifier">dict[str, list[str]]</span></dt><dd><p>Dictionary whose keys represent demographic attributes
and whose values represent words with demographic meaning.</p>
</dd>
<dt>sentences<span class="classifier">list[str]</span></dt><dd><p>List of sentences to run the demographic representation.</p>
</dd>
</dl>
</section>
<section id="id7">
<h3>Returns<a class="headerlink" href="#id7" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>demRepVect<span class="classifier">dict[str, int]</span></dt><dd><p>Dictionary with demographic counts for all considered words and sentences.</p>
</dd>
</dl>
</section>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gendered_words</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s1">&#39;male&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;he&#39;</span><span class="p">,</span> <span class="s1">&#39;him&#39;</span><span class="p">,</span> <span class="s1">&#39;his&#39;</span><span class="p">],</span>
<span class="gp">... </span>    <span class="s1">&#39;female&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;she&#39;</span><span class="p">,</span> <span class="s1">&#39;her&#39;</span><span class="p">,</span> <span class="s1">&#39;actress&#39;</span><span class="p">,</span> <span class="s1">&#39;hers&#39;</span><span class="p">]</span>
<span class="gp">... </span>    <span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="s1">&#39;She is such a good match to him.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;He is trying way too hard to be an actor.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;Her mother is trying to make ends meet.&#39;</span>
<span class="gp">... </span>    <span class="s1">&#39;My aunt is baking, do you want to try?&#39;</span>
<span class="gp">... </span>    <span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">DR</span> <span class="o">=</span> <span class="n">DemRep</span><span class="p">(</span>
<span class="go">        sentences = sentences,</span>
<span class="go">        demWords = gendered_words</span>
<span class="go">        )   </span>
</pre></div>
</div>
</section>
</dd></dl>

</section>
<section id="stereotypical-association">
<span id="sa"></span><h2>Stereotypical Association<a class="headerlink" href="#stereotypical-association" title="Link to this heading">¶</a></h2>
<p>ST <a class="reference external" href="https://arxiv.org/abs/2211.09110">(Liang et al., 2022)</a> is computed as follows: Given a specific term <span class="math notranslate nohighlight">\(w\)</span>, a corpus of sequences of generated text
<span class="math notranslate nohighlight">\(\hat{\mathbb{Y}}\)</span>, for each societal group <span class="math notranslate nohighlight">\(a\)</span> with associated words <span class="math notranslate nohighlight">\(\mathbb{A}\)</span> its stereotypical association is given by</p>
<div class="math notranslate nohighlight">
\[\text{DR}(a) = \sum_{w_i \in \mathbb{A}}\sum_{\hat{Y} \in \hat{\mathbb{Y}}} C(w_i, \hat{Y}),\]</div>
<p>where <span class="math notranslate nohighlight">\(C(w, Y)\)</span> denotes the count of how many times word <span class="math notranslate nohighlight">\(w\)</span> appears in text <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.generated_text.</span></span><span class="sig-name descname"><span class="pre">StereoAsoc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">targetWords</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">demWords</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sentences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/generated_text.html#StereoAsoc"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Computes Stereotypical Association</p>
<section id="id9">
<h3>Parameters<a class="headerlink" href="#id9" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>targetWords<span class="classifier">list[str]</span></dt><dd><p>List of words whose associations we want to compute.</p>
</dd>
<dt>demWords<span class="classifier">dict[str, list[str]]</span></dt><dd><p>Dictionary whose keys represent demographic attributes
and whose values represent words with demographic meaning.</p>
</dd>
<dt>sentences<span class="classifier">list[str]</span></dt><dd><p>List of sentences to run the stereotypical association.</p>
</dd>
</dl>
</section>
<section id="id10">
<h3>Returns<a class="headerlink" href="#id10" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>steAsocVect<span class="classifier">dict</span></dt><dd><p>Dictionary which stores demographic counts for all considered
words and sentences indexed by targetWords.</p>
</dd>
</dl>
</section>
<section id="id11">
<h3>Example<a class="headerlink" href="#id11" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gendered_words</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s1">&#39;male&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;he&#39;</span><span class="p">,</span> <span class="s1">&#39;him&#39;</span><span class="p">,</span> <span class="s1">&#39;his&#39;</span><span class="p">],</span>
<span class="gp">... </span>    <span class="s1">&#39;female&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;she&#39;</span><span class="p">,</span> <span class="s1">&#39;her&#39;</span><span class="p">,</span> <span class="s1">&#39;actress&#39;</span><span class="p">,</span> <span class="s1">&#39;hers&#39;</span><span class="p">]</span>
<span class="gp">... </span>    <span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="s1">&#39;She is such a good match to him.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;He is trying way too hard to be an actor.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;Her mother is trying to make ends meet.&#39;</span>
<span class="gp">... </span>    <span class="s1">&#39;My aunt is baking, do you want to try?&#39;</span>
<span class="gp">... </span>    <span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mother&#39;</span><span class="p">,</span> <span class="s1">&#39;baking&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ST</span> <span class="o">=</span> <span class="n">StereoAsoc</span><span class="p">(</span>
<span class="go">        sentences = sentences,</span>
<span class="go">        demWords = gendered_words,</span>
<span class="go">        targetWords = target_words</span>
<span class="go">        )</span>
</pre></div>
</div>
</section>
</dd></dl>

</section>
<section id="honest">
<span id="id12"></span><h2>HONEST<a class="headerlink" href="#honest" title="Link to this heading">¶</a></h2>
<p>HONEST <a class="reference external" href="https://aclanthology.org/2021.naacl-main.191/">(Nozza et al., 2021)</a> measures how many of the top <span class="math notranslate nohighlight">\(k`\)</span> completions of a given model,
<span class="math notranslate nohighlight">\(\hat{\mathbb{Y}}_k\)</span>, contain harmful words measured by:</p>
<div class="math notranslate nohighlight">
\[\text{HONEST}(\hat{\mathbb{Y}} ) = \frac{\sum_{\hat{Y}_k \in\hat{\mathbb{Y}}_k} \sum_{\hat{y} \in \hat{Y}_k} \mathbf{1}(\hat{y} \in \mathbb{Y}_{hurt} ) }{|\mathbb{\hat{Y}}| k}.\]</div>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.generated_text.</span></span><span class="sig-name descname"><span class="pre">HONEST</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">completions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hurtLex</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/generated_text.html#HONEST"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Computes HONEST score for a set of completions given a hurt lexicon.</p>
<section id="id14">
<h3>Parameters<a class="headerlink" href="#id14" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>completions<span class="classifier">list[list[str]]</span></dt><dd><p>List of completions given by a model. This is a list of lists: it should contain lists
of completions of the same size. The lists of completions are lists of constant size
containing the completions of a given model for a certain prompt.</p>
</dd>
<dt>hurtLex<span class="classifier">list[str]</span></dt><dd><p>List of words belonging to the hurt lexicon.</p>
</dd>
</dl>
</section>
<section id="id15">
<h3>Returns<a class="headerlink" href="#id15" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>honestScore<span class="classifier">float</span></dt><dd><p>HONEST score for the given sentences and hurt lexicon.</p>
</dd>
</dl>
</section>
<section id="id16">
<h3>Example<a class="headerlink" href="#id16" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="s1">&#39;I cannot believe she did that, she is so&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;I cannot believe he did that, he is so&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;She tries and she tries to no avail. She is such a&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;He tries and she tries to no avail. He is such a&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">completions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<span class="gp">... </span>        <span class="n">input_ids</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">early_stopping</span><span class="o">=</span><span class="kc">True</span>
<span class="gp">... </span>    <span class="p">)</span>
<span class="gp">... </span>    <span class="n">completions</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hurtLex</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;fool&#39;</span><span class="p">,</span> <span class="s1">&#39;bitch&#39;</span><span class="p">,</span> <span class="s1">&#39;stupid&#39;</span><span class="p">,</span> <span class="s1">&#39;incompetent&#39;</span><span class="p">,</span> <span class="s1">&#39;idiot&#39;</span><span class="p">,</span> <span class="s1">&#39;dumb&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">honestScore</span> <span class="o">=</span> <span class="n">HONEST</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">completions</span> <span class="o">=</span> <span class="n">completions</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">hurtLex</span> <span class="o">=</span> <span class="n">hurtLex</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</section>
</dd></dl>

</section>
<section id="lpbs">
<span id="id17"></span><h2>LPBS<a class="headerlink" href="#lpbs" title="Link to this heading">¶</a></h2>
<p>LPBS <a class="reference external" href="https://arxiv.org/abs/1906.07337">(Kurita et al., 2019)</a> measures bias for a binary demographic group. It computes the predicted probability for a token <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(p_a\)</span>,
using the template “[MASK] is [NEUTRAL ATTRIBUTE]”; and normalizes it by computing the model’s prior probability, <span class="math notranslate nohighlight">\(p_{a, prior}\)</span>,
based on the template “[MASK] is [MASK]”. The score is computed as the difference of the logarithms of the normalized probabilities,</p>
<div class="math notranslate nohighlight">
\[\text{LPBS} = \log\frac{p_1}{p_{prior, 1}} - \log\frac{p_2}{p_{prior, 2}}.\]</div>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.probability.</span></span><span class="sig-name descname"><span class="pre">LPBS</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TokenizerType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sentences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/probability.html#LPBS"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Computes LPBS score for a list of tuples of dimension 2 of target words.</p>
<section id="id19">
<h3>Parameters<a class="headerlink" href="#id19" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model<span class="classifier">nn.Module                  </span></dt><dd><p>Language model used to compute probabilities.</p>
</dd>
<dt>tokenizer<span class="classifier">TokenizerType              </span></dt><dd><p>Tokenizer associated with the model.</p>
</dd>
<dt>sentences<span class="classifier">list[str]              </span></dt><dd><p>List of sentences with masks.</p>
</dd>
<dt>target_words<span class="classifier">list[tuple[str]]    </span></dt><dd><p>List containing tuples of words whose probabilities we want to compute.</p>
</dd>
<dt>fill_words<span class="classifier">list[str]             </span></dt><dd><p>List of words which replace the secondary mask.</p>
</dd>
<dt>mask_indices<span class="classifier">list[int]           </span></dt><dd><p>List of indices which indicate to which mask of the sentence each 
target word corresponds (i.e. first (0) or second (1)).</p>
</dd>
</dl>
</section>
<section id="id20">
<h3>Returns<a class="headerlink" href="#id20" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>probs<span class="classifier">torch.Tensor               </span></dt><dd><p>List of LPBS scores</p>
</dd>
</dl>
</section>
<section id="id21">
<h3>Example<a class="headerlink" href="#id21" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;[MASK] is a [MASK].&quot;</span><span class="p">,</span> <span class="s2">&quot;[MASK] is a [MASK].&quot;</span><span class="p">,</span> <span class="s2">&quot;The [MASK] was a [MASK].&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_words</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;John&quot;</span><span class="p">,</span> <span class="s2">&quot;Mary&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;He&quot;</span><span class="p">,</span> <span class="s2">&quot;She&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="s2">&quot;woman&quot;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fill_words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;engineer&quot;</span><span class="p">,</span><span class="s2">&quot;nurse&quot;</span><span class="p">,</span><span class="s2">&quot;doctor&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask_indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">LPBSscore</span> <span class="o">=</span> <span class="n">LPBS</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">sentences</span> <span class="o">=</span> <span class="n">sentences</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">target_words</span> <span class="o">=</span> <span class="n">target_words</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">fill_words</span> <span class="o">=</span> <span class="n">fill_words</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">mask_indices</span> <span class="o">=</span> <span class="n">mask_indices</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</section>
</dd></dl>

</section>
<section id="cbs">
<span id="id22"></span><h2>CBS<a class="headerlink" href="#cbs" title="Link to this heading">¶</a></h2>
<p>CBS <a class="reference external" href="https://aclanthology.org/2021.emnlp-main.42/">(Ahn et al., 2021)</a> generalizes measurent of bias for non-binary demographic groups. It computes the predicted probability for a token <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(p_a\)</span>,
using the template “[MASK] is [NEUTRAL ATTRIBUTE]”; and normalizes it by computing the model’s prior probability, <span class="math notranslate nohighlight">\(p_{a, prior}\)</span>,
based on the template “[MASK] is [MASK]”. The score is computed as the variance of the logarithms of the normalized probabilities,</p>
<div class="math notranslate nohighlight">
\[\text{CBS} =  \text{Var}_{a\in \mathbb{A}}\log\frac{p_a}{p_{prior, a}}.\]</div>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.probability.</span></span><span class="sig-name descname"><span class="pre">CBS</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TokenizerType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sentences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/probability.html#CBS"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Computes CBS score for a list of tuples of dimension n of target words.</p>
<section id="id24">
<h3>Parameters<a class="headerlink" href="#id24" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model<span class="classifier">nn.Module                  </span></dt><dd><p>Language model used to compute probabilities.</p>
</dd>
<dt>tokenizer<span class="classifier">TokenizerType              </span></dt><dd><p>Tokenizer associated with the model</p>
</dd>
<dt>sentences<span class="classifier">list[str]     </span></dt><dd><p>List of sentences with masks</p>
</dd>
<dt>target_words<span class="classifier">list[tuple[str]]  </span></dt><dd><p>List containing tuples of words whose probabilities we want to compute</p>
</dd>
<dt>fill_words<span class="classifier">list[str]          </span></dt><dd><p>List of words which replace the secondary mask</p>
</dd>
<dt>mask_indices<span class="classifier">list[int]          </span></dt><dd><p>List of indices which indicate to which mask of the sentence
each target word corresponds (i.e. first (0) or second (1))</p>
</dd>
</dl>
</section>
<section id="id25">
<h3>Returns<a class="headerlink" href="#id25" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>probs<span class="classifier">torch.Tensor</span></dt><dd><p>List of CBS scores</p>
</dd>
</dl>
</section>
<section id="id26">
<h3>Example<a class="headerlink" href="#id26" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_words</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;John&quot;</span><span class="p">,</span> <span class="s2">&quot;Mamadouk&quot;</span><span class="p">,</span> <span class="s2">&quot;Liu&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;white&quot;</span><span class="p">,</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="s2">&quot;asian&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;white&quot;</span><span class="p">,</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="s2">&quot;asian&quot;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;[MASK] is a [MASK]&quot;</span><span class="p">,</span> <span class="s2">&quot;The [MASK] kid got [MASK] results&quot;</span><span class="p">,</span> <span class="s2">&quot;The [MASK] kid wanted to be a [MASK]&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fill_words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;outstanding&quot;</span><span class="p">,</span> <span class="s2">&quot;doctor&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask_indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">CBSscore</span> <span class="o">=</span> <span class="n">CBS</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">sentences</span> <span class="o">=</span> <span class="n">sentences</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">target_words</span> <span class="o">=</span> <span class="n">target_words</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">fill_words</span> <span class="o">=</span> <span class="n">fill_words</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">mask_indices</span> <span class="o">=</span> <span class="n">mask_indices</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</section>
</dd></dl>

</section>
<section id="cps">
<span id="id27"></span><h2>CPS<a class="headerlink" href="#cps" title="Link to this heading">¶</a></h2>
<p>CPS <a class="reference external" href="https://aclanthology.org/2020.emnlp-main.154/">(Nangia et al., 2020)</a> uses sentence pairs which coincide in a series of unmodified tokens,
<span class="math notranslate nohighlight">\(U\)</span>, and only differ on words containing demographic information, <span class="math notranslate nohighlight">\(A\)</span>. The score is given by</p>
<div class="math notranslate nohighlight">
\[\text{CPS}(S) = \sum_{u\in U} \log \mathbb{P}(u| U_{\backslash u}, A),\]</div>
<p>that is, we compute the pseudo-loglikelihood resulting from progessively masking every token but the sensitive ones.</p>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.probability.</span></span><span class="sig-name descname"><span class="pre">CPS</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TokenizerType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sentences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/probability.html#CPS"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Computes the CPS score for list of sentences.</p>
<section id="id29">
<h3>Parameters<a class="headerlink" href="#id29" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model<span class="classifier">nn.Module</span></dt><dd><p>Language model used to compute probabilities.</p>
</dd>
<dt>tokenizer<span class="classifier">TokenizerType</span></dt><dd><p>Tokenizer associated with the model.</p>
</dd>
<dt>sentences<span class="classifier">list[str]</span></dt><dd><p>List of sentences for whom we will compute the CPS score.</p>
</dd>
<dt>target_words<span class="classifier">list[str]</span></dt><dd><p>List of target words which should not be masked.</p>
</dd>
</dl>
</section>
<section id="id30">
<h3>Returns<a class="headerlink" href="#id30" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>score<span class="classifier">list[float]</span></dt><dd><p>List of CPS score of the sentences.</p>
</dd>
</dl>
</section>
<section id="id31">
<h3>Example<a class="headerlink" href="#id31" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;The actor did a terrible job&#39;</span><span class="p">,</span> <span class="s1">&#39;The actress did a terrible job&#39;</span><span class="p">,</span> <span class="s1">&#39;The doctor was an exemplary man&#39;</span><span class="p">,</span> <span class="s1">&#39;The doctor was an exemplary woman&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;actor&#39;</span><span class="p">,</span> <span class="s1">&#39;actress&#39;</span><span class="p">,</span> <span class="s1">&#39;man&#39;</span><span class="p">,</span> <span class="s1">&#39;woman&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">CPSscore</span> <span class="o">=</span> <span class="n">CPS</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">sentences</span> <span class="o">=</span> <span class="n">sentences</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">target_words</span> <span class="o">=</span> <span class="n">target_words</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</section>
</dd></dl>

</section>
<section id="aul">
<span id="id32"></span><h2>AUL<a class="headerlink" href="#aul" title="Link to this heading">¶</a></h2>
<p>AUL <a class="reference external" href="https://arxiv.org/abs/2104.07496">(Kaneko et al., 2021)</a> predicts the probability of all tokens in the sentence without masking to prevent selection bias,</p>
<div class="math notranslate nohighlight">
\[\text{AUL}(S) = \frac{1}{|S|} \sum_{s\in S} \log \mathbb{P}(s|S).\]</div>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.probability.</span></span><span class="sig-name descname"><span class="pre">AUL</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TokenizerType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sentences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/probability.html#AUL"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Computes the AUL score for list of sentences.</p>
<section id="id34">
<h3>Parameters<a class="headerlink" href="#id34" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model<span class="classifier">nn.Module</span></dt><dd><p>Language model used to compute probabilities.</p>
</dd>
<dt>tokenizer<span class="classifier">TokenizerType</span></dt><dd><p>Tokenizer associated with the model.</p>
</dd>
<dt>sentences<span class="classifier">list[str]</span></dt><dd><p>List of sentences for whom we will compute the AUL score.</p>
</dd>
</dl>
</section>
<section id="id35">
<h3>Returns<a class="headerlink" href="#id35" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>score<span class="classifier">list[float]</span></dt><dd><p>List of AUL score of the sentences.</p>
</dd>
</dl>
</section>
<section id="id36">
<h3>Example<a class="headerlink" href="#id36" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;The actor did a terrible job&#39;</span><span class="p">,</span> <span class="s1">&#39;The actress did a terrible job&#39;</span><span class="p">,</span> <span class="s1">&#39;The doctor was an exemplary man&#39;</span><span class="p">,</span> <span class="s1">&#39;The doctor was an exemplary woman&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">AULscore</span> <span class="o">=</span> <span class="n">AUL</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">sentences</span> <span class="o">=</span> <span class="n">sentences</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</section>
</dd></dl>

</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">FairLangProc</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="FairLangProc.datasets.html">FairLangProc.datasets package</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">FairLangProc.metrics package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#weat">WEAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="#demographic-representation">Demographic Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#stereotypical-association">Stereotypical Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="#honest">HONEST</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lpbs">LPBS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cbs">CBS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cps">CPS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#aul">AUL</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="FairLangProc.algorithms.html">FairLangProc.algorithms package</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="FairLangProc.datasets.html" title="previous chapter">FairLangProc.datasets package</a></li>
      <li>Next: <a href="FairLangProc.algorithms.html" title="next chapter">FairLangProc.algorithms package</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Arturo Perez-Peralta.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.1.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/FairLangProc.metrics.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>