<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>FairLangProc.metrics package &#8212; FairLangProc 0.1.3 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=686e5160" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=360bc84d"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="fairlangproc-metrics-package">
<h1>FairLangProc.metrics package<a class="headerlink" href="#fairlangproc-metrics-package" title="Link to this heading">¶</a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Link to this heading">¶</a></h2>
</section>
<section id="module-FairLangProc.metrics.embedding">
<span id="fairlangproc-metrics-embedding-module"></span><h2>FairLangProc.metrics.embedding module<a class="headerlink" href="#module-FairLangProc.metrics.embedding" title="Link to this heading">¶</a></h2>
<p>FairLangProc.metrics.embedding.py</p>
<p>Submodule inside of the FairLangProc.metrics module which stores all methods and metrics related
with the embeddings of a Language Model.</p>
<p>The WEAT class is flexible enough to implement other embedding metrics like SEAT or CEAT.</p>
<dl class="py class">
<dt class="sig sig-object py" id="FairLangProc.metrics.embedding.BertWEAT">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.embedding.</span></span><span class="sig-name descname"><span class="pre">BertWEAT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TokenizerType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cuda'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/FairLangProc/metrics/embedding.html#BertWEAT"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.embedding.BertWEAT" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#FairLangProc.metrics.embedding.WEAT" title="FairLangProc.metrics.embedding.WEAT"><code class="xref py py-class docutils literal notranslate"><span class="pre">WEAT</span></code></a></p>
<p>Class with implementation of _get_embedding for bidirectional transformers</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="FairLangProc.metrics.embedding.WEAT">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.embedding.</span></span><span class="sig-name descname"><span class="pre">WEAT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TokenizerType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cuda'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/FairLangProc/metrics/embedding.html#WEAT"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.embedding.WEAT" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">ABC</span></code></p>
<p>Class for handling WEAT metric with a PyTorch model and tokenizer.</p>
<section id="attributes">
<h3>Attributes<a class="headerlink" href="#attributes" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model<span class="classifier">nn.Module     </span></dt><dd><p>PyTorch model (e.g., BERT, GPT from HuggingFace).</p>
</dd>
<dt>tokenizer<span class="classifier">TokenizerType</span></dt><dd><p>Tokenizer for the model.</p>
</dd>
<dt>device<span class="classifier">str</span></dt><dd><p>Device to run the WEAT test on.</p>
</dd>
</dl>
</section>
<section id="methods">
<h3>Methods<a class="headerlink" href="#methods" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>metric(W1_words, W2_words, A1_words, A2_words, n_perm, pval)</dt><dd><p>Computation of the WEAT effect size between W1, W2 and A1, A2.</p>
</dd>
<dt>_get_embedding(outputs)</dt><dd><p>Abstract method whose implementation is required and which aims to compute the embedding of an output given
by the model.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="FairLangProc.metrics.embedding.WEAT.cosine_similarity">
<span class="sig-name descname"><span class="pre">cosine_similarity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/embedding.html#WEAT.cosine_similarity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.embedding.WEAT.cosine_similarity" title="Link to this definition">¶</a></dt>
<dd><p>Compute cosine similarity between two tensors.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="FairLangProc.metrics.embedding.WEAT.effect_size">
<span class="sig-name descname"><span class="pre">effect_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">A</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">B</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/embedding.html#WEAT.effect_size"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.embedding.WEAT.effect_size" title="Link to this definition">¶</a></dt>
<dd><p>Compute WEAT effect size.</p>
<section id="parameters">
<h4>Parameters<a class="headerlink" href="#parameters" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">torch.Tensor</span></dt><dd><p>Target concept 1 embeddings (n_X, dim)</p>
</dd>
<dt>Y<span class="classifier">torch.Tensor</span></dt><dd><p>Target concept 2 embeddings (n_Y, dim)</p>
</dd>
<dt>A<span class="classifier">torch.Tensor</span></dt><dd><p>Attribute 1 embeddings (n_A, dim)</p>
</dd>
<dt>B<span class="classifier">torch.Tensor</span></dt><dd><p>Attribute 2 embeddings (n_B, dim)</p>
</dd>
</dl>
</section>
<section id="returns">
<h4>Returns<a class="headerlink" href="#returns" title="Link to this heading">¶</a></h4>
<p>Effect size : float</p>
</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Link to this heading">¶</a></h4>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">FairLangProc.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">WEAT</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">BertWEAT</span><span class="p">(</span><span class="n">WEAT</span><span class="p">):</span>
<span class="go">    def _get_embedding(self, outputs):</span>
<span class="go">        return outputs.last_hidden_state[:, 0, :]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">math</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;math&#39;</span><span class="p">,</span> <span class="s1">&#39;algebra&#39;</span><span class="p">,</span> <span class="s1">&#39;geometry&#39;</span><span class="p">,</span> <span class="s1">&#39;calculus&#39;</span><span class="p">,</span> <span class="s1">&#39;equations&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">arts</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;poetry&#39;</span><span class="p">,</span> <span class="s1">&#39;art&#39;</span><span class="p">,</span> <span class="s1">&#39;dance&#39;</span><span class="p">,</span> <span class="s1">&#39;literature&#39;</span><span class="p">,</span> <span class="s1">&#39;novel&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">masc</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;male&#39;</span><span class="p">,</span> <span class="s1">&#39;man&#39;</span><span class="p">,</span> <span class="s1">&#39;boy&#39;</span><span class="p">,</span> <span class="s1">&#39;brother&#39;</span><span class="p">,</span> <span class="s1">&#39;he&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">femn</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;female&#39;</span><span class="p">,</span> <span class="s1">&#39;woman&#39;</span><span class="p">,</span> <span class="s1">&#39;girl&#39;</span><span class="p">,</span> <span class="s1">&#39;sister&#39;</span><span class="p">,</span> <span class="s1">&#39;she&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weatClass</span> <span class="o">=</span> <span class="n">BertWEAT</span><span class="p">(</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weatClass</span><span class="o">.</span><span class="n">metric</span><span class="p">(</span>
<span class="go">        W1_words = math, W2_words = arts,</span>
<span class="go">        A1_words = masc, A2_words = femn,</span>
<span class="go">        pval = False</span>
<span class="go">        )</span>
</pre></div>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="FairLangProc.metrics.embedding.WEAT.get_embeddings">
<span class="sig-name descname"><span class="pre">get_embeddings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/embedding.html#WEAT.get_embeddings"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.embedding.WEAT.get_embeddings" title="Link to this definition">¶</a></dt>
<dd><p>Get embeddings for a list of words using the LLM.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="FairLangProc.metrics.embedding.WEAT.metric">
<span class="sig-name descname"><span class="pre">metric</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">W1_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">W2_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">A1_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">A2_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_perm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pval</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/embedding.html#WEAT.metric"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.embedding.WEAT.metric" title="Link to this definition">¶</a></dt>
<dd><p>Run WEAT test.</p>
<section id="id1">
<h4>Parameters<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>W1_words<span class="classifier">list[str]</span></dt><dd><p>Target concept 1 words/sentences</p>
</dd>
<dt>W2_words<span class="classifier">list[str]</span></dt><dd><p>Target concept 2 words</p>
</dd>
<dt>A1_words<span class="classifier">list[str]</span></dt><dd><p>Attribute 1 words/sentences</p>
</dd>
<dt>A2_words<span class="classifier">list[str]</span></dt><dd><p>Attribute 2 words/sentences</p>
</dd>
<dt>n_perm<span class="classifier">int</span></dt><dd><p>Number of permutations for p-value</p>
</dd>
<dt>pval<span class="classifier">bool</span></dt><dd><p>Whether to compute or not the p-value</p>
</dd>
</dl>
</section>
<section id="id2">
<h4>Returns<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>results<span class="classifier">dict[str, float]</span></dt><dd><p>Dictionary with test results, namely mean similarity between W1, W2 and A1, A2; their sizes,
the WEAT effect size and the p-value if needed.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="FairLangProc.metrics.embedding.WEAT.p_value">
<span class="sig-name descname"><span class="pre">p_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">A</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">B</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_perm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10000</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/embedding.html#WEAT.p_value"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.embedding.WEAT.p_value" title="Link to this definition">¶</a></dt>
<dd><p>Compute p-value using permutation test.</p>
<section id="id3">
<h4>Parameters<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X, Y, A, B<span class="classifier">torch.Tensor</span></dt><dd><p>Embedding tensors</p>
</dd>
<dt>n_perm<span class="classifier">int</span></dt><dd><p>Number of permutations</p>
</dd>
</dl>
</section>
<section id="id4">
<h4>Returns<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h4>
<p>p-value : float</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-FairLangProc.metrics.generated_text">
<span id="fairlangproc-metrics-generated-text-module"></span><h2>FairLangProc.metrics.generated_text module<a class="headerlink" href="#module-FairLangProc.metrics.generated_text" title="Link to this heading">¶</a></h2>
<p>FairLangProc.metrics.generated_text.py</p>
<p>Submodule inside of the FairLangProc.metrics module which stores all methods and metrics related
with generated text.</p>
<p>The supported metrics are Demographic Representation (DemRep), Stereotypical Association (StereoAsoc) and HONEST.</p>
<dl class="py function">
<dt class="sig sig-object py" id="FairLangProc.metrics.generated_text.DemRep">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.generated_text.</span></span><span class="sig-name descname"><span class="pre">DemRep</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">demWords</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sentences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/generated_text.html#DemRep"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.generated_text.DemRep" title="Link to this definition">¶</a></dt>
<dd><p>Computes Demographic representation.</p>
<section id="id5">
<h3>Parameters<a class="headerlink" href="#id5" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>demWords<span class="classifier">dict[str, list[str]]</span></dt><dd><p>Dictionary whose keys represent demographic attributes
and whose values represent words with demographic meaning.</p>
</dd>
<dt>sentences<span class="classifier">list[str]</span></dt><dd><p>List of sentences to run the demographic representation.</p>
</dd>
</dl>
</section>
<section id="id6">
<h3>Returns<a class="headerlink" href="#id6" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>demRepVect<span class="classifier">dict[str, int]</span></dt><dd><p>Dictionary with demographic counts for all considered words and sentences.</p>
</dd>
</dl>
</section>
<section id="id7">
<h3>Example<a class="headerlink" href="#id7" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gendered_words</span> <span class="o">=</span> <span class="p">{</span>
<span class="go">        &#39;male&#39;: [&#39;he&#39;, &#39;him&#39;, &#39;his&#39;],</span>
<span class="go">        &#39;female&#39;: [&#39;she&#39;, &#39;her&#39;, &#39;actress&#39;, &#39;hers&#39;]</span>
<span class="go">        }</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span>
<span class="go">        &#39;She is such a good match to him.&#39;,</span>
<span class="go">        &#39;He is trying way too hard to be an actor.&#39;,</span>
<span class="go">        &#39;Her mother is trying to make ends meet.&#39;</span>
<span class="go">        &#39;My aunt is baking, do you want to try?&#39;</span>
<span class="go">        ]</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">DR</span> <span class="o">=</span> <span class="n">DemRep</span><span class="p">(</span>
<span class="go">        sentences = sentences,</span>
<span class="go">        demWords = gendered_words</span>
<span class="go">        )   </span>
</pre></div>
</div>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="FairLangProc.metrics.generated_text.HONEST">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.generated_text.</span></span><span class="sig-name descname"><span class="pre">HONEST</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">completions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hurtLex</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/generated_text.html#HONEST"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.generated_text.HONEST" title="Link to this definition">¶</a></dt>
<dd><p>Computes HONEST score for a set of completions given a hurt lexicon.</p>
<section id="id8">
<h3>Parameters<a class="headerlink" href="#id8" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>completions<span class="classifier">list[list[str]]</span></dt><dd><p>List of completions given by a model. This is a list of lists: it should contain lists
of completions of the same size. The lists of completions are lists of constant size
containing the completions of a given model for a certain prompt.</p>
</dd>
<dt>hurtLex<span class="classifier">list[str]</span></dt><dd><p>List of words belonging to the hurt lexicon.</p>
</dd>
</dl>
</section>
<section id="id9">
<h3>Returns<a class="headerlink" href="#id9" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>honestScore<span class="classifier">float</span></dt><dd><p>HONEST score for the given sentences and hurt lexicon.</p>
</dd>
</dl>
</section>
<section id="id10">
<h3>Example<a class="headerlink" href="#id10" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
<span class="go">        &#39;I cannot believe she did that, she is so&#39;,</span>
<span class="go">        &#39;I cannot believe he did that, he is so&#39;,</span>
<span class="go">        &#39;She tries and she tries to no avail. She is such a&#39;,</span>
<span class="go">        &#39;He tries and she tries to no avail. He is such a&#39;,</span>
<span class="go">        ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">completions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
<span class="go">        input_ids = tokenizer.encode(prompt, return_tensors=&quot;pt&quot;)</span>
<span class="go">        outputs = model.generate(</span>
<span class="go">            input_ids,</span>
<span class="go">            max_length=30,</span>
<span class="go">            num_return_sequences=3,</span>
<span class="go">            do_sample=True,</span>
<span class="go">            early_stopping=True</span>
<span class="go">        )</span>
<span class="go">        completions.append([tokenizer.decode(output, skip_special_tokens=True) for output in outputs])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hurtLex</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;fool&#39;</span><span class="p">,</span> <span class="s1">&#39;bitch&#39;</span><span class="p">,</span> <span class="s1">&#39;stupid&#39;</span><span class="p">,</span> <span class="s1">&#39;incompetent&#39;</span><span class="p">,</span> <span class="s1">&#39;idiot&#39;</span><span class="p">,</span> <span class="s1">&#39;dumb&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">honestScore</span> <span class="o">=</span> <span class="n">HONEST</span><span class="p">(</span>
<span class="go">        completions = completions,</span>
<span class="go">        hurtLex = hurtLex</span>
<span class="go">    )</span>
</pre></div>
</div>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="FairLangProc.metrics.generated_text.StereoAsoc">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.generated_text.</span></span><span class="sig-name descname"><span class="pre">StereoAsoc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">targetWords</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">demWords</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sentences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/generated_text.html#StereoAsoc"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.generated_text.StereoAsoc" title="Link to this definition">¶</a></dt>
<dd><p>Computes Stereotypical Association</p>
<section id="id11">
<h3>Parameters<a class="headerlink" href="#id11" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>targetWords<span class="classifier">list[str]</span></dt><dd><p>List of words whose associations we want to compute.</p>
</dd>
<dt>demWords<span class="classifier">dict[str, list[str]]</span></dt><dd><p>Dictionary whose keys represent demographic attributes
and whose values represent words with demographic meaning.</p>
</dd>
<dt>sentences<span class="classifier">list[str]</span></dt><dd><p>List of sentences to run the stereotypical association.</p>
</dd>
</dl>
</section>
<section id="id12">
<h3>Returns<a class="headerlink" href="#id12" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>steAsocVect<span class="classifier">dict</span></dt><dd><p>Dictionary which stores demographic counts for all considered
words and sentences indexed by targetWords.</p>
</dd>
</dl>
</section>
<section id="id13">
<h3>Example<a class="headerlink" href="#id13" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gendered_words</span> <span class="o">=</span> <span class="p">{</span>
<span class="go">        &#39;male&#39;: [&#39;he&#39;, &#39;him&#39;, &#39;his&#39;],</span>
<span class="go">        &#39;female&#39;: [&#39;she&#39;, &#39;her&#39;, &#39;actress&#39;, &#39;hers&#39;]</span>
<span class="go">        }</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span>
<span class="go">        &#39;She is such a good match to him.&#39;,</span>
<span class="go">        &#39;He is trying way too hard to be an actor.&#39;,</span>
<span class="go">        &#39;Her mother is trying to make ends meet.&#39;</span>
<span class="go">        &#39;My aunt is baking, do you want to try?&#39;</span>
<span class="go">        ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mother&#39;</span><span class="p">,</span> <span class="s1">&#39;baking&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ST</span> <span class="o">=</span> <span class="n">StereoAsoc</span><span class="p">(</span>
<span class="go">        sentences = sentences,</span>
<span class="go">        demWords = gendered_words,</span>
<span class="go">        targetWords = target_words</span>
<span class="go">        )</span>
</pre></div>
</div>
</section>
</dd></dl>

</section>
<section id="module-FairLangProc.metrics.probability">
<span id="fairlangproc-metrics-probability-module"></span><h2>FairLangProc.metrics.probability module<a class="headerlink" href="#module-FairLangProc.metrics.probability" title="Link to this heading">¶</a></h2>
<p>FairLangProc.metrics.probability.py</p>
<p>Submodule inside of the FairLangProc.metrics module which stores all methods and metrics related
with Language Modelling.</p>
<p>The supported metrics are LPBS, CBS, CPS, AUL.</p>
<dl class="py function">
<dt class="sig sig-object py" id="FairLangProc.metrics.probability.AUL">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.probability.</span></span><span class="sig-name descname"><span class="pre">AUL</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TokenizerType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sentences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/probability.html#AUL"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.probability.AUL" title="Link to this definition">¶</a></dt>
<dd><p>Computes the AUL score for list of sentences.</p>
<section id="id14">
<h3>Parameters<a class="headerlink" href="#id14" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model<span class="classifier">nn.Module</span></dt><dd><p>Language model used to compute probabilities.</p>
</dd>
<dt>tokenizer<span class="classifier">TokenizerType</span></dt><dd><p>Tokenizer associated with the model.</p>
</dd>
<dt>sentences<span class="classifier">list[str]</span></dt><dd><p>List of sentences for whom we will compute the AUL score.</p>
</dd>
</dl>
</section>
<section id="id15">
<h3>Returns<a class="headerlink" href="#id15" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>score<span class="classifier">list[float]</span></dt><dd><p>List of AUL score of the sentences.</p>
</dd>
</dl>
</section>
<section id="id16">
<h3>Example<a class="headerlink" href="#id16" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;The actor did a terrible job&#39;</span><span class="p">,</span> <span class="s1">&#39;The actress did a terrible job&#39;</span><span class="p">,</span> <span class="s1">&#39;The doctor was an exemplary man&#39;</span><span class="p">,</span> <span class="s1">&#39;The doctor was an exemplary woman&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">AULscore</span> <span class="o">=</span> <span class="n">AUL</span><span class="p">(</span>
<span class="go">        model = model,</span>
<span class="go">        tokenizer = tokenizer,</span>
<span class="go">        sentences = sentences</span>
<span class="go">    )</span>
</pre></div>
</div>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="FairLangProc.metrics.probability.CBS">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.probability.</span></span><span class="sig-name descname"><span class="pre">CBS</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TokenizerType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sentences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/probability.html#CBS"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.probability.CBS" title="Link to this definition">¶</a></dt>
<dd><p>Computes CBS score for a list of tuples of dimension n of target words.</p>
<section id="id17">
<h3>Parameters<a class="headerlink" href="#id17" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model<span class="classifier">nn.Module                  </span></dt><dd><p>Language model used to compute probabilities.</p>
</dd>
<dt>tokenizer<span class="classifier">TokenizerType              </span></dt><dd><p>Tokenizer associated with the model</p>
</dd>
<dt>sentences<span class="classifier">list[str]     </span></dt><dd><p>List of sentences with masks</p>
</dd>
<dt>target_words<span class="classifier">list[tuple[str]]  </span></dt><dd><p>List containing tuples of words whose probabilities we want to compute</p>
</dd>
<dt>fill_words<span class="classifier">list[str]          </span></dt><dd><p>List of words which replace the secondary mask</p>
</dd>
<dt>mask_indices<span class="classifier">list[int]          </span></dt><dd><p>List of indices which indicate to which mask of the sentence
each target word corresponds (i.e. first (0) or second (1))</p>
</dd>
</dl>
</section>
<section id="id18">
<h3>Returns<a class="headerlink" href="#id18" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>probs<span class="classifier">torch.Tensor</span></dt><dd><p>List of CBS scores</p>
</dd>
</dl>
</section>
<section id="id19">
<h3>Example<a class="headerlink" href="#id19" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_words</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;John&quot;</span><span class="p">,</span> <span class="s2">&quot;Mamadouk&quot;</span><span class="p">,</span> <span class="s2">&quot;Liu&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;white&quot;</span><span class="p">,</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="s2">&quot;asian&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;white&quot;</span><span class="p">,</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="s2">&quot;asian&quot;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;[MASK] is a [MASK]&quot;</span><span class="p">,</span> <span class="s2">&quot;The [MASK] kid got [MASK] results&quot;</span><span class="p">,</span> <span class="s2">&quot;The [MASK] kid wanted to be a [MASK]&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fill_words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;outstanding&quot;</span><span class="p">,</span> <span class="s2">&quot;doctor&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask_indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">CBSscore</span> <span class="o">=</span> <span class="n">CBS</span><span class="p">(</span>
<span class="go">        model = model,</span>
<span class="go">        tokenizer = tokenizer,</span>
<span class="go">        sentences = sentences,</span>
<span class="go">        target_words = target_words,</span>
<span class="go">        fill_words = fill_words,</span>
<span class="go">        mask_indices = mask_indices</span>
<span class="go">    )</span>
</pre></div>
</div>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="FairLangProc.metrics.probability.CPS">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.probability.</span></span><span class="sig-name descname"><span class="pre">CPS</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TokenizerType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sentences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/probability.html#CPS"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.probability.CPS" title="Link to this definition">¶</a></dt>
<dd><p>Computes the CPS score for list of sentences.</p>
<section id="id20">
<h3>Parameters<a class="headerlink" href="#id20" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model<span class="classifier">nn.Module</span></dt><dd><p>Language model used to compute probabilities.</p>
</dd>
<dt>tokenizer<span class="classifier">TokenizerType</span></dt><dd><p>Tokenizer associated with the model.</p>
</dd>
<dt>sentences<span class="classifier">list[str]</span></dt><dd><p>List of sentences for whom we will compute the CPS score.</p>
</dd>
<dt>target_words<span class="classifier">list[str]</span></dt><dd><p>List of target words which should not be masked.</p>
</dd>
</dl>
</section>
<section id="id21">
<h3>Returns<a class="headerlink" href="#id21" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>score<span class="classifier">list[float]</span></dt><dd><p>List of CPS score of the sentences.</p>
</dd>
</dl>
</section>
<section id="id22">
<h3>Example<a class="headerlink" href="#id22" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;The actor did a terrible job&#39;</span><span class="p">,</span> <span class="s1">&#39;The actress did a terrible job&#39;</span><span class="p">,</span> <span class="s1">&#39;The doctor was an exemplary man&#39;</span><span class="p">,</span> <span class="s1">&#39;The doctor was an exemplary woman&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;actor&#39;</span><span class="p">,</span> <span class="s1">&#39;actress&#39;</span><span class="p">,</span> <span class="s1">&#39;man&#39;</span><span class="p">,</span> <span class="s1">&#39;woman&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">CPSscore</span> <span class="o">=</span> <span class="n">CPS</span><span class="p">(</span>
<span class="go">        model = model,</span>
<span class="go">        tokenizer = tokenizer,</span>
<span class="go">        sentences = sentences,</span>
<span class="go">        target_words = target_words</span>
<span class="go">    )</span>
</pre></div>
</div>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="FairLangProc.metrics.probability.LPBS">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.probability.</span></span><span class="sig-name descname"><span class="pre">LPBS</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TokenizerType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sentences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/probability.html#LPBS"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.probability.LPBS" title="Link to this definition">¶</a></dt>
<dd><p>Computes LPBS score for a list of tuples of dimension 2 of target words.</p>
<section id="id23">
<h3>Parameters<a class="headerlink" href="#id23" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model<span class="classifier">nn.Module                  </span></dt><dd><p>Language model used to compute probabilities.</p>
</dd>
<dt>tokenizer<span class="classifier">TokenizerType              </span></dt><dd><p>Tokenizer associated with the model.</p>
</dd>
<dt>sentences<span class="classifier">list[str]              </span></dt><dd><p>List of sentences with masks.</p>
</dd>
<dt>target_words<span class="classifier">list[tuple[str]]    </span></dt><dd><p>List containing tuples of words whose probabilities we want to compute.</p>
</dd>
<dt>fill_words<span class="classifier">list[str]             </span></dt><dd><p>List of words which replace the secondary mask.</p>
</dd>
<dt>mask_indices<span class="classifier">list[int]           </span></dt><dd><p>List of indices which indicate to which mask of the sentence each 
target word corresponds (i.e. first (0) or second (1)).</p>
</dd>
</dl>
</section>
<section id="id24">
<h3>Returns<a class="headerlink" href="#id24" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>probs<span class="classifier">torch.Tensor               </span></dt><dd><p>List of LPBS scores</p>
</dd>
</dl>
</section>
<section id="id25">
<h3>Example<a class="headerlink" href="#id25" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;[MASK] is a [MASK].&quot;</span><span class="p">,</span> <span class="s2">&quot;[MASK] is a [MASK].&quot;</span><span class="p">,</span> <span class="s2">&quot;The [MASK] was a [MASK].&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_words</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;John&quot;</span><span class="p">,</span> <span class="s2">&quot;Mary&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;He&quot;</span><span class="p">,</span> <span class="s2">&quot;She&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="s2">&quot;woman&quot;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fill_words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;engineer&quot;</span><span class="p">,</span><span class="s2">&quot;nurse&quot;</span><span class="p">,</span><span class="s2">&quot;doctor&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask_indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">LPBSscore</span> <span class="o">=</span> <span class="n">LPBS</span><span class="p">(</span>
<span class="go">        model = model,</span>
<span class="go">        tokenizer = tokenizer,</span>
<span class="go">        sentences = sentences,</span>
<span class="go">        target_words = target_words,</span>
<span class="go">        fill_words = fill_words,</span>
<span class="go">        mask_indices = mask_indices</span>
<span class="go">    )</span>
</pre></div>
</div>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="FairLangProc.metrics.probability.MaskProbability">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.probability.</span></span><span class="sig-name descname"><span class="pre">MaskProbability</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TokenizerType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sentences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">how_many</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/probability.html#MaskProbability"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.probability.MaskProbability" title="Link to this definition">¶</a></dt>
<dd><p>Computation of masked probability with a Language Model.</p>
<p>Computes the probability of a list of target words in the positions of certain masks given a list
of masked sentences (the number of masks is assumed to be constant)</p>
<section id="id26">
<h3>Parameters<a class="headerlink" href="#id26" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model<span class="classifier">nn.Module</span></dt><dd><p>Language Model used to compute probabilities.</p>
</dd>
<dt>tokenizer<span class="classifier">TokenizerType</span></dt><dd><p>Tokenizer associated with the model.</p>
</dd>
<dt>sentences<span class="classifier">list[str]</span></dt><dd><p>List of sentences with masks.</p>
</dd>
<dt>target_words<span class="classifier">list[str]</span></dt><dd><p>List of words whose probabilities we want to compute.</p>
</dd>
<dt>mask_indices<span class="classifier">list[int]</span></dt><dd><p>List of indices which indicate to which mask of the sentence
each word corresponds to (i.e. first, second,…)</p>
</dd>
<dt>how_many<span class="classifier">int</span></dt><dd><p>How many masks are in each sentence</p>
</dd>
</dl>
</section>
<section id="id27">
<h3>Returns<a class="headerlink" href="#id27" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>prob_target<span class="classifier">torch.Tensor</span></dt><dd><p>Probability of target_words in the positions indicated by mask_indices.</p>
</dd>
</dl>
</section>
<section id="id28">
<h3>Example<a class="headerlink" href="#id28" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;The [MASK] is a [MASK]&quot;</span><span class="p">,</span> <span class="s2">&quot;[MASK] is such a [MASK]&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;He&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask_indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">how_many</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">probabilities</span> <span class="o">=</span> <span class="n">MaskProbability</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">target_words</span><span class="p">,</span> <span class="n">mask_indices</span><span class="p">,</span> <span class="n">how_many</span> <span class="o">=</span> <span class="n">how_many</span><span class="p">)</span>
</pre></div>
</div>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="FairLangProc.metrics.probability.MaskProbabilityQuotient">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.probability.</span></span><span class="sig-name descname"><span class="pre">MaskProbabilityQuotient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TokenizerType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sentences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/probability.html#MaskProbabilityQuotient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.probability.MaskProbabilityQuotient" title="Link to this definition">¶</a></dt>
<dd><p>Computes the quotient of the probabilities of two different words in the same spot in a sentence.</p>
<p>Assumes sentences with two masks. Computes the quotient of the probability of target_words being in
the position of mask_indices divided by the prior probability of target_words in said position but with
fill_words masked.</p>
<section id="id29">
<h3>Parameters<a class="headerlink" href="#id29" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model<span class="classifier">nn.Module</span></dt><dd><p>Language Model used to compute probabilities.</p>
</dd>
<dt>tokenizer<span class="classifier">TokenizerType</span></dt><dd><p>Tokenizer associated with the model.</p>
</dd>
<dt>sentences<span class="classifier">list[str]</span></dt><dd><p>List of sentences with masks.</p>
</dd>
<dt>target_words<span class="classifier">list[tuple[str]]</span></dt><dd><p>List containing tuples of words whose probabilities we want to compute.</p>
</dd>
<dt>fill_words<span class="classifier">list[str]</span></dt><dd><p>List of words which replace the secondary mask.</p>
</dd>
<dt>mask_indices<span class="classifier">list[int]</span></dt><dd><p>List of indices which indicate to which mask of the sentence each
target word corresponds to (i.e. first (0) or second (1)).</p>
</dd>
</dl>
</section>
<section id="id30">
<h3>Returns<a class="headerlink" href="#id30" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>probs<span class="classifier">list[torch.Tensor]</span></dt><dd><p>Quotients of probabilities given as a list of tensors</p>
</dd>
</dl>
</section>
<section id="id31">
<h3>Example<a class="headerlink" href="#id31" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;The [MASK] is a [MASK]&quot;</span><span class="p">,</span> <span class="s2">&quot;[MASK] is such a [MASK]&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_words</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="s2">&quot;woman&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;He&quot;</span><span class="p">,</span> <span class="s2">&quot;She&quot;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fill_words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;drag&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask_indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quotients</span> <span class="o">=</span> <span class="n">MaskProbabilityQuotient</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">target_words</span><span class="p">,</span> <span class="n">fill_word</span><span class="p">,</span> <span class="n">mask_indices</span><span class="p">)</span>
</pre></div>
</div>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="FairLangProc.metrics.probability.MaskedPseudoLogLikelihood">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.probability.</span></span><span class="sig-name descname"><span class="pre">MaskedPseudoLogLikelihood</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cls_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/probability.html#MaskedPseudoLogLikelihood"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.probability.MaskedPseudoLogLikelihood" title="Link to this definition">¶</a></dt>
<dd><p>Computes the PLL score for a sentence where all words are progressively masked with the exception of a word
given by target_id.</p>
<section id="id32">
<h3>Parameters<a class="headerlink" href="#id32" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model<span class="classifier">nn.Module</span></dt><dd><p>Language model used to compute probabilities.</p>
</dd>
<dt>input_ids<span class="classifier">list[int]</span></dt><dd><p>List of tokens forming the sentence.</p>
</dd>
<dt>target_id<span class="classifier">int        </span></dt><dd><p>Id of the token which should not be masked.</p>
</dd>
<dt>mask_id<span class="classifier">int          </span></dt><dd><p>Id of the mask token.</p>
</dd>
<dt>cls_id<span class="classifier">int         </span></dt><dd><p>Id of the cls token.</p>
</dd>
<dt>pad_id<span class="classifier">int       </span></dt><dd><p>Id of the pad token.</p>
</dd>
</dl>
</section>
<section id="id33">
<h3>Returns<a class="headerlink" href="#id33" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>score<span class="classifier">float</span></dt><dd><p>PLL of the masked sentence.</p>
</dd>
</dl>
</section>
<section id="id34">
<h3>Example<a class="headerlink" href="#id34" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentence</span> <span class="o">=</span> <span class="s1">&#39;The actor did a terrible job&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">sentence</span><span class="p">],</span> <span class="n">return_tensors</span> <span class="o">=</span> <span class="s1">&#39;pt&#39;</span><span class="p">)[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;actor&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">mask_token_id</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pad_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_type_id</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cls_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">cls_token_id</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">MaskedPseudoLogLikelihood</span><span class="p">(</span>
<span class="go">        model = model,</span>
<span class="go">        input_ids = input_ids,</span>
<span class="go">        target_id = target_id,</span>
<span class="go">        mask_id = mask_id,</span>
<span class="go">        pad_id = pad_id,</span>
<span class="go">        cls_id = cls_id</span>
<span class="go">    )</span>
</pre></div>
</div>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="FairLangProc.metrics.probability.UnMaskedPseudoLogLikelihood">
<span class="sig-prename descclassname"><span class="pre">FairLangProc.metrics.probability.</span></span><span class="sig-name descname"><span class="pre">UnMaskedPseudoLogLikelihood</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cls_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/FairLangProc/metrics/probability.html#UnMaskedPseudoLogLikelihood"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#FairLangProc.metrics.probability.UnMaskedPseudoLogLikelihood" title="Link to this definition">¶</a></dt>
<dd><p>Computes the PLL score of an unmasked sentence.</p>
<section id="id35">
<h3>Parameters<a class="headerlink" href="#id35" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model<span class="classifier">nn.Module      </span></dt><dd><p>Language model used to compute probabilities.</p>
</dd>
<dt>input_ids<span class="classifier">list[int]</span></dt><dd><p>List of tokens forming the sentence.</p>
</dd>
<dt>cls_id<span class="classifier">int</span></dt><dd><p>Id of the cls token.</p>
</dd>
<dt>pad_id<span class="classifier">int</span></dt><dd><p>Id of the pad token.</p>
</dd>
</dl>
</section>
<section id="id36">
<h3>Returns<a class="headerlink" href="#id36" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>score<span class="classifier">float</span></dt><dd><p>PLL of the masked sentence.</p>
</dd>
</dl>
</section>
<section id="id37">
<h3>Example<a class="headerlink" href="#id37" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentence</span> <span class="o">=</span> <span class="s1">&#39;The actor did a terrible job&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">sentence</span><span class="p">],</span> <span class="n">return_tensors</span> <span class="o">=</span> <span class="s1">&#39;pt&#39;</span><span class="p">)[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pad_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_type_id</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cls_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">cls_token_id</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">UnMaskedPseudoLogLikelihood</span><span class="p">(</span>
<span class="go">        model = model,</span>
<span class="go">        input_ids = input_ids,</span>
<span class="go">        pad_id = pad_id,</span>
<span class="go">        cls_id = cls_id</span>
<span class="go">    )</span>
</pre></div>
</div>
</section>
</dd></dl>

</section>
<section id="module-FairLangProc.metrics">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-FairLangProc.metrics" title="Link to this heading">¶</a></h2>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">FairLangProc</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Arturo Perez-Peralta.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.1.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/FairLangProc.metrics.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>