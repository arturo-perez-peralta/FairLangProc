<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>FairLangProc.algorithms.preprocessors package &#8212; FairLangProc 0.1.3 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=686e5160" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=360bc84d"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="FairLangProc.algorithms.intraprocessors package" href="FairLangProc.algorithms.intraprocessors.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="fairlangproc-algorithms-preprocessors-package">
<h1>FairLangProc.algorithms.preprocessors package<a class="headerlink" href="#fairlangproc-algorithms-preprocessors-package" title="Link to this heading">¶</a></h1>
<p>Pre-processors are fairness processors that modify the model inputs.</p>
<p>The supported methods are:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#cda"><span class="std std-ref">Counterfactual Data Augmentation (CDA)</span></a> <a class="reference external" href="https://arxiv.org/abs/2010.06032">(Webster et al. 2020)</a>.</p></li>
<li><p><a class="reference internal" href="#emb"><span class="std std-ref">Projection based debiasing</span></a> <a class="reference external" href="https://arxiv.org/abs/1607.06520">(Bolukbasi et al., 2023)</a>.</p></li>
<li><p><a class="reference internal" href="#blind"><span class="std std-ref">Bias removaL wIth No Demographics (BLIND)</span></a> <a class="reference external" href="https://aclanthology.org/2023.acl-long.490/">(Orgad et al., 2023)</a>.</p></li>
</ul>
<section id="counterfactual-data-augmentation-cda">
<span id="cda"></span><h2>Counterfactual Data Augmentation (CDA)<a class="headerlink" href="#counterfactual-data-augmentation-cda" title="Link to this heading">¶</a></h2>
<p>Data augmentation is the process of curating or upsampling the dataset to obtain a more representative distribution to train the model on.
In particular, Counterfactual Data Augmentation (CDA) <a class="reference external" href="https://arxiv.org/abs/2010.06032">(Webster et al. 2020)</a> consists of flipping words with demographic information
while preserving semantic correctness. This procedure can be one-sided and discard the original sentence or two-sided to consider both the original and its
augmented version.</p>
</section>
<section id="projection-based-debiasing">
<span id="emb"></span><h2>Projection-based debiasing<a class="headerlink" href="#projection-based-debiasing" title="Link to this heading">¶</a></h2>
<p>Projection-based debiasing methods <a class="reference external" href="https://arxiv.org/abs/1607.06520">(Bolukbasi et al., 2023)</a> operate on latent space, looking to identify a bias subspace given by an
orthogonal basis, <span class="math notranslate nohighlight">\(\{v_i\}_{i=1}^{n_{bias}}\)</span>. Then, the hidden representation of any input can be debiased by removing its projection onto this space, formally</p>
<div class="math notranslate nohighlight">
\[h_{proj} = h - \sum_{i = 1}^{n_{bias} } \langle h, v_i \rangle \, v_i.\]</div>
<p>This can be done either at the word or sentence level. In either case the bias subspace is generally identified through PCA,
and usually its dimension is one, resulting in the construction of a bias direction.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">FairLangProc.algorithms.preprocessors.projection_based.</span></span><span class="sig-name descname"><span class="pre">SentDebiasModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TokenizerType</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_pairs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_components</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs_loss</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/FairLangProc/algorithms/preprocessors/projection_based.html#SentDebiasModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Implements SentDebiasModel, requires the implementation of _get_embedding, _loss and _get_loss methods.</p>
<section id="attributes">
<h3>Attributes<a class="headerlink" href="#attributes" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>bias_subpsace<span class="classifier">torch.Tensor</span></dt><dd><p>Tensor that stores the matrix/vector resulting from performing PCA on the difference of 
the words/sentences with sensitive attributes.</p>
</dd>
</dl>
</section>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">FairLangProc.algorithms.preprocessors</span><span class="w"> </span><span class="kn">import</span> <span class="n">SentDebiasForSequenceClassification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gendered_pairs</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;he&#39;</span><span class="p">,</span> <span class="s1">&#39;she&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;his&#39;</span><span class="p">,</span> <span class="s1">&#39;hers&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;monk&#39;</span><span class="p">,</span> <span class="s1">&#39;nun&#39;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">SentDebiasBert</span><span class="p">(</span><span class="n">SentDebiasForSequenceClassification</span><span class="p">):</span>        
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="nf">_get_embedding</span><span class="p">(</span>
<span class="gp">... </span>            <span class="bp">self</span><span class="p">,</span>
<span class="gp">... </span>            <span class="n">input_ids</span><span class="p">,</span>
<span class="gp">... </span>            <span class="n">attention_mask</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>            <span class="n">token_type_ids</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">... </span>            <span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">bert</span><span class="p">(</span>
<span class="gp">... </span>            <span class="n">input_ids</span><span class="p">,</span>
<span class="gp">... </span>            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">,</span>
<span class="gp">... </span>            <span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">token_type_ids</span>
<span class="gp">... </span>            <span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">EmbedModel</span> <span class="o">=</span> <span class="n">SentDebiasBert</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">config</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">TOKENIZER</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">word_pairs</span> <span class="o">=</span> <span class="n">gendered_pairs</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_components</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_labels</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">model</span><span class="o">=</span><span class="n">EmbedModel</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">val_dataset</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">optimizers</span><span class="o">=</span><span class="p">(</span>
<span class="gp">... </span>        <span class="n">AdamW</span><span class="p">(</span><span class="n">EmbedModel</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
<span class="gp">... </span>        <span class="kc">None</span>
<span class="gp">... </span>        <span class="p">)</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TokenizerType</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_pairs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_components</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs_loss</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/FairLangProc/algorithms/preprocessors/projection_based.html#SentDebiasModel.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Constructor of the SentDebiasModel class.</p>
<section id="parameters">
<h4>Parameters<a class="headerlink" href="#parameters" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>model<span class="classifier">nn.Module, str              </span></dt><dd><p>Language Model used.</p>
</dd>
<dt>config<span class="classifier">str</span></dt><dd><p>Optional, configuration to use when using AutoModel (i.e. when model is a string).</p>
</dd>
<dt>tokenizer<span class="classifier">TokenizerType</span></dt><dd><p>Tokenizer associated with the model.</p>
</dd>
<dt>word_pairs<span class="classifier">list[tuple[str]]</span></dt><dd><p>List of counterfactual tuples (might be words, sentences,…).</p>
</dd>
<dt>n_components<span class="classifier">int             </span></dt><dd><p>Number of components of the bias subspace.</p>
</dd>
<dt>device<span class="classifier">str</span></dt><dd><p>Device to run the model on.</p>
</dd>
</dl>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="blind-debiasing">
<span id="blind"></span><h2>BLIND debiasing<a class="headerlink" href="#blind-debiasing" title="Link to this heading">¶</a></h2>
<p>BLIND <a class="reference external" href="https://aclanthology.org/2023.acl-long.490/">(Orgad et al., 2023)</a> is a debiasing procedure based on a complementary classifier
<span class="math notranslate nohighlight">\(g_{B} : \mathbb{R}^{d_L} \longrightarrow \mathbb{R}\)</span> with parameters <span class="math notranslate nohighlight">\(\theta_{B}\)</span>, that takes the hidden representation vector
as inputs and outputs the success probability of the model head for the downstream task. This probability is then used as a weight for said observation
whose magnitude is controlled through a hyper-parameter <span class="math notranslate nohighlight">\(\gamma \geq 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{BLIND} = \left(1 - \sigma \left( g_{B}(h; \theta_{B} ) \right) \right)^{\gamma} \mathcal{L}^{task}(\hat{y}, y).\]</div>
<p>The term <span class="math notranslate nohighlight">\(\sigma(g_{B}(h;\theta_B))\)</span> represents the model success probability for the downstream task:
the bigger it is the less weight the observation has, while the smaller it is the more weight it carries.
This forces the model to pay special attention to observations with low probability of success during training.
Note that when <span class="math notranslate nohighlight">\(\gamma = 0\)</span> the original loss function is restored, while <span class="math notranslate nohighlight">\(\gamma &gt;&gt; 1\)</span> exacerbates the effect of the reweighting.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">FairLangProc</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="FairLangProc.datasets.html">FairLangProc.datasets package</a></li>
<li class="toctree-l1"><a class="reference internal" href="FairLangProc.metrics.html">FairLangProc.metrics package</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="FairLangProc.algorithms.html">FairLangProc.algorithms package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="FairLangProc.algorithms.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="FairLangProc.algorithms.inprocessors.html">FairLangProc.algorithms.inprocessors package</a></li>
<li class="toctree-l3"><a class="reference internal" href="FairLangProc.algorithms.intraprocessors.html">FairLangProc.algorithms.intraprocessors package</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">FairLangProc.algorithms.preprocessors package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#counterfactual-data-augmentation-cda">Counterfactual Data Augmentation (CDA)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#projection-based-debiasing">Projection-based debiasing</a></li>
<li class="toctree-l4"><a class="reference internal" href="#blind-debiasing">BLIND debiasing</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  <li><a href="FairLangProc.algorithms.html">FairLangProc.algorithms package</a><ul>
      <li>Previous: <a href="FairLangProc.algorithms.intraprocessors.html" title="previous chapter">FairLangProc.algorithms.intraprocessors package</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Arturo Perez-Peralta.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.1.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/FairLangProc.algorithms.preprocessors.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>