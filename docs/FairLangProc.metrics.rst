FairLangProc.metrics package
============================

FairLangProc supports different fairness metrics to measure discrimination in NLP. Broadly, they can be classified into three categories:

- **Embedding metrics**: if they measure bias by examining the model's hidden representations of input text.
- **Probability metrics**: if they measure bias by computing the probabilities of certain tokens or sentences.
- **Generated text metrics**: if they measure bias by examining text generated by the model, looking for harmful or stereotypical words.

The supported metrics are:

- :ref:`Generalized association tests (WEAT) <weat>` `(Caliskan et al., 2016) <https://arxiv.org/abs/1608.07187>`_.
- :ref:`Log Probability Bias Score (LPBS) <lpbs>` `(Kurita et al., 2019) <https://arxiv.org/abs/1906.07337>`_.
- :ref:`Categorical Bias Score (CBS) <cbs>` `(Ahn et al., 2021) <https://aclanthology.org/2021.emnlp-main.42/>`_.
- :ref:`CrowS-Pairs Score (CPS) <cps>` `(Nangia et al., 2020) <https://aclanthology.org/2020.emnlp-main.154/>`_.
- :ref:`All Unmasked Score (AUL) <aul>` `(Kaneko et al., 2021) <https://arxiv.org/abs/2104.07496>`_.
- :ref:`Demographic Representation (DR) <dr>` `(Liang et al., 2022) <https://arxiv.org/abs/2211.09110>`_.
- :ref:`Stereotypical Association (SA) <sa>` `(Liang et al., 2022) <https://arxiv.org/abs/2211.09110>`_.
- :ref:`HONEST <honest>` `(Nozza et al., 2021) <https://aclanthology.org/2021.naacl-main.191/>`_.

.. _weat:

WEAT
-------------------------------------

The most famous embedding metric is given the Word Embedding Association Test (WEAT) `(Caliskan et al., 2016) <https://arxiv.org/abs/1608.07187>`_,
which aims to measure associations between demographic and neutral attributes. Demographic attributes are usually binary and denoted by :math:`A_1, A_2`,
denoting two different societal groups (male and female, christians and atheist,...). Neutral attributes, on the other hand, are denoted by :math:`W_1, W_2`
and represent two different stereotypes whose demographic association we are interested in. These stereotypes can be occupational
(technical and care work, proffesions and home roles,...), academic (mathematics and arts, engineering and social sciences, medicine and nursing,...)
or related to prejudice (competence and incompetence, insults and praises,..), and their association with a societal group indicates an existing bias towards said group.
Embedding metrics measure this association through the cosine similarity of the embeddings of words belonging to text corpora associated with each neutral attribute,
:math:`\mathbb{W}_i`:

.. math::
    s(a, W_1, W_2) = \sum_{w_1\in \mathbb{W}_1} \frac{\cos(a, w_1)}{|\mathbb{W}_1|} - \sum_{w_2\in \mathbb{W}_2} \frac{\cos(a, w_2)}{|\mathbb{W}_2|},

where :math:`a` represents the embedding of an arbitrary word and $s$ represents its similarity to the neutral attributes,
with a positive score signifying an association with :math:`W_1` while a negative score implies a correlation with :math:`W_2`.
WEAT then measures bias through the effect size, which computes the average similarity between a corpora of text related to each sensitive attribute,
:math:`\mathbb{A}_i`, and the neutral parameters:

.. math::
    WEAT(A_1, A_2, W_1, W_2) = \frac{\sum_{a_1 \in A_1} s(a_1, W_1, W_2)/ |A_1| - \sum_{a_2 \in A_2} s(a_2, W_1, w_2)/ |A_2| }{\text{std}_{a\in A_1 \cup A_2} s(a, W_1, W_2)}.

A large effect size in either direction indicates a strong bias at the semantic level. This test can be run at the word or sentence level,
and it can be further generalized to contextualized embeddings. These metrics are implemented through the `WEAT` abstract class


.. autoclass:: FairLangProc.metrics.embedding.WEAT
   :members: __init__, _get_embedding, metric
   :no-index:

.. _dr:

Demographic Representation
-------------------------------------------

DR `(Liang et al., 2022) <https://arxiv.org/abs/2211.09110>`_ is computed as follows: Given a corpus of sequences of generated text
:math:`\hat{\mathbb{Y}}`, for each societal group :math:`a` with associated words :math:`\mathbb{A}` its demographic representation is given by

.. math::
    \text{DR}(a) = \sum_{w_i \in \mathbb{A}}\sum_{\hat{Y} \in \hat{\mathbb{Y}}} C(w_i, \hat{Y}),

where :math:`C(w, Y)` denotes the count of how many times word :math:`w` appears in text :math:`Y`.

.. autofunction:: FairLangProc.metrics.generated_text.DemRep

.. _sa:

Stereotypical Association
-------------------------------------------

ST `(Liang et al., 2022) <https://arxiv.org/abs/2211.09110>`_ is computed as follows: Given a specific term :math:`w`, a corpus of sequences of generated text
:math:`\hat{\mathbb{Y}}`, for each societal group :math:`a` with associated words :math:`\mathbb{A}` its stereotypical association is given by

.. math::
    \text{DR}(a) = \sum_{w_i \in \mathbb{A}}\sum_{\hat{Y} \in \hat{\mathbb{Y}}} C(w_i, \hat{Y}),

where :math:`C(w, Y)` denotes the count of how many times word :math:`w` appears in text :math:`Y`.

.. autofunction:: FairLangProc.metrics.generated_text.StereoAsoc

.. _honest:

HONEST
-------------------------------------------
HONEST `(Nozza et al., 2021) <https://aclanthology.org/2021.naacl-main.191/>`_ measures how many of the top :math:`k`` completions of a given model,
:math:`\hat{\mathbb{Y}}_k`, contain harmful words measured by:

.. math::
    \text{HONEST}(\hat{\mathbb{Y}} ) = \frac{\sum_{\hat{Y}_k \in\hat{\mathbb{Y}}_k} \sum_{\hat{y} \in \hat{Y}_k} \mathbf{1}(\hat{y} \in \mathbb{Y}_{hurt} ) }{|\mathbb{\hat{Y}}| k}.

.. autofunction:: FairLangProc.metrics.generated_text.HONEST

.. _lpbs:

LPBS
---------------------------------------

LPBS `(Kurita et al., 2019) <https://arxiv.org/abs/1906.07337>`_ measures bias for a binary demographic group. It computes the predicted probability for a token :math:`a`, :math:`p_a`,
using the template "[MASK] is [NEUTRAL ATTRIBUTE]"; and normalizes it by computing the model's prior probability, :math:`p_{a, prior}`,
based on the template "[MASK] is [MASK]". The score is computed as the difference of the logarithms of the normalized probabilities,

.. math::
    \text{LPBS} = \log\frac{p_1}{p_{prior, 1}} - \log\frac{p_2}{p_{prior, 2}}.

.. autofunction:: FairLangProc.metrics.probability.LPBS

.. _cbs:

CBS
---------------------------------------

CBS `(Ahn et al., 2021) <https://aclanthology.org/2021.emnlp-main.42/>`_ generalizes measurent of bias for non-binary demographic groups. It computes the predicted probability for a token :math:`a`, :math:`p_a`,
using the template "[MASK] is [NEUTRAL ATTRIBUTE]"; and normalizes it by computing the model's prior probability, :math:`p_{a, prior}`,
based on the template "[MASK] is [MASK]". The score is computed as the variance of the logarithms of the normalized probabilities,

.. math::
    \text{CBS} =  \text{Var}_{a\in \mathbb{A}}\log\frac{p_a}{p_{prior, a}}.

.. autofunction:: FairLangProc.metrics.probability.CBS

.. _cps:

CPS
---------------------------------------

CPS `(Nangia et al., 2020) <https://aclanthology.org/2020.emnlp-main.154/>`_ uses sentence pairs which coincide in a series of unmodified tokens,
:math:`U`, and only differ on words containing demographic information, :math:`A`. The score is given by

.. math::
    \text{CPS}(S) = \sum_{u\in U} \log \mathbb{P}(u| U_{\backslash u}, A),

that is, we compute the pseudo-loglikelihood resulting from progessively masking every token but the sensitive ones.

.. autofunction:: FairLangProc.metrics.probability.CPS

.. _aul:

AUL
---------------------------------------
AUL `(Kaneko et al., 2021) <https://arxiv.org/abs/2104.07496>`_ predicts the probability of all tokens in the sentence without masking to prevent selection bias,

.. math::
    \text{AUL}(S) = \frac{1}{|S|} \sum_{s\in S} \log \mathbb{P}(s|S).

.. autofunction:: FairLangProc.metrics.probability.AUL
